{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b918cd2e",
   "metadata": {},
   "source": [
    "# 실습1: Vanilla RNN 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5bcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# TODO: [지시사항 1번] 첫번째 모델을 완성하세요.\n",
    "def build_model1():\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Embedding(10, 5))\n",
    "    model.add(layers.SimpleRNN(3))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# TODO: [지시사항 2번] 두번째 모델을 완성하세요.\n",
    "def build_model2():\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Embedding(256, 100))\n",
    "    model.add(layers.SimpleRNN(20))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def main():\n",
    "    model1 = build_model1()\n",
    "    print(\"=\" * 20, \"첫번째 모델\", \"=\" * 20)\n",
    "    model1.summary()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    model2 = build_model2()\n",
    "    print(\"=\" * 20, \"두번째 모델\", \"=\" * 20)\n",
    "    model2.summary()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59bb62",
   "metadata": {},
   "source": [
    "# 실습2: Vanilla RNN으로 IMDb 데이터 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03948951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(num_words, max_len):\n",
    "    # imdb 데이터셋을 불러옵니다. 데이터셋에서 단어는 num_words 개를 가져옵니다.\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "    # 단어 개수가 다른 문장들을 Padding을 추가하여\n",
    "    # 단어가 가장 많은 문장의 단어 개수로 통일합니다.\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def build_rnn_model(num_words, embedding_len):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # TODO: [지시사항 1번] 지시사항에 따라 모델을 완성하세요.\n",
    "    model.add(layers.Embedding(num_words, embedding_len))\n",
    "    model.add(layers.SimpleRNN(16))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main(model=None, epochs=5):\n",
    "    # IMDb 데이터셋에서 가져올 단어의 개수\n",
    "\n",
    "    num_words = 6000\n",
    "    \n",
    "    # 각 문장이 가질 수 있는 최대 단어 개수\n",
    "    max_len = 130\n",
    "    \n",
    "    # 임베딩 된 벡터의 길이\n",
    "    embedding_len = 100\n",
    "    \n",
    "    # IMDb 데이터셋을 불러옵니다.\n",
    "    X_train, X_test, y_train, y_test = load_data(num_words, max_len)\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_rnn_model(num_words, embedding_len)\n",
    "    \n",
    "    # TODO: [지시사항 2번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # TODO: [지시사항 3번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=100, validation_split=0.2, shuffle=True, verbose=2)\n",
    "    \n",
    "    # 모델을 테스트 데이터셋으로 테스트합니다.\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print(\"테스트 Loss: {:.5f}, 테스트 정확도: {:.3f}%\".format(test_loss, test_acc * 100))\n",
    "    \n",
    "    return optimizer, hist \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af199d3c",
   "metadata": {},
   "source": [
    "# 실습3: Vanilla RNN을 통한 항공 승객 수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7eef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elice_utils import EliceUtils\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(window_size):\n",
    "    raw_data = pd.read_csv(\"./airline-passengers.csv\")\n",
    "    raw_passengers = raw_data[\"Passengers\"].to_numpy()\n",
    "\n",
    "    # 데이터의 평균과 표준편차 값으로 정규화(표준화) 합니다.\n",
    "    mean_passenger = raw_passengers.mean()\n",
    "    stdv_passenger = raw_passengers.std(ddof=0)\n",
    "    raw_passengers = (raw_passengers - mean_passenger) / stdv_passenger\n",
    "    data_stat = {\"month\": raw_data[\"Month\"], \"mean\": mean_passenger, \"stdv\": stdv_passenger}\n",
    "\n",
    "    # window_size 개의 데이터를 불러와 입력 데이터(X)로 설정하고\n",
    "    # window_size보다 한 시점 뒤의 데이터를 예측할 대상(y)으로 설정하여\n",
    "    # 데이터셋을 구성합니다.\n",
    "    X, y = [], []\n",
    "    for i in range(len(raw_passengers) - window_size):\n",
    "        cur_passenger = raw_passengers[i:i + window_size]\n",
    "        target = raw_passengers[i + window_size]\n",
    "\n",
    "        X.append(list(cur_passenger))\n",
    "        y.append(target)\n",
    "\n",
    "    # X와 y를 numpy array로 변환합니다.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # 각 입력 데이터는 sequence 길이가 window_size이고, featuer 개수는 1개가 되도록\n",
    "    # 마지막에 새로운 차원을 추가합니다.\n",
    "    # 즉, (전체 데이터 개수, window_size) -> (전체 데이터 개수, window_size, 1)이 되도록 변환합니다.\n",
    "    X = X[:, :, np.newaxis]\n",
    "\n",
    "    # 학습 데이터는 전체 데이터의 80%, 테스트 데이터는 20%로 설정합니다.\n",
    "    total_len = len(X)\n",
    "    train_len = int(total_len * 0.8)\n",
    "\n",
    "    X_train, y_train = X[:train_len], y[:train_len]\n",
    "    X_test, y_test = X[train_len:], y[train_len:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, data_stat\n",
    "\n",
    "def build_rnn_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] SimpleRNN 기반 모델을 구성하세요.\n",
    "    model.add(layers.SimpleRNN(4, input_shape=(window_size, 1)))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "    return model\n",
    "    \n",
    "def plot_result(X_true, y_true, y_pred, data_stat):\n",
    "    # 표준화된 결과를 다시 원래 값으로 변환합니다.\n",
    "    y_true_orig = (y_true * data_stat[\"stdv\"]) + data_stat[\"mean\"]\n",
    "    y_pred_orig = (y_pred * data_stat[\"stdv\"]) + data_stat[\"mean\"]\n",
    "\n",
    "\n",
    "    # 테스트 데이터에서 사용한 날짜들만 가져옵니다.\n",
    "    test_month = data_stat[\"month\"][-len(y_true):]\n",
    "\n",
    "    # 모델의 예측값을 실제값과 함께 그래프로 그립니다.\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(y_true_orig, color=\"b\", label=\"True\")\n",
    "    ax.plot(y_pred_orig, color=\"r\", label=\"Prediction\")\n",
    "    ax.set_xticks(list(range(len(test_month))))\n",
    "    ax.set_xticklabels(test_month, rotation=45)\n",
    "    ax.set_title(\"RNN Result\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.savefig(\"airline_rnn.png\")\n",
    "    elice_utils.send_image(\"airline_rnn.png\")\n",
    "\n",
    "def main(model=None, epochs=10):\n",
    "    tf.random.set_seed(2022)\n",
    "\n",
    "    window_size = 4\n",
    "    X_train, X_test, y_train, y_test, data_stat = load_data(window_size)\n",
    "\n",
    "    if model is None:\n",
    "        model = build_rnn_model(window_size)\n",
    "\n",
    "    # TODO: [지시사항 2번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    # TODO: [지시사항 3번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "\n",
    "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=8, shuffle=True, verbose=2)\n",
    "    \n",
    "    # 테스트 데이터셋으로 모델을 테스트합니다.\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print(\"테스트 MSE: {:.5f}\".format(test_loss))\n",
    "    print()\n",
    "    \n",
    "    # 모델의 예측값과 실제값을 그래프로 그립니다.\n",
    "    y_pred = model.predict(X_test)\n",
    "    plot_result(X_test, y_test, y_pred, data_stat)\n",
    "\n",
    "\n",
    "    return optimizer, hist\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7388c88",
   "metadata": {},
   "source": [
    "# 실습 4: 심층 Vanilla RNN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577de2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "def load_data(num_data, window_size):\n",
    "\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, num_data, 1)\n",
    "    \n",
    "    time = np.linspace(0, 1, window_size + 1)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
    "    series += 0.1 * np.sin((time - offsets2) * (freq2 * 10 + 10))\n",
    "    series += 0.1 * (np.random.rand(num_data, window_size + 1) - 0.5)\n",
    "    \n",
    "    num_train = int(num_data * 0.8)\n",
    "    X_train, y_train = series[:num_train, :window_size], series[:num_train, -1]\n",
    "    X_test, y_test = series[num_train:, :window_size], series[num_train:, -1]\n",
    "    \n",
    "    X_train = X_train[:, :, np.newaxis]\n",
    "    X_test = X_test[:, :, np.newaxis]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def build_rnn_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] SimpleRNN 기반 모델을 구성하세요.\n",
    "    model.add(layers.SimpleRNN(20, input_shape=(window_size, 1)))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_deep_rnn_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] 여러개의 SimpleRNN을 가지는 모델을 구성하세요.\n",
    "    model.add(layers.SimpleRNN(20, return_sequences=True, input_shape=(window_size, 1)))\n",
    "    model.add(layers.SimpleRNN(20))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=20, name=None):\n",
    "    # TODO: [지시사항 3번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "\n",
    "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=256, shuffle=True, verbose=2)\n",
    "    \n",
    "    # 테스트 데이터셋으로 모델을 테스트합니다.\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"[{}] 테스트 MSE: {:.5f}\".format(name, test_loss))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    return optimizer, hist\n",
    "    \n",
    "def main():\n",
    "    tf.random.set_seed(2022)\n",
    "    np.random.seed(2022)\n",
    "\n",
    "\n",
    "    window_size = 50\n",
    "    X_train, X_test, y_train, y_test = load_data(10000, window_size)\n",
    "\n",
    "    rnn_model = build_rnn_model(window_size)\n",
    "    run_model(rnn_model, X_train, X_test, y_train, y_test, name=\"RNN\")\n",
    "\n",
    "    deep_rnn_model = build_deep_rnn_model(window_size)\n",
    "    run_model(deep_rnn_model, X_train, X_test, y_train, y_test, name=\"Deep RNN\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceacf32",
   "metadata": {},
   "source": [
    "# 실습 5: Encoder-Decoder 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290be4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, Sequential, Input\n",
    "\n",
    "class EncoderDecoder(Model):\n",
    "    def __init__(self, hidden_dim, encoder_input_shape, decoder_input_shape, num_classes):\n",
    "\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # TODO: [지시사항 1번] SimpleRNN으로 이루어진 Encoder를 정의하세요.\n",
    "        self.encoder = layers.SimpleRNN(hidden_dim,\n",
    "                                        return_state=True,\n",
    "                                        input_shape=encoder_input_shape)\n",
    "                                        \n",
    "        # TODO: [지시사항 2번] SimpleRNN으로 이루어진 Decoder를 정의하세요.\n",
    "        self.decoder = layers.SimpleRNN(hidden_dim,\n",
    "                                        return_sequences=True,\n",
    "                                        input_shape=decoder_input_shape)\n",
    "        \n",
    "        self.dense = layers.Dense(num_classes, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, encoder_inputs, decoder_inputs):\n",
    "        # TODO: [지시사항 3번] Encoder에 입력값을 넣어 Decoder의 초기 state로 사용할 state를 얻어내세요.\n",
    "        _, encoder_state = self.encoder(encoder_inputs)\n",
    "        \n",
    "        # TODO: [지시사항 4번] Decoder에 입력값을 넣고, 초기 state는 Encoder에서 얻어낸 state로 설정하세요.\n",
    "        decoder_outputs = self.decoder(decoder_inputs, initial_state=[encoder_state])\n",
    "        \n",
    "        outputs = self.dense(decoder_outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # hidden state의 크기\n",
    "\n",
    "    hidden_dim = 20\n",
    "    \n",
    "    # Encoder에 들어갈 각 데이터의 모양\n",
    "    encoder_input_shape = (10, 1)\n",
    "    \n",
    "    # Decoder에 들어갈 각 데이터의 모양\n",
    "    decoder_input_shape = (30, 1)\n",
    "    \n",
    "    # 분류한 클래스 개수\n",
    "    num_classes = 5\n",
    "\n",
    "\n",
    "    # Encoder-Decoder 모델을 만듭니다.\n",
    "\n",
    "    model = EncoderDecoder(hidden_dim, encoder_input_shape, decoder_input_shape, num_classes)\n",
    "    \n",
    "    # 모델에 넣어줄 가상의 데이터를 생성합니다.\n",
    "    encoder_x, decoder_x = tf.random.uniform(shape=encoder_input_shape), tf.random.uniform(shape=decoder_input_shape)\n",
    "    encoder_x, decoder_x = tf.expand_dims(encoder_x, axis=0), tf.expand_dims(decoder_x, axis=0)\n",
    "    y = model(encoder_x, decoder_x)\n",
    "\n",
    "\n",
    "    # 모델의 정보를 출력합니다.\n",
    "    model.summary()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
