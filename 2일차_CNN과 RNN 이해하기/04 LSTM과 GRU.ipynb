{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191b617f",
   "metadata": {},
   "source": [
    "# 실습1: 장기 의존성 문제 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6df6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(window_size):\n",
    "    raw_data = pd.read_csv(\"./daily-min-temperatures.csv\")\n",
    "    raw_temps = raw_data[\"Temp\"]\n",
    "\n",
    "    mean_temp = raw_temps.mean()\n",
    "    stdv_temp = raw_temps.std(ddof=0)\n",
    "    raw_temps = (raw_temps - mean_temp) / stdv_temp\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(raw_temps) - window_size):\n",
    "        cur_temps = raw_temps[i:i + window_size]\n",
    "        target = raw_temps[i + window_size]\n",
    "\n",
    "        X.append(list(cur_temps))\n",
    "        y.append(target)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X[:, :, np.newaxis]\n",
    "\n",
    "    total_len = len(X)\n",
    "    train_len = int(total_len * 0.8)\n",
    "\n",
    "    X_train, y_train = X[:train_len], y[:train_len]\n",
    "    X_test, y_test = X[train_len:], y[train_len:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_rnn_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] Simple RNN과 Fully-connected Layer로 구성된 모델을 완성하세요.\n",
    "    model.add(layers.SimpleRNN(128, input_shape=(window_size, 1)))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] LSTM과 Fully-connected Layer로 구성된 모델을 완성하세요.\n",
    "    model.add(layers.LSTM(128, input_shape=(window_size, 1)))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_gru_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 3번] GRU와 Fully-connected Layer로 구성된 모델을 완성하세요.\n",
    "    model.add(layers.GRU(128, input_shape=(window_size, 1)))\n",
    "    model.add(layers.Dense(32, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=10, model_name=None):\n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    \n",
    "    # TODO: [지시사항 5번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = model.fit(X_train, y_train, batch_size=64, epochs=epochs, shuffle=True, verbose=2)\n",
    "\n",
    "\n",
    "    # 테스트 데이터셋으로 모델을 테스트합니다.\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    return test_loss, optimizer, hist\n",
    "\n",
    "def main(window_size):\n",
    "    tf.random.set_seed(2022)\n",
    "    X_train, X_test, y_train, y_test = load_data(window_size)\n",
    "\n",
    "    rnn_model = build_rnn_model(window_size)\n",
    "    lstm_model = build_lstm_model(window_size)\n",
    "    gru_model = build_gru_model(window_size)\n",
    "\n",
    "    rnn_test_loss, _, _ = run_model(rnn_model, X_train, X_test, y_train, y_test, model_name=\"RNN\")\n",
    "    lstm_test_loss, _, _ = run_model(lstm_model, X_train, X_test, y_train, y_test, model_name=\"LSTM\")\n",
    "\n",
    "    gru_test_loss, _, _ = run_model(gru_model, X_train, X_test, y_train, y_test, model_name=\"GRU\")\n",
    "    \n",
    "    return rnn_test_loss, lstm_test_loss, gru_test_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 10일치 데이터를 보고 다음날의 기온을 예측합니다.\n",
    "\n",
    "    rnn_10_test_loss, lstm_10_test_loss, gru_10_test_loss = main(10)\n",
    "    \n",
    "    # 300일치 데이터를 보고 다음날의 기온을 예측합니다.\n",
    "    rnn_300_test_loss, lstm_300_test_loss, gru_300_test_loss = main(300)\n",
    "    \n",
    "    print(\"=\" * 20, \"시계열 길이가 10 인 경우\", \"=\" * 20)\n",
    "    print(\"[RNN ] 테스트 MSE = {:.5f}\".format(rnn_10_test_loss))\n",
    "    print(\"[LSTM] 테스트 MSE = {:.5f}\".format(lstm_10_test_loss))\n",
    "    print(\"[GRU ] 테스트 MSE = {:.5f}\".format(gru_10_test_loss))\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 20, \"시계열 길이가 300 인 경우\", \"=\" * 20)\n",
    "    print(\"[RNN ] 테스트 MSE = {:.5f}\".format(rnn_300_test_loss))\n",
    "    print(\"[LSTM] 테스트 MSE = {:.5f}\".format(lstm_300_test_loss))\n",
    "    print(\"[GRU ] 테스트 MSE = {:.5f}\".format(gru_300_test_loss))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353528b0",
   "metadata": {},
   "source": [
    "# 실습 2: LSTM으로 IMDb 데이터 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elice_utils import EliceUtils\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(num_words, max_len):\n",
    "    # TODO: [지시사항 1번] IMDB 데이터셋을 불러오세요.\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_lstm_model(num_words, embedding_len):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] LSTM 기반 모델을 구성하세요.\n",
    "    model.add(layers.Embedding(num_words, embedding_len))\n",
    "    model.add(layers.LSTM(16))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "    return model\n",
    "    \n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=5):\n",
    "    # TODO: [지시사항 3번] 모델 학습을 위한 optimizer, loss 함수, 평가 지표를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = model.fit(X_train, y_train, batch_size=128, epochs=epochs, shuffle=True, verbose=2)\n",
    "    \n",
    "    # 모델을 테스트 데이터셋으로 테스트합니다.\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print(\"테스트 loss: {:.5f}, 테스트 정확도: {:.3f}%\".format(test_loss, test_acc * 100))\n",
    "    \n",
    "    return optimizer, hist\n",
    "\n",
    "\n",
    "def main():\n",
    "    tf.random.set_seed(2022)\n",
    "\n",
    "    num_words = 6000\n",
    "    max_len =  130\n",
    "    embedding_len = 100\n",
    "\n",
    "    X_train, X_test, y_train, y_test = load_data(num_words, max_len)\n",
    "\n",
    "    model = build_lstm_model(num_words, embedding_len)\n",
    "    run_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad53e1",
   "metadata": {},
   "source": [
    "# 실습3: GRU를 통한 항공 승객 수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e02c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elice_utils import EliceUtils\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(window_size):\n",
    "    raw_data = pd.read_csv(\"./airline-passengers.csv\")\n",
    "    raw_passengers = raw_data[\"Passengers\"].to_numpy()\n",
    "\n",
    "    # 데이터의 평균과 표준편차 값으로 정규화(표준화) 합니다.\n",
    "    mean_passenger = raw_passengers.mean()\n",
    "    stdv_passenger = raw_passengers.std(ddof=0)\n",
    "    raw_passengers = (raw_passengers - mean_passenger) / stdv_passenger\n",
    "    plot_data = {\"month\": raw_data[\"Month\"], \"mean\": mean_passenger, \"stdv\": stdv_passenger}\n",
    "\n",
    "    # window_size 개의 데이터를 불러와 입력 데이터(X)로 설정하고\n",
    "    # window_size보다 한 시점 뒤의 데이터를 예측할 대상(y)으로 설정하여\n",
    "    # 데이터셋을 구성합니다.\n",
    "    X, y = [], []\n",
    "    for i in range(len(raw_passengers) - window_size):\n",
    "        cur_passenger = raw_passengers[i:i + window_size]\n",
    "        target = raw_passengers[i + window_size]\n",
    "\n",
    "        X.append(list(cur_passenger))\n",
    "        y.append(target)\n",
    "\n",
    "    # X와 y를 numpy array로 변환합니다.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # 각 입력 데이터는 sequence 길이가 window_size이고, featuer 개수는 1개가 되도록\n",
    "    # 마지막에 새로운 차원을 추가합니다.\n",
    "    # 즉, (전체 데이터 개수, window_size) -> (전체 데이터 개수, window_size, 1)이 되도록 변환합니다.\n",
    "    X = X[:, :, np.newaxis]\n",
    "\n",
    "    # 학습 데이터는 전체 데이터의 80%, 테스트 데이터는 20%로 설정합니다.\n",
    "    total_len = len(X)\n",
    "    train_len = int(total_len * 0.8)\n",
    "\n",
    "    X_train, y_train = X[:train_len], y[:train_len]\n",
    "    X_test, y_test = X[train_len:], y[train_len:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, plot_data\n",
    "\n",
    "def build_gru_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] GRU 기반 모델을 구성하세요.\n",
    "    model.add(layers.GRU(4, input_shape=(window_size, 1)))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_rnn_model(window_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] SimpleRNN 기반 모델을 구성하세요.\n",
    "    model.add(layers.SimpleRNN(4, input_shape=(window_size, 1)))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=100, name=None):\n",
    "    # TODO: [지시사항 3번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = model.fit(X_train, y_train, batch_size=8, epochs=epochs, shuffle=True, verbose=2)\n",
    "\n",
    "    # 테스트 데이터셋으로 모델을 테스트합니다.\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print(\"테스트 MSE: {:.5f}\".format(test_loss))\n",
    "    print()\n",
    "\n",
    "    return optimizer, hist\n",
    "\n",
    "def plot_result(model, X_true, y_true, plot_data, name):\n",
    "    y_pred = model.predict(X_true)\n",
    "\n",
    "    # 표준화된 결과를 다시 원래 값으로 변환합니다.\n",
    "    y_true_orig = (y_true * plot_data[\"stdv\"]) + plot_data[\"mean\"]\n",
    "    y_pred_orig = (y_pred * plot_data[\"stdv\"]) + plot_data[\"mean\"]\n",
    "\n",
    "    # 테스트 데이터에서 사용한 날짜들만 가져옵니다.\n",
    "    test_month = plot_data[\"month\"][-len(y_true):]\n",
    "\n",
    "    # 모델의 예측값을 실제값과 함께 그래프로 그립니다.\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(y_true_orig, color=\"b\", label=\"True\")\n",
    "    ax.plot(y_pred_orig, color=\"r\", label=\"Prediction\")\n",
    "    ax.set_xticks(list(range(len(test_month))))\n",
    "    ax.set_xticklabels(test_month, rotation=45)\n",
    "    ax.set_title(\"{} Result\".format(name))\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.savefig(\"airline_{}.png\".format(name.lower()))\n",
    "\n",
    "def main():\n",
    "    tf.random.set_seed(2022)\n",
    "\n",
    "    window_size = 4\n",
    "    X_train, X_test, y_train, y_test, plot_data = load_data(window_size)\n",
    "\n",
    "    gru_model = build_gru_model(window_size)\n",
    "    run_model(gru_model, X_train, X_test, y_train, y_test, name=\"GRU\")\n",
    "    plot_result(gru_model, X_test, y_test, plot_data, name=\"GRU\")\n",
    "\n",
    "    rnn_model = build_rnn_model(window_size)\n",
    "    run_model(rnn_model, X_train, X_test, y_train, y_test, name=\"RNN\")\n",
    "\n",
    "    plot_result(rnn_model, X_test, y_test, plot_data, name=\"RNN\")\n",
    "    \n",
    "    elice_utils.send_image(\"airline_{}.png\".format(\"gru\"))\n",
    "    elice_utils.send_image(\"airline_{}.png\".format(\"rnn\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fbba8",
   "metadata": {},
   "source": [
    "# 실습4: RNN 기반 모델을 통한 분류 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83112ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(max_len):\n",
    "    data = pd.read_csv(\"./review_score.csv\")\n",
    "    # 리뷰 문장을 입력 데이터로, 해당 리뷰의 평점을 라벨 데이터로 설정합니다.\n",
    "    X = data['Review']\n",
    "    y = data['Score']\n",
    "    y = y - 1 # 값을 1~5에서 0~4로 변경\n",
    "\n",
    "    # 문장 내 각 단어를 숫자로 변환하는 Tokenizer를 적용합니다.\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    # 전체 단어 중에서 가장 큰 숫자로 mapping된 단어의 숫자를 가져옵니다.\n",
    "    # 즉, max_features는 전체 데이터셋에 등장하는 겹치지 않는 단어의 개수 + 1과 동일합니다.\n",
    "    max_features = max([max(_in) for _in in X]) + 1\n",
    "\n",
    "    # 불러온 데이터셋을 학습 데이터 80%, 테스트 데이터 20%로 분리합니다.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # 모든 문장들을 가장 긴 문장의 단어 개수가 되게 padding을 추가합니다.\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, max_features\n",
    "\n",
    "def build_rnn_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] Simple RNN 기반의 모델을 완성하세요.\n",
    "    model.add(layers.Embedding(max_features, embedding_size))\n",
    "    model.add(layers.SimpleRNN(20))\n",
    "    model.add(layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] LSTM 기반의 모델을 완성하세요.\n",
    "    model.add(layers.Embedding(max_features, embedding_size))\n",
    "    model.add(layers.LSTM(20))\n",
    "    model.add(layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_gru_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 3번] GRU 기반의 모델을 완성하세요.\n",
    "    model.add(layers.Embedding(max_features, embedding_size))\n",
    "    model.add(layers.GRU(20))\n",
    "    model.add(layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=10):\n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 optimizer, loss 함수, 평가 지표를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # TODO: [지시사항 5번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = model.fit(X_train, y_train, batch_size=256, epochs=epochs, shuffle=True, verbose=2)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    return test_loss, test_acc, optimizer, hist\n",
    "\n",
    "def main():\n",
    "    tf.random.set_seed(2022)\n",
    "    max_len = 150\n",
    "    embedding_size = 128\n",
    "\n",
    "    X_train, X_test, y_train, y_test, max_features = load_data(max_len)\n",
    "    rnn_model = build_rnn_model(max_features, embedding_size)\n",
    "    lstm_model = build_lstm_model(max_features, embedding_size)\n",
    "    gru_model = build_gru_model(max_features, embedding_size)\n",
    "\n",
    "    rnn_test_loss, rnn_test_acc, _, _ = run_model(rnn_model, X_train, X_test, y_train, y_test)\n",
    "    lstm_test_loss, lstm_test_acc, _, _ = run_model(lstm_model, X_train, X_test, y_train, y_test)\n",
    "    gru_test_loss, gru_test_acc, _, _ = run_model(gru_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 20, \"모델 별 Test Loss와 정확도\", \"=\" * 20)\n",
    "    print(\"[RNN ] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(rnn_test_loss, rnn_test_acc * 100))\n",
    "    print(\"[LSTM] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(lstm_test_loss, lstm_test_acc * 100))\n",
    "    print(\"[GRU ] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(gru_test_loss, gru_test_acc * 100))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b65f7",
   "metadata": {},
   "source": [
    "# 실습 5: RNN 기반 모델을 통한 회귀 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elice_utils import EliceUtils\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def load_data(window_size):\n",
    "\n",
    "    raw_data_df = pd.read_csv(\"./AAPL.csv\", index_col=\"Date\")\n",
    "    \n",
    "    # 데이터 전체를 표준화합니다.\n",
    "    scaler = StandardScaler()\n",
    "    raw_data = scaler.fit_transform(raw_data_df)\n",
    "    plot_data = {\"mean\": scaler.mean_[3], \"var\": scaler.var_[3], \"date\": raw_data_df.index}\n",
    "\n",
    "\n",
    "    # 입력 데이터(X)는 시작가, 일 최고가, 일 최저가, 종가 데이터를 사용하고\n",
    "    # 라벨 데이터(y)는 4번째 컬럼에 해당하는 종가 데이터만 사용합니다.\n",
    "    raw_X = raw_data[:, :4]\n",
    "    raw_y = raw_data[:, 3]\n",
    "\n",
    "    # window_size 개의 데이터를 불러와 입력 데이터(X)로 설정하고\n",
    "    # window_size보다 한 시점 뒤의 데이터를 예측할 대상(y)으로 설정하여\n",
    "    # 데이터셋을 구성합니다.\n",
    "    X, y = [], []\n",
    "    for i in range(len(raw_X) - window_size):\n",
    "        cur_prices = raw_X[i:i + window_size, :]\n",
    "        target = raw_y[i + window_size]\n",
    "\n",
    "        X.append(list(cur_prices))\n",
    "        y.append(target)\n",
    "\n",
    "    # X와 y를 numpy array로 변환합니다.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # 학습 데이터는 전체 데이터의 80%, 테스트 데이터는 20%로 설정합니다.\n",
    "    total_len = len(X)\n",
    "    train_len = int(total_len * 0.8)\n",
    "\n",
    "    X_train, y_train = X[:train_len], y[:train_len]\n",
    "    X_test, y_test = X[train_len:], y[train_len:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, plot_data\n",
    "\n",
    "def build_rnn_model(window_size, num_features):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] SimpleRNN 기반 모델을 구성하세요.\n",
    "    model.add(layers.SimpleRNN(256, input_shape=(window_size, num_features)))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(window_size, num_features):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] LSTM 기반 모델을 구성하세요.\n",
    "    model.add(layers.LSTM(256, input_shape=(window_size, num_features)))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_gru_model(window_size, num_features):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 3번] GRU 기반 모델을 구성하세요.\n",
    "    model.add(layers.GRU(256, input_shape=(window_size, num_features)))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=10, name=None):\n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    # TODO: [지시사항 5번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "\n",
    "    hist = model.fit(X_train, y_train, batch_size=128, epochs=epochs, shuffle=True, verbose=2)\n",
    "    \n",
    "    # 테스트 데이터셋으로 모델을 테스트합니다.\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"[{}] 테스트 loss: {:.5f}\".format(name, test_loss))\n",
    "    print()\n",
    "\n",
    "\n",
    "    return optimizer, hist\n",
    "\n",
    "def plot_result(model, X_true, y_true, plot_data, name):\n",
    "    y_pred = model.predict(X_true)\n",
    "\n",
    "    # 표준화된 결과를 다시 원래 값으로 변환합니다.\n",
    "    y_true_orig = (y_true * np.sqrt(plot_data[\"var\"])) + plot_data[\"mean\"]\n",
    "    y_pred_orig = (y_pred * np.sqrt(plot_data[\"var\"])) + plot_data[\"mean\"]\n",
    "\n",
    "    # 테스트 데이터에서 사용한 날짜들만 가져옵니다.\n",
    "    test_date = plot_data[\"date\"][-len(y_true):]\n",
    "\n",
    "    # 모델의 예측값을 실제값과 함께 그래프로 그립니다.\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(y_true_orig, color=\"b\", label=\"True\")\n",
    "    ax.plot(y_pred_orig, color=\"r\", label=\"Prediction\")\n",
    "    ax.set_xticks(list(range(len(test_date))))\n",
    "    ax.set_xticklabels(test_date, rotation=45)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(100))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(100))\n",
    "    ax.set_title(\"{} Result\".format(name))\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"apple_stock_{}\".format(name.lower()))\n",
    "    \n",
    "    elice_utils.send_image(\"apple_stock_{}.png\".format(name.lower()))\n",
    "\n",
    "\n",
    "def main():\n",
    "    tf.random.set_seed(2022)\n",
    "\n",
    "    window_size = 30\n",
    "    X_train, X_test, y_train, y_test, plot_data = load_data(window_size)\n",
    "    num_features = X_train[0].shape[1]\n",
    "\n",
    "    rnn_model = build_rnn_model(window_size, num_features)\n",
    "    lstm_model = build_lstm_model(window_size, num_features)\n",
    "    gru_model = build_gru_model(window_size, num_features)\n",
    "\n",
    "    run_model(rnn_model, X_train, X_test, y_train, y_test, name=\"RNN\")\n",
    "    run_model(lstm_model, X_train, X_test, y_train, y_test, name=\"LSTM\")\n",
    "    run_model(gru_model, X_train, X_test, y_train, y_test, name=\"GRU\")\n",
    "\n",
    "    plot_result(rnn_model, X_test, y_test, plot_data, name=\"RNN\")\n",
    "    plot_result(lstm_model, X_test, y_test, plot_data, name=\"LSTM\")\n",
    "    plot_result(gru_model, X_test, y_test, plot_data, name=\"GRU\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
