{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d728b508",
   "metadata": {},
   "source": [
    "# [실습6] AI 기법 성능 향상 방법론 (정답)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930bc416",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38471a",
   "metadata": {},
   "source": [
    "## 실습 목표\n",
    "---\n",
    "- 하이퍼 매개변수 튜닝에 대해 배워봅니다.\n",
    "- K-fold 교차 검증을 수행해봅니다.\n",
    "- Residual network를 구현해봅니다.\n",
    "- Positional encoding을 구현해봅니다.\n",
    "- 금속분말 데이터셋에 대한 최적의 인공지능 모델을 구현해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aedf33",
   "metadata": {},
   "source": [
    "## 실습 목차\n",
    "---\n",
    "1. **Hyper-parameter 튜닝:** 하이퍼 매개변수 튜닝을 수행해봅니다.\n",
    "\n",
    "2. **K-fold 교차검증:** K-fold 교차검증으로 모델을 평가해봅니다.\n",
    "\n",
    "3. **Residual network:** Residual network를 구현해봅니다.\n",
    "\n",
    "4. **Positional encoding:** Positional encoding을 구현해봅니다.\n",
    "\n",
    "5. **최적의 모델 구현:** 여태까지 배웠던 것들을 종합하여 최적의 인공지능 모델을 만들어봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8d71f",
   "metadata": {},
   "source": [
    "## 실습 개요\n",
    "---\n",
    "\n",
    "이번 실습에서는 AI 모델의 성능 향상을 위한 다양한 기법을 수행해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5791f7b",
   "metadata": {},
   "source": [
    "## 1. Hyper-parameter 튜닝\n",
    "---\n",
    "금속분말 데이터셋을 이용하여 하이퍼 매개변수 튜닝을 수행해보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc64b3",
   "metadata": {},
   "source": [
    "### 1.1 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e951ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "import json\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8904ed83",
   "metadata": {},
   "source": [
    "### 1.2 데이터셋 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a1fc4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1 = {\n",
    "    'train_X': np.load('./Data/train_data_stage1_X.npy'),\n",
    "    'train_y': np.load('./Data/train_data_stage1_y.npy'),\n",
    "    'valid_X': np.load('./Data/valid_data_stage1_X.npy'),\n",
    "    'valid_y': np.load('./Data/valid_data_stage1_y.npy'),\n",
    "    'test_X': np.load('./Data/test_data_stage1_X.npy'),\n",
    "    'test_y': np.load('./Data/test_data_stage1_y.npy'),\n",
    "}\n",
    "\n",
    "stage2 = {\n",
    "    'train_X': np.load('./Data/train_data_stage2_X.npy'),\n",
    "    'train_y': np.load('./Data/train_data_stage2_y.npy'),\n",
    "    'valid_X': np.load('./Data/valid_data_stage2_X.npy'),\n",
    "    'valid_y': np.load('./Data/valid_data_stage2_y.npy'),\n",
    "    'test_X': np.load('./Data/test_data_stage2_X.npy'),\n",
    "    'test_y': np.load('./Data/test_data_stage2_y.npy'),\n",
    "}\n",
    "\n",
    "# 삭제\n",
    "# columns = json.load(open('./Data/valid_columns.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694374d4",
   "metadata": {},
   "source": [
    "### 1.3 데이터 표준화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c463f",
   "metadata": {},
   "source": [
    "### 1.3.1 Stage1 데이터 표준화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fed86735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 평균: [  11.85294587  205.74478705  951.00255435 1242.437388     72.01273285\n",
      "   72.01162996   70.34571071   11.07603585  408.95062376   81.47648118\n",
      "   75.96962985   12.79330524  566.38390195  202.65058918   68.99620414\n",
      "   69.10054828   73.3958155    13.89662164  226.12474952   76.81680896\n",
      "   59.99898793    9.08815439  205.71384206  425.06943434  202.26546402\n",
      "   78.00841235   78.00453278  345.11532917   13.26721071  246.73680272\n",
      "   74.14375402   65.00868711  108.96768422   84.98831262   80.00354672\n",
      "   15.32405834   23.84402174]\n",
      "출력값 평균: [12.89768652 13.69662267  8.00480187 11.36113325 21.31940678 32.87843839\n",
      "  0.12721732  1.34725458  1.09990652 19.8051213   7.68305334  1.4919701\n",
      "  1.20588607  2.89104562 10.02515553]\n"
     ]
    }
   ],
   "source": [
    "stage1_X_mean = stage1['train_X'].mean(axis = 0)\n",
    "stage1_y_mean = stage1['train_y'].mean(axis = 0)\n",
    "print('입력값 평균:', stage1_X_mean)\n",
    "print('출력값 평균:', stage1_y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8823e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 표준편차: [5.10993143e-01 1.16806618e+01 1.27620111e+02 9.77321235e+01\n",
      " 6.23396785e-02 4.06238919e-01 5.49867530e+00 6.36170724e-01\n",
      " 2.05368437e+01 9.21642493e-01 2.06429784e+00 1.07360805e-01\n",
      " 1.83380514e+01 1.48381842e+01 5.44970706e-02 1.06721153e-01\n",
      " 3.94100055e-01 2.91335938e-02 3.06908107e+00 8.30358591e-01\n",
      " 1.61971729e-01 3.95092636e-01 1.63223059e+01 9.53699574e+00\n",
      " 1.58921785e+01 7.73526529e-02 1.14547773e-01 9.05396328e+00\n",
      " 4.34061012e-01 6.10492128e+00 2.05042127e+00 6.27961350e-02\n",
      " 5.59761035e+00 1.85637963e+01 1.18126539e-01 1.18770739e+00\n",
      " 3.72221796e-01]\n",
      "출력값 표준편차: [0.9270819  0.85503751 6.90580271 1.03664992 2.14313701 3.882343\n",
      " 0.57606676 1.13060617 1.41265541 4.68687927 1.07824517 2.54294205\n",
      " 0.66114934 0.92884134 7.38778856]\n"
     ]
    }
   ],
   "source": [
    "stage1_X_std = stage1['train_X'].std(axis = 0)\n",
    "stage1_y_std = stage1['train_y'].std(axis = 0)\n",
    "print('입력값 표준편차:', stage1_X_std)\n",
    "print('출력값 표준편차:', stage1_y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e11d74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 표준화\n",
    "stage1['train_X'] = (stage1['train_X'] - stage1_X_mean) / stage1_X_std\n",
    "stage1['train_y'] = (stage1['train_y'] - stage1_y_mean) / stage1_y_std\n",
    "# 검증용 데이터 표준화\n",
    "stage1['valid_X'] = (stage1['valid_X'] - stage1_X_mean) / stage1_X_std\n",
    "stage1['valid_y'] = (stage1['valid_y'] - stage1_y_mean) / stage1_y_std\n",
    "# 테스트 데이터 표준화\n",
    "stage1['test_X'] = (stage1['test_X'] - stage1_X_mean) / stage1_X_std\n",
    "stage1['test_y'] = (stage1['test_y'] - stage1_y_mean) / stage1_y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1040b54",
   "metadata": {},
   "source": [
    "### 1.3.2 Stage2 데이터 표준화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "422d8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 평균: [1.28976865e+01 1.36966227e+01 8.00480187e+00 1.13611333e+01\n",
      " 2.13194068e+01 3.28784384e+01 1.27217323e-01 1.34725458e+00\n",
      " 1.09990652e+00 1.98051213e+01 7.68305334e+00 1.49197010e+00\n",
      " 1.20588607e+00 2.89104562e+00 1.00251555e+01 3.60122933e+02\n",
      " 3.60136275e+02 1.72294333e+01 3.22627969e+02 3.09786854e+02\n",
      " 1.87200947e+02 3.09998282e+02 2.89997582e+02 2.69681649e+02\n",
      " 2.42661246e+02 2.44994629e+02 6.34195390e+01 1.54046742e+02\n",
      " 1.53240583e+01 2.38440217e+01]\n",
      "출력값 평균: [11.69212911  6.25614828 10.2499656  19.33555126  2.87697932  2.7475277\n",
      "  0.53310706  2.91750962 18.38898671 11.61424043  7.53420311  3.54181714\n",
      "  7.51699723]\n"
     ]
    }
   ],
   "source": [
    "stage2_X_mean = stage2['train_X'].mean(axis = 0)\n",
    "stage2_y_mean = stage2['train_y'].mean(axis = 0)\n",
    "print('입력값 평균:', stage2_X_mean)\n",
    "print('출력값 평균:', stage2_y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "886998c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 표준편차: [ 0.9270819   0.85503751  6.90580271  1.03664992  2.14313701  3.882343\n",
      "  0.57606676  1.13060617  1.41265541  4.68687927  1.07824517  2.54294205\n",
      "  0.66114934  0.92884134  7.38778856  1.9209727   2.64895901  0.94209921\n",
      "  3.61537985  2.74078134 23.41676809  0.03961971  0.05202987  1.01853204\n",
      "  1.59098238  0.10771334  0.39587306  9.90869048  1.18770739  0.3722218 ]\n",
      "출력값 표준편차: [3.62230517 1.60057596 2.30753588 4.62537631 9.09270122 0.37574164\n",
      " 0.20467783 0.49744294 4.97294003 7.62127826 1.64901449 0.45450424\n",
      " 2.0723342 ]\n"
     ]
    }
   ],
   "source": [
    "stage2_X_std = stage2['train_X'].std(axis = 0)\n",
    "stage2_y_std = stage2['train_y'].std(axis = 0)\n",
    "print('입력값 표준편차:', stage2_X_std)\n",
    "print('출력값 표준편차:', stage2_y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d610f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 표준화\n",
    "stage2['train_X'] = (stage2['train_X'] - stage2_X_mean) / stage2_X_std\n",
    "stage2['train_y'] = (stage2['train_y'] - stage2_y_mean) / stage2_y_std\n",
    "\n",
    "# 검증용 데이터 표준화\n",
    "stage2['valid_X'] = (stage2['valid_X'] - stage2_X_mean) / stage2_X_std\n",
    "stage2['valid_y'] = (stage2['valid_y'] - stage2_y_mean) / stage2_y_std\n",
    "\n",
    "# 테스트 데이터 표준화\n",
    "stage2['test_X'] = (stage2['test_X'] - stage2_X_mean) / stage2_X_std\n",
    "stage2['test_y'] = (stage2['test_y'] - stage2_y_mean) / stage2_y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9857dff",
   "metadata": {},
   "source": [
    "### 1.4 Hyper-parameter 범위 설정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510e00a",
   "metadata": {},
   "source": [
    "랜덤 서치를 이용한 하이퍼파라미터 설정을 해봅니다. 이때 실습 시간을 고려하여, 학습데이터 중 1000개만 사용해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dba0bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.005, 0.03]\n",
    "dropout_rate = [0.0, 0.2]\n",
    "trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32be9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤한 하이퍼파라미터를 리턴하는 함수\n",
    "def sampling(parameter_range):\n",
    "    min_value, max_value = parameter_range\n",
    "    random_value = np.random.random()\n",
    "    return random_value * (max_value - min_value) + min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdcdb9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 시도 - 학습률: 0.019, dropout rate: 0.143\n",
      "    -> R2 score: 0.588637\n",
      "2 번째 시도 - 학습률: 0.015, dropout rate: 0.144\n",
      "    -> R2 score: 0.553708\n",
      "3 번째 시도 - 학습률: 0.016, dropout rate: 0.005\n",
      "    -> R2 score: 0.591842\n",
      "4 번째 시도 - 학습률: 0.019, dropout rate: 0.142\n",
      "    -> R2 score: 0.586992\n",
      "5 번째 시도 - 학습률: 0.029, dropout rate: 0.109\n",
      "    -> R2 score: 0.615365\n",
      "6 번째 시도 - 학습률: 0.011, dropout rate: 0.174\n",
      "    -> R2 score: 0.505968\n",
      "7 번째 시도 - 학습률: 0.027, dropout rate: 0.066\n",
      "    -> R2 score: 0.606369\n",
      "8 번째 시도 - 학습률: 0.007, dropout rate: 0.156\n",
      "    -> R2 score: 0.461730\n",
      "9 번째 시도 - 학습률: 0.027, dropout rate: 0.194\n",
      "    -> R2 score: 0.585974\n",
      "10 번째 시도 - 학습률: 0.005, dropout rate: 0.100\n",
      "    -> R2 score: 0.440979\n"
     ]
    }
   ],
   "source": [
    "lrs = []\n",
    "drs = []\n",
    "r2s = []\n",
    "\n",
    "# 총 10회 반복\n",
    "# 매 반복마다 seed 를 다르게 설정 -> 매 반복마다 random한 값들로 구성됨\n",
    "for try_ in range(trials):\n",
    "    np.random.seed(try_)\n",
    "    \n",
    "    # 0.005부터 0.03의 범위에서 랜덤한 값이 도출됨\n",
    "    lr = sampling(learning_rate)\n",
    "    \n",
    "    # 0.0부터 0.2의 범위에서 랜덤한 값이 도출됨\n",
    "    dr = sampling(dropout_rate)\n",
    "    print('%d 번째 시도 - 학습률: %.3f, dropout rate: %.3f'%(try_ + 1, lr, dr))\n",
    "    \n",
    "    # 모델 정의\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "    MLP_model = tf.keras.Sequential([\n",
    "        Input(shape = stage1['train_X'].shape[1]),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate=dr), \n",
    "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate=dr), \n",
    "        tf.keras.layers.Dense(stage1['train_y'].shape[1])\n",
    "    ])\n",
    "    # 모델 컴파일\n",
    "    MLP_model.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(learning_rate = lr),\n",
    "    )\n",
    "    # 모델 학습\n",
    "    history = MLP_model.fit(stage1['train_X'], stage1['train_y'], epochs = 50, batch_size = 16, verbose = 0)\n",
    "    # 모델 예측\n",
    "    pred = MLP_model.predict(stage1['test_X'])\n",
    "    # 모델 평가\n",
    "    r2 = sklearn.metrics.r2_score(stage1['test_y'], pred)\n",
    "    print(\"    -> R2 score: %f\"%r2)\n",
    "    \n",
    "    # 빈 리스트에 매 반복마다 학습률, dropout rate, r-score 추가\n",
    "    # 마지막에 시각화하기 위함\n",
    "    lrs.append(lr)\n",
    "    drs.append(dr)\n",
    "    r2s.append(r2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3de8c9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(lrs, r2s)\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('r2 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ba4f98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(drs, r2s)\n",
    "plt.xlabel('dropout rate')\n",
    "plt.ylabel('r2 score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26534d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899412206593504"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(lrs, r2s)[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db3f70b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24580392983456553"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(drs, r2s)[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451371c",
   "metadata": {},
   "source": [
    "학습률은 모델의 성능과 상관 관계가 큰 반면, dropout rate는 비교적 상관 관계가 약한 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0d2a7",
   "metadata": {},
   "source": [
    "**[TODO] Stage 2 데이터에 대해 hyper-parameter 튜닝을 수행해보세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2be9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 시도 - 학습률: 0.019, dropout rate: 0.143\n",
      "    -> R2 score: 0.664407\n",
      "2 번째 시도 - 학습률: 0.015, dropout rate: 0.144\n",
      "    -> R2 score: 0.655150\n",
      "3 번째 시도 - 학습률: 0.016, dropout rate: 0.005\n",
      "    -> R2 score: 0.664958\n",
      "4 번째 시도 - 학습률: 0.019, dropout rate: 0.142\n",
      "    -> R2 score: 0.663431\n",
      "5 번째 시도 - 학습률: 0.029, dropout rate: 0.109\n",
      "    -> R2 score: 0.677270\n",
      "6 번째 시도 - 학습률: 0.011, dropout rate: 0.174\n",
      "    -> R2 score: 0.632924\n",
      "7 번째 시도 - 학습률: 0.027, dropout rate: 0.066\n",
      "    -> R2 score: 0.682712\n",
      "8 번째 시도 - 학습률: 0.007, dropout rate: 0.156\n",
      "    -> R2 score: 0.602088\n",
      "9 번째 시도 - 학습률: 0.027, dropout rate: 0.194\n",
      "    -> R2 score: 0.664465\n",
      "10 번째 시도 - 학습률: 0.005, dropout rate: 0.100\n",
      "    -> R2 score: 0.590288\n"
     ]
    }
   ],
   "source": [
    "lrs = []\n",
    "drs = []\n",
    "r2s = []\n",
    "for try_ in range(trials):\n",
    "    np.random.seed(try_)\n",
    "    lr = sampling(learning_rate)\n",
    "    dr = sampling(dropout_rate)\n",
    "    print('%d 번째 시도 - 학습률: %.3f, dropout rate: %.3f'%(try_ + 1, lr, dr))\n",
    "    \n",
    "    # 모델 정의\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "    MLP_model = tf.keras.Sequential([\n",
    "        Input(shape = stage2['train_X'].shape[1]),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate=dr), \n",
    "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate=dr), \n",
    "        tf.keras.layers.Dense(stage2['train_y'].shape[1])\n",
    "    ])\n",
    "    # 모델 컴파일\n",
    "    MLP_model.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(learning_rate = lr),\n",
    "    )\n",
    "    # 모델 학습\n",
    "    history = MLP_model.fit(stage2['train_X'], stage2['train_y'], epochs = 50, batch_size = 16, verbose = 0)\n",
    "    # 모델 예측\n",
    "    pred = MLP_model.predict(stage2['test_X'])\n",
    "    # 모델 평가\n",
    "    r2 = sklearn.metrics.r2_score(stage2['test_y'], pred)\n",
    "    print(\"    -> R2 score: %f\"%r2)\n",
    "    lrs.append(lr)\n",
    "    drs.append(dr)\n",
    "    r2s.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8a4fce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8974428764260832"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(lrs, r2s)[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe3e9c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21867152990530025"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(drs, r2s)[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccff72f",
   "metadata": {},
   "source": [
    "## 2. K-fold 교차 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eeb3b0",
   "metadata": {},
   "source": [
    "### 2.1 데이터 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d8956ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stage1 = np.concatenate([stage1['train_X'], stage1['valid_X']])\n",
    "y_stage1 = np.concatenate([stage1['train_y'], stage1['valid_y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복1\n",
    "# training idxs\n",
    "# 0 ~ 1/5까지\n",
    "# 2/5 ~ 끝까지\n",
    "\n",
    "# valid idxs\n",
    "# 1/5 ~ 2/5 Rkwl\n",
    "\n",
    "# 반복2\n",
    "# training idxs\n",
    "# 0 ~ 2/5까지\n",
    "# 3/5 ~ 끝까지\n",
    "\n",
    "# valid idxs\n",
    "# 2/5 ~ 3/5까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70b39202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold 데이터셋을 도출하는 함수\n",
    "# 매개변수로 독립변수, 종속변수, fold 개수를 받음\n",
    "# k-1개: training, 1개: validation\n",
    "def get_K_fold_dataset(X, y, K):\n",
    "    dataset = {}\n",
    "    len_data = len(X)\n",
    "    idxs = np.arange(len_data)\n",
    "    \n",
    "    # k번 반복수행\n",
    "    for k in range(K):\n",
    "        # training: 데이터 길이의 1/5부터 2/5, 2/5부터 \n",
    "        training_idxs = np.concatenate([idxs[:int(len_data * k/K)], idxs[int(len_data * (k+1)/K):]])\n",
    "        valid_idxs = idxs[int(len_data * k/K) : int(len_data * (k+1)/K)]\n",
    "        dataset['%d-fold'%(k+1)] = {\n",
    "            'train_X': X[training_idxs],\n",
    "            'valid_X': X[valid_idxs],\n",
    "            'train_y': y[training_idxs],\n",
    "            'valid_y': y[valid_idxs]\n",
    "        }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd40cda",
   "metadata": {},
   "source": [
    "데이터를 K 개의 학습-테스트 셋으로 분리하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07ff8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개의 fold\n",
    "# K_fold는 dictionary 안에 dictionary 형태의 데이터셋\n",
    "# 총 5개의 key: '1-fold', '2-fold', '3-fold', '4-fold', '5-fold' 가 있음.\n",
    "# 각각의 key에 대한 value에는 train_X, valid_X, train_y, valid_y라는 또다른 key값이 있음\n",
    "K_fold = get_K_fold_dataset(X_stage1, y_stage1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cea7fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-fold\n",
      "    -> R2 score: 0.484774\n",
      "2-fold\n",
      "    -> R2 score: 0.558955\n",
      "3-fold\n",
      "    -> R2 score: 0.488310\n",
      "4-fold\n",
      "    -> R2 score: 0.536155\n",
      "5-fold\n",
      "    -> R2 score: 0.494331\n"
     ]
    }
   ],
   "source": [
    "r2s = []\n",
    "# 매 반복마다 K에는 각각\n",
    "# train_X, valid_X, train_y, valid_y 이 들어감\n",
    "for K in K_fold.keys():\n",
    "    # 첫번째 반복이라 가정 -> 이때 K는 1-fold\n",
    "    # 따라서 데이터셋은 1-fold 에 해당하는 train_X, valid_X, train_y, valid_y\n",
    "    dataset = K_fold[K]\n",
    "    print('%s'%K)\n",
    "    # 모델 정의\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "    MLP_model = tf.keras.Sequential([\n",
    "        Input(shape = stage1['train_X'].shape[1]),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(stage1['train_y'].shape[1])\n",
    "    ])\n",
    "    # 모델 컴파일\n",
    "    MLP_model.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    "    )\n",
    "    # 모델 학습\n",
    "    history = MLP_model.fit(dataset['train_X'], dataset['train_y'], epochs = 50, batch_size = 16, verbose = 0)\n",
    "    # 모델 예측\n",
    "    pred = MLP_model.predict(dataset['valid_X'])\n",
    "    # 모델 평가\n",
    "    r2 = sklearn.metrics.r2_score(dataset['valid_y'], pred)\n",
    "    print(\"    -> R2 score: %f\"%r2)\n",
    "    r2s.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39dcf80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R2 score: 0.512505\n"
     ]
    }
   ],
   "source": [
    "# K개 fold에서 나온 성능의 평균\n",
    "Average_r2 = np.mean(r2s)\n",
    "print(\"Average R2 score: %f\"%Average_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ccb778",
   "metadata": {},
   "source": [
    "모델의 최종 성능을 평가할 때에는, K개의 fold에서 나온 성능의 평균을 취합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd4e33",
   "metadata": {},
   "source": [
    "**[TODO] Stage2 데이터에 대해 K-fold 교차검증을 수행해보세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f085a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stage2 = np.concatenate([stage2['train_X'], stage2['valid_X']])\n",
    "y_stage2 = np.concatenate([stage2['train_y'], stage2['valid_y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14928130",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_fold = get_K_fold_dataset(X_stage2, y_stage2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6459082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-fold\n",
      "    -> R2 score: 0.649730\n",
      "2-fold\n",
      "    -> R2 score: 0.625591\n",
      "3-fold\n",
      "    -> R2 score: 0.670080\n",
      "4-fold\n",
      "    -> R2 score: 0.631092\n",
      "5-fold\n",
      "    -> R2 score: 0.665117\n"
     ]
    }
   ],
   "source": [
    "r2s = []\n",
    "for K in K_fold.keys():\n",
    "    dataset = K_fold[K]\n",
    "    print('%s'%K)\n",
    "    # 모델 정의\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "    MLP_model = tf.keras.Sequential([\n",
    "        Input(shape = stage2['train_X'].shape[1]),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(stage2['train_y'].shape[1])\n",
    "    ])\n",
    "    # 모델 컴파일\n",
    "    MLP_model.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    "    )\n",
    "    # 모델 학습\n",
    "    history = MLP_model.fit(dataset['train_X'], dataset['train_y'], epochs = 50, batch_size = 16, verbose = 0)\n",
    "    # 모델 예측\n",
    "    pred = MLP_model.predict(dataset['valid_X'])\n",
    "    # 모델 평가\n",
    "    r2 = sklearn.metrics.r2_score(dataset['valid_y'], pred)\n",
    "    print(\"    -> R2 score: %f\"%r2)\n",
    "    r2s.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76767141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R2 score: 0.648322\n"
     ]
    }
   ],
   "source": [
    "Average_r2 = np.mean(r2s)\n",
    "print(\"Average R2 score: %f\"%Average_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101bb67",
   "metadata": {},
   "source": [
    "## 3. Residual network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f48b4",
   "metadata": {},
   "source": [
    "### 3.1 Residual network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d95ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.Model 이라는 부모클래스에서 상속받아서 새로운 클래스 정의\n",
    "# 상속을 받으면 tf.keras.Model 이 갖고 있는 모든 특성 (메소드, 멤버변수 등) 가져다가 사용할 수 있음\n",
    "# 붕어빵 틀\n",
    "\n",
    "# 클래스와 객체\n",
    "# 클래스가 붕어빵 틀이라면 객체는 붕어빵\n",
    "# 생성자는 슈크림 붕어빵인지 팥 붕어빵인지 초기에 세팅해주는 것\n",
    "\n",
    "class ResidualMLP(tf.keras.Model):\n",
    "    # 생성자\n",
    "    # self는 객체의 주소값을 받아옴 -> 내가 빵틀에서 빵을 찍어낼건데, 너는 어떤 빵이니?\n",
    "    # out_dim, use_residual을 생성자로 받아옴\n",
    "    \n",
    "    def __init__(self, out_dim, use_residual):\n",
    "        # 부모클래스의 생성자를 그대로 가져옴 -> Model이 가지고 있는 모든 함수 이용 가능\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        self.use_residual = use_residual\n",
    "        # 총 8개의 fully connected layer\n",
    "        self.fc1 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc4 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc5 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc6 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc7 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc8 = tf.keras.layers.Dense(16, activation = 'relu')\n",
    "        self.fc9 = tf.keras.layers.Dense(out_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # 객체의 use_residual이 True라면 실행\n",
    "        if self.use_residual:\n",
    "            h = self.fc1(x)\n",
    "            h = self.fc2(h) + h\n",
    "            h = self.fc3(h) + h\n",
    "            h = self.fc4(h) + h\n",
    "            h = self.fc5(h) + h\n",
    "            h = self.fc6(h) + h\n",
    "            h = self.fc7(h) + h\n",
    "            h = self.fc8(h) + h\n",
    "            h = self.fc9(h)\n",
    "        else:\n",
    "            h = self.fc1(x)\n",
    "            h = self.fc2(h)\n",
    "            h = self.fc3(h)\n",
    "            h = self.fc4(h)\n",
    "            h = self.fc5(h)\n",
    "            h = self.fc6(h)\n",
    "            h = self.fc7(h)\n",
    "            h = self.fc8(h)\n",
    "            h = self.fc9(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e487f29",
   "metadata": {},
   "source": [
    "Resiudal network 같이 복잡한 네트워크를 정의하기 위해서는 Sequential 보다 위 처럼 직접 class 를 정의하는 것이 편리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "305a36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# 객체 생성 (use_residual 은 True)\n",
    "MLP_model_residual = ResidualMLP(stage1['train_y'].shape[1], use_residual = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3be61",
   "metadata": {},
   "source": [
    "### 3.2 Residual network 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f36a8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model_residual.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c16f3",
   "metadata": {},
   "source": [
    "### 3.3 Residual network 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d362a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "564/564 - 1s - loss: 1.1554\n",
      "Epoch 2/500\n",
      "564/564 - 0s - loss: 0.8017\n",
      "Epoch 3/500\n",
      "564/564 - 0s - loss: 0.7440\n",
      "Epoch 4/500\n",
      "564/564 - 0s - loss: 0.7030\n",
      "Epoch 5/500\n",
      "564/564 - 0s - loss: 0.6750\n",
      "Epoch 6/500\n",
      "564/564 - 0s - loss: 0.6579\n",
      "Epoch 7/500\n",
      "564/564 - 0s - loss: 0.6429\n",
      "Epoch 8/500\n",
      "564/564 - 0s - loss: 0.6279\n",
      "Epoch 9/500\n",
      "564/564 - 0s - loss: 0.6123\n",
      "Epoch 10/500\n",
      "564/564 - 0s - loss: 0.5989\n",
      "Epoch 11/500\n",
      "564/564 - 0s - loss: 0.5862\n",
      "Epoch 12/500\n",
      "564/564 - 0s - loss: 0.5720\n",
      "Epoch 13/500\n",
      "564/564 - 0s - loss: 0.5610\n",
      "Epoch 14/500\n",
      "564/564 - 0s - loss: 0.5466\n",
      "Epoch 15/500\n",
      "564/564 - 0s - loss: 0.5364\n",
      "Epoch 16/500\n",
      "564/564 - 0s - loss: 0.5268\n",
      "Epoch 17/500\n",
      "564/564 - 0s - loss: 0.5291\n",
      "Epoch 18/500\n",
      "564/564 - 0s - loss: 0.5158\n",
      "Epoch 19/500\n",
      "564/564 - 0s - loss: 0.4923\n",
      "Epoch 20/500\n",
      "564/564 - 0s - loss: 0.4986\n",
      "Epoch 21/500\n",
      "564/564 - 0s - loss: 0.5045\n",
      "Epoch 22/500\n",
      "564/564 - 0s - loss: 0.4801\n",
      "Epoch 23/500\n",
      "564/564 - 0s - loss: 0.4866\n",
      "Epoch 24/500\n",
      "564/564 - 0s - loss: 0.4699\n",
      "Epoch 25/500\n",
      "564/564 - 0s - loss: 0.4672\n",
      "Epoch 26/500\n",
      "564/564 - 0s - loss: 0.4556\n",
      "Epoch 27/500\n",
      "564/564 - 0s - loss: 0.4524\n",
      "Epoch 28/500\n",
      "564/564 - 0s - loss: 0.4608\n",
      "Epoch 29/500\n",
      "564/564 - 0s - loss: 0.4499\n",
      "Epoch 30/500\n",
      "564/564 - 0s - loss: 0.4292\n",
      "Epoch 31/500\n",
      "564/564 - 0s - loss: 0.4510\n",
      "Epoch 32/500\n",
      "564/564 - 0s - loss: 0.4348\n",
      "Epoch 33/500\n",
      "564/564 - 0s - loss: 0.4161\n",
      "Epoch 34/500\n",
      "564/564 - 0s - loss: 0.4338\n",
      "Epoch 35/500\n",
      "564/564 - 0s - loss: 0.4334\n",
      "Epoch 36/500\n",
      "564/564 - 0s - loss: 0.4463\n",
      "Epoch 37/500\n",
      "564/564 - 0s - loss: 0.5285\n",
      "Epoch 38/500\n",
      "564/564 - 0s - loss: 0.4908\n",
      "Epoch 39/500\n",
      "564/564 - 0s - loss: 0.4917\n",
      "Epoch 40/500\n",
      "564/564 - 0s - loss: 0.4789\n",
      "Epoch 41/500\n",
      "564/564 - 0s - loss: 0.4735\n",
      "Epoch 42/500\n",
      "564/564 - 0s - loss: 0.4745\n",
      "Epoch 43/500\n",
      "564/564 - 0s - loss: 0.4645\n",
      "Epoch 44/500\n",
      "564/564 - 0s - loss: 0.4702\n",
      "Epoch 45/500\n",
      "564/564 - 0s - loss: 0.4586\n",
      "Epoch 46/500\n",
      "564/564 - 0s - loss: 0.4611\n",
      "Epoch 47/500\n",
      "564/564 - 0s - loss: 0.4575\n",
      "Epoch 48/500\n",
      "564/564 - 0s - loss: 0.4515\n",
      "Epoch 49/500\n",
      "564/564 - 0s - loss: 0.4478\n",
      "Epoch 50/500\n",
      "564/564 - 0s - loss: 0.4525\n",
      "Epoch 51/500\n",
      "564/564 - 0s - loss: 0.4381\n",
      "Epoch 52/500\n",
      "564/564 - 0s - loss: 0.4358\n",
      "Epoch 53/500\n",
      "564/564 - 0s - loss: 0.4286\n",
      "Epoch 54/500\n",
      "564/564 - 0s - loss: 0.4312\n",
      "Epoch 55/500\n",
      "564/564 - 0s - loss: 0.4376\n",
      "Epoch 56/500\n",
      "564/564 - 0s - loss: 0.4213\n",
      "Epoch 57/500\n",
      "564/564 - 0s - loss: 0.4157\n",
      "Epoch 58/500\n",
      "564/564 - 0s - loss: 0.4267\n",
      "Epoch 59/500\n",
      "564/564 - 0s - loss: 0.3979\n",
      "Epoch 60/500\n",
      "564/564 - 0s - loss: 0.4029\n",
      "Epoch 61/500\n",
      "564/564 - 0s - loss: 0.4065\n",
      "Epoch 62/500\n",
      "564/564 - 0s - loss: 0.3938\n",
      "Epoch 63/500\n",
      "564/564 - 0s - loss: 0.4081\n",
      "Epoch 64/500\n",
      "564/564 - 0s - loss: 0.3827\n",
      "Epoch 65/500\n",
      "564/564 - 0s - loss: 0.3901\n",
      "Epoch 66/500\n",
      "564/564 - 0s - loss: 0.3894\n",
      "Epoch 67/500\n",
      "564/564 - 0s - loss: 0.3880\n",
      "Epoch 68/500\n",
      "564/564 - 0s - loss: 0.3711\n",
      "Epoch 69/500\n",
      "564/564 - 0s - loss: 0.3729\n",
      "Epoch 70/500\n",
      "564/564 - 0s - loss: 0.3686\n",
      "Epoch 71/500\n",
      "564/564 - 0s - loss: 0.3650\n",
      "Epoch 72/500\n",
      "564/564 - 0s - loss: 0.3656\n",
      "Epoch 73/500\n",
      "564/564 - 0s - loss: 0.3594\n",
      "Epoch 74/500\n",
      "564/564 - 0s - loss: 0.3612\n",
      "Epoch 75/500\n",
      "564/564 - 0s - loss: 0.3613\n",
      "Epoch 76/500\n",
      "564/564 - 0s - loss: 0.3573\n",
      "Epoch 77/500\n",
      "564/564 - 0s - loss: 0.3550\n",
      "Epoch 78/500\n",
      "564/564 - 0s - loss: 0.3592\n",
      "Epoch 79/500\n",
      "564/564 - 0s - loss: 0.3706\n",
      "Epoch 80/500\n",
      "564/564 - 0s - loss: 0.3616\n",
      "Epoch 81/500\n",
      "564/564 - 0s - loss: 0.3710\n",
      "Epoch 82/500\n",
      "564/564 - 0s - loss: 0.3565\n",
      "Epoch 83/500\n",
      "564/564 - 0s - loss: 0.3515\n",
      "Epoch 84/500\n",
      "564/564 - 0s - loss: 0.3509\n",
      "Epoch 85/500\n",
      "564/564 - 0s - loss: 0.3471\n",
      "Epoch 86/500\n",
      "564/564 - 0s - loss: 0.3473\n",
      "Epoch 87/500\n",
      "564/564 - 0s - loss: 0.3480\n",
      "Epoch 88/500\n",
      "564/564 - 0s - loss: 0.3474\n",
      "Epoch 89/500\n",
      "564/564 - 0s - loss: 0.3521\n",
      "Epoch 90/500\n",
      "564/564 - 0s - loss: 0.3577\n",
      "Epoch 91/500\n",
      "564/564 - 0s - loss: 0.3490\n",
      "Epoch 92/500\n",
      "564/564 - 0s - loss: 0.3451\n",
      "Epoch 93/500\n",
      "564/564 - 0s - loss: 0.3452\n",
      "Epoch 94/500\n",
      "564/564 - 0s - loss: 0.3458\n",
      "Epoch 95/500\n",
      "564/564 - 0s - loss: 0.3427\n",
      "Epoch 96/500\n",
      "564/564 - 0s - loss: 0.3427\n",
      "Epoch 97/500\n",
      "564/564 - 0s - loss: 0.3420\n",
      "Epoch 98/500\n",
      "564/564 - 0s - loss: 0.3658\n",
      "Epoch 99/500\n",
      "564/564 - 0s - loss: 0.3543\n",
      "Epoch 100/500\n",
      "564/564 - 0s - loss: 0.3501\n",
      "Epoch 101/500\n",
      "564/564 - 0s - loss: 0.3499\n",
      "Epoch 102/500\n",
      "564/564 - 0s - loss: 0.3445\n",
      "Epoch 103/500\n",
      "564/564 - 0s - loss: 0.3442\n",
      "Epoch 104/500\n",
      "564/564 - 0s - loss: 0.3444\n",
      "Epoch 105/500\n",
      "564/564 - 0s - loss: 0.3425\n",
      "Epoch 106/500\n",
      "564/564 - 0s - loss: 0.3414\n",
      "Epoch 107/500\n",
      "564/564 - 0s - loss: 0.3398\n",
      "Epoch 108/500\n",
      "564/564 - 0s - loss: 0.3373\n",
      "Epoch 109/500\n",
      "564/564 - 0s - loss: 0.3372\n",
      "Epoch 110/500\n",
      "564/564 - 0s - loss: 0.3369\n",
      "Epoch 111/500\n",
      "564/564 - 0s - loss: 0.3400\n",
      "Epoch 112/500\n",
      "564/564 - 0s - loss: 0.3424\n",
      "Epoch 113/500\n",
      "564/564 - 0s - loss: 0.3374\n",
      "Epoch 114/500\n",
      "564/564 - 0s - loss: 0.3373\n",
      "Epoch 115/500\n",
      "564/564 - 0s - loss: 0.3353\n",
      "Epoch 116/500\n",
      "564/564 - 0s - loss: 0.3349\n",
      "Epoch 117/500\n",
      "564/564 - 0s - loss: 0.3336\n",
      "Epoch 118/500\n",
      "564/564 - 0s - loss: 0.3339\n",
      "Epoch 119/500\n",
      "564/564 - 0s - loss: 0.3344\n",
      "Epoch 120/500\n",
      "564/564 - 0s - loss: 0.3343\n",
      "Epoch 121/500\n",
      "564/564 - 0s - loss: 0.3327\n",
      "Epoch 122/500\n",
      "564/564 - 0s - loss: 0.3326\n",
      "Epoch 123/500\n",
      "564/564 - 0s - loss: 0.3324\n",
      "Epoch 124/500\n",
      "564/564 - 0s - loss: 0.3312\n",
      "Epoch 125/500\n",
      "564/564 - 0s - loss: 0.3345\n",
      "Epoch 126/500\n",
      "564/564 - 0s - loss: 0.3300\n",
      "Epoch 127/500\n",
      "564/564 - 0s - loss: 0.3317\n",
      "Epoch 128/500\n",
      "564/564 - 0s - loss: 0.3327\n",
      "Epoch 129/500\n",
      "564/564 - 0s - loss: 0.3324\n",
      "Epoch 130/500\n",
      "564/564 - 0s - loss: 0.3313\n",
      "Epoch 131/500\n",
      "564/564 - 0s - loss: 0.3295\n",
      "Epoch 132/500\n",
      "564/564 - 0s - loss: 0.3292\n",
      "Epoch 133/500\n",
      "564/564 - 0s - loss: 0.3287\n",
      "Epoch 134/500\n",
      "564/564 - 0s - loss: 0.3324\n",
      "Epoch 135/500\n",
      "564/564 - 0s - loss: 0.3600\n",
      "Epoch 136/500\n",
      "564/564 - 0s - loss: 0.3671\n",
      "Epoch 137/500\n",
      "564/564 - 0s - loss: 0.3339\n",
      "Epoch 138/500\n",
      "564/564 - 0s - loss: 0.3307\n",
      "Epoch 139/500\n",
      "564/564 - 0s - loss: 0.3295\n",
      "Epoch 140/500\n",
      "564/564 - 0s - loss: 0.3319\n",
      "Epoch 141/500\n",
      "564/564 - 0s - loss: 0.3307\n",
      "Epoch 142/500\n",
      "564/564 - 0s - loss: 0.3287\n",
      "Epoch 143/500\n",
      "564/564 - 0s - loss: 0.3286\n",
      "Epoch 144/500\n",
      "564/564 - 0s - loss: 0.3274\n",
      "Epoch 145/500\n",
      "564/564 - 0s - loss: 0.3271\n",
      "Epoch 146/500\n",
      "564/564 - 0s - loss: 0.3266\n",
      "Epoch 147/500\n",
      "564/564 - 0s - loss: 0.3270\n",
      "Epoch 148/500\n",
      "564/564 - 0s - loss: 0.3268\n",
      "Epoch 149/500\n",
      "564/564 - 0s - loss: 0.3263\n",
      "Epoch 150/500\n",
      "564/564 - 0s - loss: 0.3309\n",
      "Epoch 151/500\n",
      "564/564 - 0s - loss: 0.3255\n",
      "Epoch 152/500\n",
      "564/564 - 0s - loss: 0.3269\n",
      "Epoch 153/500\n",
      "564/564 - 0s - loss: 0.3256\n",
      "Epoch 154/500\n",
      "564/564 - 0s - loss: 0.3247\n",
      "Epoch 155/500\n",
      "564/564 - 0s - loss: 0.3255\n",
      "Epoch 156/500\n",
      "564/564 - 0s - loss: 0.3230\n",
      "Epoch 157/500\n",
      "564/564 - 0s - loss: 0.3254\n",
      "Epoch 158/500\n",
      "564/564 - 0s - loss: 0.3237\n",
      "Epoch 159/500\n",
      "564/564 - 0s - loss: 0.3227\n",
      "Epoch 160/500\n",
      "564/564 - 0s - loss: 0.3230\n",
      "Epoch 161/500\n",
      "564/564 - 0s - loss: 0.3252\n",
      "Epoch 162/500\n",
      "564/564 - 0s - loss: 0.3307\n",
      "Epoch 163/500\n",
      "564/564 - 0s - loss: 0.3236\n",
      "Epoch 164/500\n",
      "564/564 - 0s - loss: 0.3216\n",
      "Epoch 165/500\n",
      "564/564 - 0s - loss: 0.3239\n",
      "Epoch 166/500\n",
      "564/564 - 0s - loss: 0.3223\n",
      "Epoch 167/500\n",
      "564/564 - 0s - loss: 0.3209\n",
      "Epoch 168/500\n",
      "564/564 - 0s - loss: 0.3246\n",
      "Epoch 169/500\n",
      "564/564 - 0s - loss: 0.3197\n",
      "Epoch 170/500\n",
      "564/564 - 0s - loss: 0.3400\n",
      "Epoch 171/500\n",
      "564/564 - 0s - loss: 0.3317\n",
      "Epoch 172/500\n",
      "564/564 - 0s - loss: 0.3356\n",
      "Epoch 173/500\n",
      "564/564 - 0s - loss: 0.3286\n",
      "Epoch 174/500\n",
      "564/564 - 0s - loss: 0.3239\n",
      "Epoch 175/500\n",
      "564/564 - 0s - loss: 0.3189\n",
      "Epoch 176/500\n",
      "564/564 - 0s - loss: 0.3202\n",
      "Epoch 177/500\n",
      "564/564 - 0s - loss: 0.3208\n",
      "Epoch 178/500\n",
      "564/564 - 0s - loss: 0.3206\n",
      "Epoch 179/500\n",
      "564/564 - 0s - loss: 0.3195\n",
      "Epoch 180/500\n",
      "564/564 - 0s - loss: 0.3204\n",
      "Epoch 181/500\n",
      "564/564 - 0s - loss: 0.3179\n",
      "Epoch 182/500\n",
      "564/564 - 0s - loss: 0.3204\n",
      "Epoch 183/500\n",
      "564/564 - 0s - loss: 0.3183\n",
      "Epoch 184/500\n",
      "564/564 - 0s - loss: 0.3175\n",
      "Epoch 185/500\n",
      "564/564 - 0s - loss: 0.3854\n",
      "Epoch 186/500\n",
      "564/564 - 0s - loss: 0.4015\n",
      "Epoch 187/500\n",
      "564/564 - 0s - loss: 0.3500\n",
      "Epoch 188/500\n",
      "564/564 - 0s - loss: 0.3405\n",
      "Epoch 189/500\n",
      "564/564 - 0s - loss: 0.3362\n",
      "Epoch 190/500\n",
      "564/564 - 0s - loss: 0.3206\n",
      "Epoch 191/500\n",
      "564/564 - 0s - loss: 0.3184\n",
      "Epoch 192/500\n",
      "564/564 - 0s - loss: 0.3174\n",
      "Epoch 193/500\n",
      "564/564 - 0s - loss: 0.3177\n",
      "Epoch 194/500\n",
      "564/564 - 0s - loss: 0.3182\n",
      "Epoch 195/500\n",
      "564/564 - 0s - loss: 0.3159\n",
      "Epoch 196/500\n",
      "564/564 - 0s - loss: 0.3159\n",
      "Epoch 197/500\n",
      "564/564 - 0s - loss: 0.3225\n",
      "Epoch 198/500\n",
      "564/564 - 0s - loss: 0.3162\n",
      "Epoch 199/500\n",
      "564/564 - 0s - loss: 0.3152\n",
      "Epoch 200/500\n",
      "564/564 - 0s - loss: 0.3144\n",
      "Epoch 201/500\n",
      "564/564 - 0s - loss: 0.3163\n",
      "Epoch 202/500\n",
      "564/564 - 0s - loss: 0.3162\n",
      "Epoch 203/500\n",
      "564/564 - 0s - loss: 0.3141\n",
      "Epoch 204/500\n",
      "564/564 - 0s - loss: 0.3323\n",
      "Epoch 205/500\n",
      "564/564 - 0s - loss: 0.3192\n",
      "Epoch 206/500\n",
      "564/564 - 0s - loss: 0.3171\n",
      "Epoch 207/500\n",
      "564/564 - 0s - loss: 0.3141\n",
      "Epoch 208/500\n",
      "564/564 - 0s - loss: 0.3159\n",
      "Epoch 209/500\n",
      "564/564 - 0s - loss: 0.3146\n",
      "Epoch 210/500\n",
      "564/564 - 0s - loss: 0.3146\n",
      "Epoch 211/500\n",
      "564/564 - 0s - loss: 0.3137\n",
      "Epoch 212/500\n",
      "564/564 - 0s - loss: 0.3127\n",
      "Epoch 213/500\n",
      "564/564 - 0s - loss: 0.3125\n",
      "Epoch 214/500\n",
      "564/564 - 0s - loss: 0.3133\n",
      "Epoch 215/500\n",
      "564/564 - 0s - loss: 0.3287\n",
      "Epoch 216/500\n",
      "564/564 - 0s - loss: 0.3534\n",
      "Epoch 217/500\n",
      "564/564 - 0s - loss: 0.3170\n",
      "Epoch 218/500\n",
      "564/564 - 0s - loss: 0.3156\n",
      "Epoch 219/500\n",
      "564/564 - 0s - loss: 0.3113\n",
      "Epoch 220/500\n",
      "564/564 - 0s - loss: 0.3135\n",
      "Epoch 221/500\n",
      "564/564 - 0s - loss: 0.3104\n",
      "Epoch 222/500\n",
      "564/564 - 0s - loss: 0.3117\n",
      "Epoch 223/500\n",
      "564/564 - 0s - loss: 0.3110\n",
      "Epoch 224/500\n",
      "564/564 - 0s - loss: 0.3121\n",
      "Epoch 225/500\n",
      "564/564 - 0s - loss: 0.3131\n",
      "Epoch 226/500\n",
      "564/564 - 0s - loss: 0.3098\n",
      "Epoch 227/500\n",
      "564/564 - 0s - loss: 0.3112\n",
      "Epoch 228/500\n",
      "564/564 - 0s - loss: 0.3095\n",
      "Epoch 229/500\n",
      "564/564 - 0s - loss: 0.3125\n",
      "Epoch 230/500\n",
      "564/564 - 0s - loss: 0.3078\n",
      "Epoch 231/500\n",
      "564/564 - 0s - loss: 0.3109\n",
      "Epoch 232/500\n",
      "564/564 - 0s - loss: 0.3091\n",
      "Epoch 233/500\n",
      "564/564 - 0s - loss: 0.3089\n",
      "Epoch 234/500\n",
      "564/564 - 0s - loss: 0.3066\n",
      "Epoch 235/500\n",
      "564/564 - 0s - loss: 0.3107\n",
      "Epoch 236/500\n",
      "564/564 - 0s - loss: 0.3093\n",
      "Epoch 237/500\n",
      "564/564 - 0s - loss: 0.3133\n",
      "Epoch 238/500\n",
      "564/564 - 0s - loss: 0.3071\n",
      "Epoch 239/500\n",
      "564/564 - 0s - loss: 0.3098\n",
      "Epoch 240/500\n",
      "564/564 - 0s - loss: 0.3088\n",
      "Epoch 241/500\n",
      "564/564 - 0s - loss: 0.3072\n",
      "Epoch 242/500\n",
      "564/564 - 0s - loss: 0.3058\n",
      "Epoch 243/500\n",
      "564/564 - 0s - loss: 0.3108\n",
      "Epoch 244/500\n",
      "564/564 - 0s - loss: 0.3104\n",
      "Epoch 245/500\n",
      "564/564 - 0s - loss: 0.3050\n",
      "Epoch 246/500\n",
      "564/564 - 0s - loss: 0.3071\n",
      "Epoch 247/500\n",
      "564/564 - 0s - loss: 0.3076\n",
      "Epoch 248/500\n",
      "564/564 - 0s - loss: 0.3064\n",
      "Epoch 249/500\n",
      "564/564 - 0s - loss: 0.3110\n",
      "Epoch 250/500\n",
      "564/564 - 0s - loss: 0.3219\n",
      "Epoch 251/500\n",
      "564/564 - 0s - loss: 0.3104\n",
      "Epoch 252/500\n",
      "564/564 - 0s - loss: 0.3096\n",
      "Epoch 253/500\n",
      "564/564 - 0s - loss: 0.3297\n",
      "Epoch 254/500\n",
      "564/564 - 0s - loss: 0.3171\n",
      "Epoch 255/500\n",
      "564/564 - 0s - loss: 0.3130\n",
      "Epoch 256/500\n",
      "564/564 - 0s - loss: 0.3063\n",
      "Epoch 257/500\n",
      "564/564 - 0s - loss: 0.3070\n",
      "Epoch 258/500\n",
      "564/564 - 0s - loss: 0.3122\n",
      "Epoch 259/500\n",
      "564/564 - 0s - loss: 0.3572\n",
      "Epoch 260/500\n",
      "564/564 - 0s - loss: 0.3467\n",
      "Epoch 261/500\n",
      "564/564 - 0s - loss: 0.3175\n",
      "Epoch 262/500\n",
      "564/564 - 0s - loss: 0.3080\n",
      "Epoch 263/500\n",
      "564/564 - 0s - loss: 0.3058\n",
      "Epoch 264/500\n",
      "564/564 - 0s - loss: 0.3052\n",
      "Epoch 265/500\n",
      "564/564 - 0s - loss: 0.3061\n",
      "Epoch 266/500\n",
      "564/564 - 0s - loss: 0.3046\n",
      "Epoch 267/500\n",
      "564/564 - 0s - loss: 0.3060\n",
      "Epoch 268/500\n",
      "564/564 - 0s - loss: 0.3068\n",
      "Epoch 269/500\n",
      "564/564 - 0s - loss: 0.3073\n",
      "Epoch 270/500\n",
      "564/564 - 0s - loss: 0.3032\n",
      "Epoch 271/500\n",
      "564/564 - 0s - loss: 0.3060\n",
      "Epoch 272/500\n",
      "564/564 - 0s - loss: 0.3036\n",
      "Epoch 273/500\n",
      "564/564 - 0s - loss: 0.3203\n",
      "Epoch 274/500\n",
      "564/564 - 0s - loss: 0.3746\n",
      "Epoch 275/500\n",
      "564/564 - 0s - loss: 0.3084\n",
      "Epoch 276/500\n",
      "564/564 - 0s - loss: 0.3076\n",
      "Epoch 277/500\n",
      "564/564 - 0s - loss: 0.3075\n",
      "Epoch 278/500\n",
      "564/564 - 0s - loss: 0.3036\n",
      "Epoch 279/500\n",
      "564/564 - 0s - loss: 0.3016\n",
      "Epoch 280/500\n",
      "564/564 - 0s - loss: 0.3042\n",
      "Epoch 281/500\n",
      "564/564 - 0s - loss: 0.3024\n",
      "Epoch 282/500\n",
      "564/564 - 0s - loss: 0.3058\n",
      "Epoch 283/500\n",
      "564/564 - 0s - loss: 0.3050\n",
      "Epoch 284/500\n",
      "564/564 - 0s - loss: 0.3048\n",
      "Epoch 285/500\n",
      "564/564 - 0s - loss: 0.3043\n",
      "Epoch 286/500\n",
      "564/564 - 0s - loss: 0.3007\n",
      "Epoch 287/500\n",
      "564/564 - 0s - loss: 0.3036\n",
      "Epoch 288/500\n",
      "564/564 - 0s - loss: 0.3014\n",
      "Epoch 289/500\n",
      "564/564 - 0s - loss: 0.2979\n",
      "Epoch 290/500\n",
      "564/564 - 0s - loss: 0.3041\n",
      "Epoch 291/500\n",
      "564/564 - 0s - loss: 0.3030\n",
      "Epoch 292/500\n",
      "564/564 - 0s - loss: 0.3538\n",
      "Epoch 293/500\n",
      "564/564 - 0s - loss: 0.3123\n",
      "Epoch 294/500\n",
      "564/564 - 0s - loss: 0.3346\n",
      "Epoch 295/500\n",
      "564/564 - 0s - loss: 0.3151\n",
      "Epoch 296/500\n",
      "564/564 - 0s - loss: 0.3055\n",
      "Epoch 297/500\n",
      "564/564 - 0s - loss: 0.3040\n",
      "Epoch 298/500\n",
      "564/564 - 0s - loss: 0.3050\n",
      "Epoch 299/500\n",
      "564/564 - 0s - loss: 0.3013\n",
      "Epoch 300/500\n",
      "564/564 - 0s - loss: 0.3012\n",
      "Epoch 301/500\n",
      "564/564 - 0s - loss: 0.2999\n",
      "Epoch 302/500\n",
      "564/564 - 0s - loss: 0.3041\n",
      "Epoch 303/500\n",
      "564/564 - 0s - loss: 0.2996\n",
      "Epoch 304/500\n",
      "564/564 - 0s - loss: 0.2996\n",
      "Epoch 305/500\n",
      "564/564 - 0s - loss: 0.2995\n",
      "Epoch 306/500\n",
      "564/564 - 0s - loss: 0.3005\n",
      "Epoch 307/500\n",
      "564/564 - 0s - loss: 0.3025\n",
      "Epoch 308/500\n",
      "564/564 - 0s - loss: 0.2998\n",
      "Epoch 309/500\n",
      "564/564 - 0s - loss: 0.2998\n",
      "Epoch 310/500\n",
      "564/564 - 0s - loss: 0.3002\n",
      "Epoch 311/500\n",
      "564/564 - 0s - loss: 0.3080\n",
      "Epoch 312/500\n",
      "564/564 - 0s - loss: 0.3067\n",
      "Epoch 313/500\n",
      "564/564 - 0s - loss: 0.3028\n",
      "Epoch 314/500\n",
      "564/564 - 0s - loss: 0.2990\n",
      "Epoch 315/500\n",
      "564/564 - 0s - loss: 0.3004\n",
      "Epoch 316/500\n",
      "564/564 - 0s - loss: 0.2997\n",
      "Epoch 317/500\n",
      "564/564 - 0s - loss: 0.3012\n",
      "Epoch 318/500\n",
      "564/564 - 0s - loss: 0.2991\n",
      "Epoch 319/500\n",
      "564/564 - 0s - loss: 0.3015\n",
      "Epoch 320/500\n",
      "564/564 - 0s - loss: 0.3018\n",
      "Epoch 321/500\n",
      "564/564 - 0s - loss: 0.3565\n",
      "Epoch 322/500\n",
      "564/564 - 0s - loss: 0.3486\n",
      "Epoch 323/500\n",
      "564/564 - 0s - loss: 0.3081\n",
      "Epoch 324/500\n",
      "564/564 - 0s - loss: 0.2996\n",
      "Epoch 325/500\n",
      "564/564 - 0s - loss: 0.3000\n",
      "Epoch 326/500\n",
      "564/564 - 0s - loss: 0.2988\n",
      "Epoch 327/500\n",
      "564/564 - 0s - loss: 0.3012\n",
      "Epoch 328/500\n",
      "564/564 - 0s - loss: 0.3002\n",
      "Epoch 329/500\n",
      "564/564 - 0s - loss: 0.2985\n",
      "Epoch 330/500\n",
      "564/564 - 0s - loss: 0.2969\n",
      "Epoch 331/500\n",
      "564/564 - 0s - loss: 0.2979\n",
      "Epoch 332/500\n",
      "564/564 - 0s - loss: 0.2997\n",
      "Epoch 333/500\n",
      "564/564 - 0s - loss: 0.2994\n",
      "Epoch 334/500\n",
      "564/564 - 0s - loss: 0.3015\n",
      "Epoch 335/500\n",
      "564/564 - 0s - loss: 0.3006\n",
      "Epoch 336/500\n",
      "564/564 - 0s - loss: 0.2973\n",
      "Epoch 337/500\n",
      "564/564 - 0s - loss: 0.3009\n",
      "Epoch 338/500\n",
      "564/564 - 0s - loss: 0.2969\n",
      "Epoch 339/500\n",
      "564/564 - 0s - loss: 0.2961\n",
      "Epoch 340/500\n",
      "564/564 - 0s - loss: 0.2974\n",
      "Epoch 341/500\n",
      "564/564 - 0s - loss: 0.2970\n",
      "Epoch 342/500\n",
      "564/564 - 0s - loss: 0.2977\n",
      "Epoch 343/500\n",
      "564/564 - 0s - loss: 0.2994\n",
      "Epoch 344/500\n",
      "564/564 - 0s - loss: 0.2960\n",
      "Epoch 345/500\n",
      "564/564 - 0s - loss: 0.3013\n",
      "Epoch 346/500\n",
      "564/564 - 0s - loss: 0.2993\n",
      "Epoch 347/500\n",
      "564/564 - 0s - loss: 0.2994\n",
      "Epoch 348/500\n",
      "564/564 - 0s - loss: 0.3009\n",
      "Epoch 349/500\n",
      "564/564 - 0s - loss: 0.2962\n",
      "Epoch 350/500\n",
      "564/564 - 0s - loss: 0.2968\n",
      "Epoch 351/500\n",
      "564/564 - 0s - loss: 0.2935\n",
      "Epoch 352/500\n",
      "564/564 - 0s - loss: 0.2943\n",
      "Epoch 353/500\n",
      "564/564 - 0s - loss: 0.2999\n",
      "Epoch 354/500\n",
      "564/564 - 0s - loss: 0.2947\n",
      "Epoch 355/500\n",
      "564/564 - 0s - loss: 0.3006\n",
      "Epoch 356/500\n",
      "564/564 - 0s - loss: 0.2940\n",
      "Epoch 357/500\n",
      "564/564 - 0s - loss: 0.2943\n",
      "Epoch 358/500\n",
      "564/564 - 0s - loss: 0.2962\n",
      "Epoch 359/500\n",
      "564/564 - 0s - loss: 0.2941\n",
      "Epoch 360/500\n",
      "564/564 - 0s - loss: 0.3009\n",
      "Epoch 361/500\n",
      "564/564 - 0s - loss: 0.2970\n",
      "Epoch 362/500\n",
      "564/564 - 0s - loss: 0.2937\n",
      "Epoch 363/500\n",
      "564/564 - 0s - loss: 0.2948\n",
      "Epoch 364/500\n",
      "564/564 - 0s - loss: 0.2984\n",
      "Epoch 365/500\n",
      "564/564 - 0s - loss: 0.2946\n",
      "Epoch 366/500\n",
      "564/564 - 0s - loss: 0.2952\n",
      "Epoch 367/500\n",
      "564/564 - 0s - loss: 0.2919\n",
      "Epoch 368/500\n",
      "564/564 - 0s - loss: 0.2950\n",
      "Epoch 369/500\n",
      "564/564 - 0s - loss: 0.2946\n",
      "Epoch 370/500\n",
      "564/564 - 0s - loss: 0.2926\n",
      "Epoch 371/500\n",
      "564/564 - 0s - loss: 0.3002\n",
      "Epoch 372/500\n",
      "564/564 - 0s - loss: 0.2948\n",
      "Epoch 373/500\n",
      "564/564 - 0s - loss: 0.2997\n",
      "Epoch 374/500\n",
      "564/564 - 0s - loss: 0.2946\n",
      "Epoch 375/500\n",
      "564/564 - 0s - loss: 0.2957\n",
      "Epoch 376/500\n",
      "564/564 - 0s - loss: 0.2937\n",
      "Epoch 377/500\n",
      "564/564 - 0s - loss: 0.2946\n",
      "Epoch 378/500\n",
      "564/564 - 0s - loss: 0.2921\n",
      "Epoch 379/500\n",
      "564/564 - 0s - loss: 0.2969\n",
      "Epoch 380/500\n",
      "564/564 - 0s - loss: 0.2950\n",
      "Epoch 381/500\n",
      "564/564 - 0s - loss: 0.2935\n",
      "Epoch 382/500\n",
      "564/564 - 0s - loss: 0.2925\n",
      "Epoch 383/500\n",
      "564/564 - 0s - loss: 0.2930\n",
      "Epoch 384/500\n",
      "564/564 - 0s - loss: 0.2986\n",
      "Epoch 385/500\n",
      "564/564 - 0s - loss: 0.2939\n",
      "Epoch 386/500\n",
      "564/564 - 0s - loss: 0.2945\n",
      "Epoch 387/500\n",
      "564/564 - 0s - loss: 0.2948\n",
      "Epoch 388/500\n",
      "564/564 - 0s - loss: 0.2929\n",
      "Epoch 389/500\n",
      "564/564 - 0s - loss: 0.2955\n",
      "Epoch 390/500\n",
      "564/564 - 0s - loss: 0.2937\n",
      "Epoch 391/500\n",
      "564/564 - 0s - loss: 0.2925\n",
      "Epoch 392/500\n",
      "564/564 - 0s - loss: 0.2969\n",
      "Epoch 393/500\n",
      "564/564 - 0s - loss: 0.2933\n",
      "Epoch 394/500\n",
      "564/564 - 0s - loss: 0.2965\n",
      "Epoch 395/500\n",
      "564/564 - 0s - loss: 0.2919\n",
      "Epoch 396/500\n",
      "564/564 - 0s - loss: 0.2948\n",
      "Epoch 397/500\n",
      "564/564 - 0s - loss: 0.2926\n",
      "Epoch 398/500\n",
      "564/564 - 0s - loss: 0.2911\n",
      "Epoch 399/500\n",
      "564/564 - 0s - loss: 0.2938\n",
      "Epoch 400/500\n",
      "564/564 - 0s - loss: 0.2929\n",
      "Epoch 401/500\n",
      "564/564 - 0s - loss: 0.2923\n",
      "Epoch 402/500\n",
      "564/564 - 0s - loss: 0.2927\n",
      "Epoch 403/500\n",
      "564/564 - 0s - loss: 0.2945\n",
      "Epoch 404/500\n",
      "564/564 - 0s - loss: 0.3147\n",
      "Epoch 405/500\n",
      "564/564 - 0s - loss: 0.3047\n",
      "Epoch 406/500\n",
      "564/564 - 0s - loss: 0.3027\n",
      "Epoch 407/500\n",
      "564/564 - 0s - loss: 0.2911\n",
      "Epoch 408/500\n",
      "564/564 - 0s - loss: 0.2990\n",
      "Epoch 409/500\n",
      "564/564 - 0s - loss: 0.2958\n",
      "Epoch 410/500\n",
      "564/564 - 0s - loss: 0.2930\n",
      "Epoch 411/500\n",
      "564/564 - 0s - loss: 0.2943\n",
      "Epoch 412/500\n",
      "564/564 - 0s - loss: 0.2923\n",
      "Epoch 413/500\n",
      "564/564 - 0s - loss: 0.2905\n",
      "Epoch 414/500\n",
      "564/564 - 0s - loss: 0.2934\n",
      "Epoch 415/500\n",
      "564/564 - 0s - loss: 0.2906\n",
      "Epoch 416/500\n",
      "564/564 - 0s - loss: 0.2936\n",
      "Epoch 417/500\n",
      "564/564 - 0s - loss: 0.2883\n",
      "Epoch 418/500\n",
      "564/564 - 0s - loss: 0.2913\n",
      "Epoch 419/500\n",
      "564/564 - 0s - loss: 0.2933\n",
      "Epoch 420/500\n",
      "564/564 - 0s - loss: 0.2902\n",
      "Epoch 421/500\n",
      "564/564 - 0s - loss: 0.2932\n",
      "Epoch 422/500\n",
      "564/564 - 0s - loss: 0.2980\n",
      "Epoch 423/500\n",
      "564/564 - 0s - loss: 0.2966\n",
      "Epoch 424/500\n",
      "564/564 - 0s - loss: 0.2958\n",
      "Epoch 425/500\n",
      "564/564 - 0s - loss: 0.2902\n",
      "Epoch 426/500\n",
      "564/564 - 0s - loss: 0.2932\n",
      "Epoch 427/500\n",
      "564/564 - 0s - loss: 0.2992\n",
      "Epoch 428/500\n",
      "564/564 - 0s - loss: 0.2934\n",
      "Epoch 429/500\n",
      "564/564 - 0s - loss: 0.2941\n",
      "Epoch 430/500\n",
      "564/564 - 0s - loss: 0.2886\n",
      "Epoch 431/500\n",
      "564/564 - 0s - loss: 0.2880\n",
      "Epoch 432/500\n",
      "564/564 - 0s - loss: 0.2917\n",
      "Epoch 433/500\n",
      "564/564 - 0s - loss: 0.2941\n",
      "Epoch 434/500\n",
      "564/564 - 0s - loss: 0.2941\n",
      "Epoch 435/500\n",
      "564/564 - 0s - loss: 0.2860\n",
      "Epoch 436/500\n",
      "564/564 - 0s - loss: 0.2869\n",
      "Epoch 437/500\n",
      "564/564 - 0s - loss: 0.2922\n",
      "Epoch 438/500\n",
      "564/564 - 0s - loss: 0.2903\n",
      "Epoch 439/500\n",
      "564/564 - 0s - loss: 0.2920\n",
      "Epoch 440/500\n",
      "564/564 - 0s - loss: 0.2873\n",
      "Epoch 441/500\n",
      "564/564 - 0s - loss: 0.2890\n",
      "Epoch 442/500\n",
      "564/564 - 0s - loss: 0.2975\n",
      "Epoch 443/500\n",
      "564/564 - 0s - loss: 0.2890\n",
      "Epoch 444/500\n",
      "564/564 - 0s - loss: 0.2884\n",
      "Epoch 445/500\n",
      "564/564 - 0s - loss: 0.2885\n",
      "Epoch 446/500\n",
      "564/564 - 0s - loss: 0.2916\n",
      "Epoch 447/500\n",
      "564/564 - 0s - loss: 0.2866\n",
      "Epoch 448/500\n",
      "564/564 - 0s - loss: 0.3000\n",
      "Epoch 449/500\n",
      "564/564 - 0s - loss: 0.2876\n",
      "Epoch 450/500\n",
      "564/564 - 0s - loss: 0.2874\n",
      "Epoch 451/500\n",
      "564/564 - 0s - loss: 0.2897\n",
      "Epoch 452/500\n",
      "564/564 - 0s - loss: 0.2872\n",
      "Epoch 453/500\n",
      "564/564 - 0s - loss: 0.2889\n",
      "Epoch 454/500\n",
      "564/564 - 0s - loss: 0.2907\n",
      "Epoch 455/500\n",
      "564/564 - 0s - loss: 0.2924\n",
      "Epoch 456/500\n",
      "564/564 - 0s - loss: 0.2899\n",
      "Epoch 457/500\n",
      "564/564 - 0s - loss: 0.2896\n",
      "Epoch 458/500\n",
      "564/564 - 0s - loss: 0.2879\n",
      "Epoch 459/500\n",
      "564/564 - 0s - loss: 0.2950\n",
      "Epoch 460/500\n",
      "564/564 - 0s - loss: 0.2863\n",
      "Epoch 461/500\n",
      "564/564 - 0s - loss: 0.2910\n",
      "Epoch 462/500\n",
      "564/564 - 0s - loss: 0.3022\n",
      "Epoch 463/500\n",
      "564/564 - 0s - loss: 0.2883\n",
      "Epoch 464/500\n",
      "564/564 - 0s - loss: 0.2872\n",
      "Epoch 465/500\n",
      "564/564 - 0s - loss: 0.2904\n",
      "Epoch 466/500\n",
      "564/564 - 0s - loss: 0.2874\n",
      "Epoch 467/500\n",
      "564/564 - 0s - loss: 0.2857\n",
      "Epoch 468/500\n",
      "564/564 - 0s - loss: 0.2902\n",
      "Epoch 469/500\n",
      "564/564 - 0s - loss: 0.2868\n",
      "Epoch 470/500\n",
      "564/564 - 0s - loss: 0.2872\n",
      "Epoch 471/500\n",
      "564/564 - 0s - loss: 0.2832\n",
      "Epoch 472/500\n",
      "564/564 - 0s - loss: 0.2857\n",
      "Epoch 473/500\n",
      "564/564 - 0s - loss: 0.3003\n",
      "Epoch 474/500\n",
      "564/564 - 0s - loss: 0.3419\n",
      "Epoch 475/500\n",
      "564/564 - 0s - loss: 0.3001\n",
      "Epoch 476/500\n",
      "564/564 - 0s - loss: 0.3119\n",
      "Epoch 477/500\n",
      "564/564 - 0s - loss: 0.2928\n",
      "Epoch 478/500\n",
      "564/564 - 0s - loss: 0.2969\n",
      "Epoch 479/500\n",
      "564/564 - 0s - loss: 0.2866\n",
      "Epoch 480/500\n",
      "564/564 - 0s - loss: 0.2847\n",
      "Epoch 481/500\n",
      "564/564 - 0s - loss: 0.2851\n",
      "Epoch 482/500\n",
      "564/564 - 0s - loss: 0.2882\n",
      "Epoch 483/500\n",
      "564/564 - 0s - loss: 0.2913\n",
      "Epoch 484/500\n",
      "564/564 - 0s - loss: 0.2868\n",
      "Epoch 485/500\n",
      "564/564 - 0s - loss: 0.2850\n",
      "Epoch 486/500\n",
      "564/564 - 0s - loss: 0.2819\n",
      "Epoch 487/500\n",
      "564/564 - 0s - loss: 0.2835\n",
      "Epoch 488/500\n",
      "564/564 - 0s - loss: 0.2881\n",
      "Epoch 489/500\n",
      "564/564 - 0s - loss: 0.2889\n",
      "Epoch 490/500\n",
      "564/564 - 0s - loss: 0.2846\n",
      "Epoch 491/500\n",
      "564/564 - 0s - loss: 0.2842\n",
      "Epoch 492/500\n",
      "564/564 - 0s - loss: 0.2845\n",
      "Epoch 493/500\n",
      "564/564 - 0s - loss: 0.2833\n",
      "Epoch 494/500\n",
      "564/564 - 0s - loss: 0.2868\n",
      "Epoch 495/500\n",
      "564/564 - 0s - loss: 0.2857\n",
      "Epoch 496/500\n",
      "564/564 - 0s - loss: 0.2862\n",
      "Epoch 497/500\n",
      "564/564 - 0s - loss: 0.2825\n",
      "Epoch 498/500\n",
      "564/564 - 0s - loss: 0.2843\n",
      "Epoch 499/500\n",
      "564/564 - 0s - loss: 0.2889\n",
      "Epoch 500/500\n",
      "564/564 - 0s - loss: 0.3252\n"
     ]
    }
   ],
   "source": [
    "history = MLP_model_residual.fit(stage1['train_X'], stage1['train_y'], epochs = 500, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9096dd",
   "metadata": {},
   "source": [
    "### 3.4 Residual network 예측 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7c7ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = MLP_model_residual.predict(stage1['test_X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fe86389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.619701\n"
     ]
    }
   ],
   "source": [
    "r2 = sklearn.metrics.r2_score(stage1['test_y'], pred)\n",
    "print(\"R2 score: %f\"%r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1dc7c",
   "metadata": {},
   "source": [
    "### 3.5 일반 MLP 와의 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad69f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# 객체 생성 (use_residual 은 False)\n",
    "MLP_model = ResidualMLP(stage1['train_y'].shape[1], use_residual = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b771b",
   "metadata": {},
   "source": [
    "use_residual 을 False로 설정하여 일반 MLP 네트워크를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "113876f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "MLP_model.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df0324a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "564/564 - 1s - loss: 0.9989\n",
      "Epoch 2/50\n",
      "564/564 - 0s - loss: 0.9883\n",
      "Epoch 3/50\n",
      "564/564 - 0s - loss: 0.9532\n",
      "Epoch 4/50\n",
      "564/564 - 0s - loss: 0.8913\n",
      "Epoch 5/50\n",
      "564/564 - 0s - loss: 0.8536\n",
      "Epoch 6/50\n",
      "564/564 - 0s - loss: 0.8333\n",
      "Epoch 7/50\n",
      "564/564 - 0s - loss: 0.8171\n",
      "Epoch 8/50\n",
      "564/564 - 0s - loss: 0.8006\n",
      "Epoch 9/50\n",
      "564/564 - 0s - loss: 0.7834\n",
      "Epoch 10/50\n",
      "564/564 - 0s - loss: 0.7641\n",
      "Epoch 11/50\n",
      "564/564 - 0s - loss: 0.7450\n",
      "Epoch 12/50\n",
      "564/564 - 0s - loss: 0.7278\n",
      "Epoch 13/50\n",
      "564/564 - 0s - loss: 0.7129\n",
      "Epoch 14/50\n",
      "564/564 - 0s - loss: 0.6995\n",
      "Epoch 15/50\n",
      "564/564 - 0s - loss: 0.6880\n",
      "Epoch 16/50\n",
      "564/564 - 0s - loss: 0.6788\n",
      "Epoch 17/50\n",
      "564/564 - 0s - loss: 0.6704\n",
      "Epoch 18/50\n",
      "564/564 - 0s - loss: 0.6635\n",
      "Epoch 19/50\n",
      "564/564 - 0s - loss: 0.6564\n",
      "Epoch 20/50\n",
      "564/564 - 0s - loss: 0.6506\n",
      "Epoch 21/50\n",
      "564/564 - 0s - loss: 0.6461\n",
      "Epoch 22/50\n",
      "564/564 - 0s - loss: 0.6398\n",
      "Epoch 23/50\n",
      "564/564 - 0s - loss: 0.6366\n",
      "Epoch 24/50\n",
      "564/564 - 0s - loss: 0.6312\n",
      "Epoch 25/50\n",
      "564/564 - 0s - loss: 0.6316\n",
      "Epoch 26/50\n",
      "564/564 - 0s - loss: 0.6236\n",
      "Epoch 27/50\n",
      "564/564 - 0s - loss: 0.6218\n",
      "Epoch 28/50\n",
      "564/564 - 0s - loss: 0.6186\n",
      "Epoch 29/50\n",
      "564/564 - 0s - loss: 0.6158\n",
      "Epoch 30/50\n",
      "564/564 - 0s - loss: 0.6112\n",
      "Epoch 31/50\n",
      "564/564 - 0s - loss: 0.6103\n",
      "Epoch 32/50\n",
      "564/564 - 0s - loss: 0.6056\n",
      "Epoch 33/50\n",
      "564/564 - 0s - loss: 0.6017\n",
      "Epoch 34/50\n",
      "564/564 - 0s - loss: 0.5979\n",
      "Epoch 35/50\n",
      "564/564 - 0s - loss: 0.5935\n",
      "Epoch 36/50\n",
      "564/564 - 0s - loss: 0.5946\n",
      "Epoch 37/50\n",
      "564/564 - 0s - loss: 0.5820\n",
      "Epoch 38/50\n",
      "564/564 - 0s - loss: 0.5921\n",
      "Epoch 39/50\n",
      "564/564 - 0s - loss: 0.5898\n",
      "Epoch 40/50\n",
      "564/564 - 0s - loss: 0.5798\n",
      "Epoch 41/50\n",
      "564/564 - 0s - loss: 0.5678\n",
      "Epoch 42/50\n",
      "564/564 - 0s - loss: 0.5657\n",
      "Epoch 43/50\n",
      "564/564 - 0s - loss: 0.5788\n",
      "Epoch 44/50\n",
      "564/564 - 0s - loss: 0.5695\n",
      "Epoch 45/50\n",
      "564/564 - 0s - loss: 0.5612\n",
      "Epoch 46/50\n",
      "564/564 - 0s - loss: 0.5500\n",
      "Epoch 47/50\n",
      "564/564 - 0s - loss: 0.5456\n",
      "Epoch 48/50\n",
      "564/564 - 0s - loss: 0.5377\n",
      "Epoch 49/50\n",
      "564/564 - 0s - loss: 0.5183\n",
      "Epoch 50/50\n",
      "564/564 - 0s - loss: 0.5119\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = MLP_model.fit(stage1['train_X'], stage1['train_y'], epochs = 50, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0ba203f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "pred = MLP_model.predict(stage1['test_X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ebb9a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.431485\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "r2 = sklearn.metrics.r2_score(stage1['test_y'], pred)\n",
    "print(\"R2 score: %f\"%r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8a648",
   "metadata": {},
   "source": [
    "Residual network 가 더 성능이 좋은 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df8be3",
   "metadata": {},
   "source": [
    "**[TODO] Stage2 데이터에 대해 Residual network 를 학습해보세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "81a5feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resiudal network 모델 정의\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "MLP_model_residual = ResidualMLP(stage2['train_y'].shape[1], use_residual = True)\n",
    "\n",
    "# resiudal network 사용하지 않는 모델 정의\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "MLP_model = ResidualMLP(stage2['train_y'].shape[1], use_residual = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ed113dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "MLP_model.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    ")\n",
    "\n",
    "MLP_model_residual.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e789991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "564/564 - 1s - loss: 1.1114\n",
      "Epoch 2/50\n",
      "564/564 - 0s - loss: 0.5810\n",
      "Epoch 3/50\n",
      "564/564 - 0s - loss: 0.5183\n",
      "Epoch 4/50\n",
      "564/564 - 0s - loss: 0.4848\n",
      "Epoch 5/50\n",
      "564/564 - 0s - loss: 0.4677\n",
      "Epoch 6/50\n",
      "564/564 - 0s - loss: 0.4565\n",
      "Epoch 7/50\n",
      "564/564 - 0s - loss: 0.4483\n",
      "Epoch 8/50\n",
      "564/564 - 0s - loss: 0.4411\n",
      "Epoch 9/50\n",
      "564/564 - 0s - loss: 0.4345\n",
      "Epoch 10/50\n",
      "564/564 - 0s - loss: 0.4279\n",
      "Epoch 11/50\n",
      "564/564 - 0s - loss: 0.4217\n",
      "Epoch 12/50\n",
      "564/564 - 0s - loss: 0.4165\n",
      "Epoch 13/50\n",
      "564/564 - 0s - loss: 0.4117\n",
      "Epoch 14/50\n",
      "564/564 - 0s - loss: 0.4065\n",
      "Epoch 15/50\n",
      "564/564 - 0s - loss: 0.4021\n",
      "Epoch 16/50\n",
      "564/564 - 0s - loss: 0.3972\n",
      "Epoch 17/50\n",
      "564/564 - 0s - loss: 0.3914\n",
      "Epoch 18/50\n",
      "564/564 - 0s - loss: 0.3882\n",
      "Epoch 19/50\n",
      "564/564 - 0s - loss: 0.3843\n",
      "Epoch 20/50\n",
      "564/564 - 0s - loss: 0.3805\n",
      "Epoch 21/50\n",
      "564/564 - 0s - loss: 0.3773\n",
      "Epoch 22/50\n",
      "564/564 - 0s - loss: 0.3733\n",
      "Epoch 23/50\n",
      "564/564 - 0s - loss: 0.3713\n",
      "Epoch 24/50\n",
      "564/564 - 0s - loss: 0.3679\n",
      "Epoch 25/50\n",
      "564/564 - 0s - loss: 0.3651\n",
      "Epoch 26/50\n",
      "564/564 - 0s - loss: 0.3621\n",
      "Epoch 27/50\n",
      "564/564 - 0s - loss: 0.3608\n",
      "Epoch 28/50\n",
      "564/564 - 0s - loss: 0.3581\n",
      "Epoch 29/50\n",
      "564/564 - 0s - loss: 0.3559\n",
      "Epoch 30/50\n",
      "564/564 - 0s - loss: 0.3541\n",
      "Epoch 31/50\n",
      "564/564 - 0s - loss: 0.3524\n",
      "Epoch 32/50\n",
      "564/564 - 0s - loss: 0.3504\n",
      "Epoch 33/50\n",
      "564/564 - 0s - loss: 0.3489\n",
      "Epoch 34/50\n",
      "564/564 - 0s - loss: 0.3478\n",
      "Epoch 35/50\n",
      "564/564 - 0s - loss: 0.3461\n",
      "Epoch 36/50\n",
      "564/564 - 0s - loss: 0.3454\n",
      "Epoch 37/50\n",
      "564/564 - 0s - loss: 0.3441\n",
      "Epoch 38/50\n",
      "564/564 - 0s - loss: 0.3428\n",
      "Epoch 39/50\n",
      "564/564 - 0s - loss: 0.3421\n",
      "Epoch 40/50\n",
      "564/564 - 0s - loss: 0.3408\n",
      "Epoch 41/50\n",
      "564/564 - 0s - loss: 0.3395\n",
      "Epoch 42/50\n",
      "564/564 - 0s - loss: 0.3394\n",
      "Epoch 43/50\n",
      "564/564 - 0s - loss: 0.3382\n",
      "Epoch 44/50\n",
      "564/564 - 0s - loss: 0.3373\n",
      "Epoch 45/50\n",
      "564/564 - 0s - loss: 0.3364\n",
      "Epoch 46/50\n",
      "564/564 - 0s - loss: 0.3360\n",
      "Epoch 47/50\n",
      "564/564 - 0s - loss: 0.3354\n",
      "Epoch 48/50\n",
      "564/564 - 0s - loss: 0.3345\n",
      "Epoch 49/50\n",
      "564/564 - 0s - loss: 0.3327\n",
      "Epoch 50/50\n",
      "564/564 - 0s - loss: 0.3327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0961db3c40>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# residual 모델 학습\n",
    "MLP_model_residual.fit(stage2['train_X'], stage2['train_y'], epochs = 50, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7808f782",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "564/564 - 1s - loss: 0.9889\n",
      "Epoch 2/50\n",
      "564/564 - 0s - loss: 0.8115\n",
      "Epoch 3/50\n",
      "564/564 - 0s - loss: 0.6353\n",
      "Epoch 4/50\n",
      "564/564 - 0s - loss: 0.6166\n",
      "Epoch 5/50\n",
      "564/564 - 0s - loss: 0.5992\n",
      "Epoch 6/50\n",
      "564/564 - 0s - loss: 0.5779\n",
      "Epoch 7/50\n",
      "564/564 - 0s - loss: 0.5559\n",
      "Epoch 8/50\n",
      "564/564 - 0s - loss: 0.5352\n",
      "Epoch 9/50\n",
      "564/564 - 0s - loss: 0.5168\n",
      "Epoch 10/50\n",
      "564/564 - 0s - loss: 0.5020\n",
      "Epoch 11/50\n",
      "564/564 - 0s - loss: 0.4891\n",
      "Epoch 12/50\n",
      "564/564 - 0s - loss: 0.4796\n",
      "Epoch 13/50\n",
      "564/564 - 0s - loss: 0.4726\n",
      "Epoch 14/50\n",
      "564/564 - 0s - loss: 0.4643\n",
      "Epoch 15/50\n",
      "564/564 - 0s - loss: 0.4577\n",
      "Epoch 16/50\n",
      "564/564 - 0s - loss: 0.4473\n",
      "Epoch 17/50\n",
      "564/564 - 0s - loss: 0.4345\n",
      "Epoch 18/50\n",
      "564/564 - 0s - loss: 0.4247\n",
      "Epoch 19/50\n",
      "564/564 - 0s - loss: 0.4192\n",
      "Epoch 20/50\n",
      "564/564 - 0s - loss: 0.4142\n",
      "Epoch 21/50\n",
      "564/564 - 0s - loss: 0.4106\n",
      "Epoch 22/50\n",
      "564/564 - 0s - loss: 0.4067\n",
      "Epoch 23/50\n",
      "564/564 - 0s - loss: 0.4038\n",
      "Epoch 24/50\n",
      "564/564 - 0s - loss: 0.4004\n",
      "Epoch 25/50\n",
      "564/564 - 0s - loss: 0.3950\n",
      "Epoch 26/50\n",
      "564/564 - 0s - loss: 0.3897\n",
      "Epoch 27/50\n",
      "564/564 - 0s - loss: 0.3876\n",
      "Epoch 28/50\n",
      "564/564 - 0s - loss: 0.3820\n",
      "Epoch 29/50\n",
      "564/564 - 0s - loss: 0.3790\n",
      "Epoch 30/50\n",
      "564/564 - 0s - loss: 0.3762\n",
      "Epoch 31/50\n",
      "564/564 - 0s - loss: 0.3721\n",
      "Epoch 32/50\n",
      "564/564 - 0s - loss: 0.3678\n",
      "Epoch 33/50\n",
      "564/564 - 0s - loss: 0.3675\n",
      "Epoch 34/50\n",
      "564/564 - 0s - loss: 0.3631\n",
      "Epoch 35/50\n",
      "564/564 - 0s - loss: 0.3625\n",
      "Epoch 36/50\n",
      "564/564 - 0s - loss: 0.3617\n",
      "Epoch 37/50\n",
      "564/564 - 0s - loss: 0.3596\n",
      "Epoch 38/50\n",
      "564/564 - 0s - loss: 0.3576\n",
      "Epoch 39/50\n",
      "564/564 - 0s - loss: 0.3568\n",
      "Epoch 40/50\n",
      "564/564 - 0s - loss: 0.3554\n",
      "Epoch 41/50\n",
      "564/564 - 0s - loss: 0.3527\n",
      "Epoch 42/50\n",
      "564/564 - 0s - loss: 0.3506\n",
      "Epoch 43/50\n",
      "564/564 - 0s - loss: 0.3522\n",
      "Epoch 44/50\n",
      "564/564 - 0s - loss: 0.3503\n",
      "Epoch 45/50\n",
      "564/564 - 0s - loss: 0.3497\n",
      "Epoch 46/50\n",
      "564/564 - 0s - loss: 0.3485\n",
      "Epoch 47/50\n",
      "564/564 - 0s - loss: 0.3474\n",
      "Epoch 48/50\n",
      "564/564 - 0s - loss: 0.3448\n",
      "Epoch 49/50\n",
      "564/564 - 0s - loss: 0.3435\n",
      "Epoch 50/50\n",
      "564/564 - 0s - loss: 0.3444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0960842c40>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일반 MLP 모델 학습\n",
    "MLP_model.fit(stage2['train_X'], stage2['train_y'], epochs = 50, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b34cd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual 모델 예측\n",
    "pred = MLP_model_residual.predict(stage2['test_X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c9c7fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.660385\n"
     ]
    }
   ],
   "source": [
    "# Residual 모델 평가\n",
    "r2 = sklearn.metrics.r2_score(stage2['test_y'], pred)\n",
    "print(\"R2 score: %f\"%r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3123dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반 MLP 모델 예측\n",
    "pred = MLP_model.predict(stage2['test_X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f88c8bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.640176\n"
     ]
    }
   ],
   "source": [
    "# 일반 MLP 모델 평가\n",
    "r2 = sklearn.metrics.r2_score(stage2['test_y'], pred)\n",
    "print(\"R2 score: %f\"%r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee383e",
   "metadata": {},
   "source": [
    "## 4. Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5bcdb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L은 위치 인코딩에서 사용되는 주파수(frequency)의 개수를 의미.\n",
    "# frequency는 주파수를 의미\n",
    "# 위치 인코딩은 사인(sin)과 코사인(cos) 함수를 사용하여 위치 정보를 인코딩\n",
    "# 이 때 주파수가 높을수록 더 빠른 주기로 변화\n",
    "# 주파수는 주기를 반복하는 빈도를 나타내는데, 예를 들어 주파수가 높을수록 더 자주 반복되는 주기가 발생\n",
    "# 따라서 L 값이 클수록 주파수가 높아지며, 더 많은 주기가 포함된 위치 인코딩 값을 생성\n",
    "# 이로 인해 더 많은 세부적인 위치 정보가 인코딩되어 모델이 입력 시퀀스의 순서를 더 세밀하게 학습\n",
    "\n",
    "# X: 입력 시퀀스의 위치 정보를 나타내는 배열. 크기는 (N, d)이며, N은 시퀀스의 길이이고, d는 입력 벡터의 차원\n",
    "# L: 위치 인코딩의 차원을 결정하는 파라미터로, 정수\n",
    "def positional_encoding(X, L):\n",
    "    Xs = []\n",
    "    # l이 증가할수록 사인과 코사인 함수의 주기가 더 빠르게 변하므로, 더 높은 주파수 정보를 포함합니다.\n",
    "    for l in range(L):\n",
    "        # sin, cos의 인자값은 radian이기 때문에 pi를 곱해줌\n",
    "        Xs.append(np.sin(2 ** l * np.pi * X))\n",
    "        Xs.append(np.cos(2 ** l * np.pi * X))        \n",
    "    return np.concatenate(Xs, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86c72f",
   "metadata": {},
   "source": [
    "입력 데이터를 L개의 frequency를 가진 데이터로 변환하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "faf511f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = positional_encoding(stage1['train_X'], 5)\n",
    "test_X = positional_encoding(stage1['test_X'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fc0b44a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9016, 370)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "492d76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "MLP_model_pe = ResidualMLP(stage1['train_y'].shape[1], use_residual = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19787138",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model_pe.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7df11f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "564/564 - 1s - loss: 0.9860\n",
      "Epoch 2/50\n",
      "564/564 - 0s - loss: 0.7869\n",
      "Epoch 3/50\n",
      "564/564 - 0s - loss: 0.7251\n",
      "Epoch 4/50\n",
      "564/564 - 0s - loss: 0.6654\n",
      "Epoch 5/50\n",
      "564/564 - 0s - loss: 0.6307\n",
      "Epoch 6/50\n",
      "564/564 - 0s - loss: 0.6077\n",
      "Epoch 7/50\n",
      "564/564 - 0s - loss: 0.5850\n",
      "Epoch 8/50\n",
      "564/564 - 0s - loss: 0.5548\n",
      "Epoch 9/50\n",
      "564/564 - 0s - loss: 0.5220\n",
      "Epoch 10/50\n",
      "564/564 - 0s - loss: 0.4918\n",
      "Epoch 11/50\n",
      "564/564 - 0s - loss: 0.4750\n",
      "Epoch 12/50\n",
      "564/564 - 0s - loss: 0.4494\n",
      "Epoch 13/50\n",
      "564/564 - 0s - loss: 0.4307\n",
      "Epoch 14/50\n",
      "564/564 - 0s - loss: 0.4168\n",
      "Epoch 15/50\n",
      "564/564 - 0s - loss: 0.4058\n",
      "Epoch 16/50\n",
      "564/564 - 0s - loss: 0.3917\n",
      "Epoch 17/50\n",
      "564/564 - 0s - loss: 0.3935\n",
      "Epoch 18/50\n",
      "564/564 - 0s - loss: 0.3991\n",
      "Epoch 19/50\n",
      "564/564 - 0s - loss: 0.3858\n",
      "Epoch 20/50\n",
      "564/564 - 0s - loss: 0.3765\n",
      "Epoch 21/50\n",
      "564/564 - 0s - loss: 0.3693\n",
      "Epoch 22/50\n",
      "564/564 - 0s - loss: 0.3981\n",
      "Epoch 23/50\n",
      "564/564 - 0s - loss: 0.3894\n",
      "Epoch 24/50\n",
      "564/564 - 0s - loss: 0.3727\n",
      "Epoch 25/50\n",
      "564/564 - 0s - loss: 0.3730\n",
      "Epoch 26/50\n",
      "564/564 - 0s - loss: 0.3603\n",
      "Epoch 27/50\n",
      "564/564 - 0s - loss: 0.3556\n",
      "Epoch 28/50\n",
      "564/564 - 0s - loss: 0.3507\n",
      "Epoch 29/50\n",
      "564/564 - 0s - loss: 0.3469\n",
      "Epoch 30/50\n",
      "564/564 - 0s - loss: 0.3493\n",
      "Epoch 31/50\n",
      "564/564 - 0s - loss: 0.3437\n",
      "Epoch 32/50\n",
      "564/564 - 0s - loss: 0.3395\n",
      "Epoch 33/50\n",
      "564/564 - 0s - loss: 0.3377\n",
      "Epoch 34/50\n",
      "564/564 - 0s - loss: 0.3820\n",
      "Epoch 35/50\n",
      "564/564 - 0s - loss: 0.3613\n",
      "Epoch 36/50\n",
      "564/564 - 0s - loss: 0.3626\n",
      "Epoch 37/50\n",
      "564/564 - 0s - loss: 0.3427\n",
      "Epoch 38/50\n",
      "564/564 - 0s - loss: 0.3386\n",
      "Epoch 39/50\n",
      "564/564 - 0s - loss: 0.3311\n",
      "Epoch 40/50\n",
      "564/564 - 0s - loss: 0.3293\n",
      "Epoch 41/50\n",
      "564/564 - 0s - loss: 0.3284\n",
      "Epoch 42/50\n",
      "564/564 - 0s - loss: 0.3265\n",
      "Epoch 43/50\n",
      "564/564 - 0s - loss: 0.3292\n",
      "Epoch 44/50\n",
      "564/564 - 0s - loss: 0.3284\n",
      "Epoch 45/50\n",
      "564/564 - 0s - loss: 0.3218\n",
      "Epoch 46/50\n",
      "564/564 - 0s - loss: 0.3196\n",
      "Epoch 47/50\n",
      "564/564 - 0s - loss: 0.3151\n",
      "Epoch 48/50\n",
      "564/564 - 0s - loss: 0.3180\n",
      "Epoch 49/50\n",
      "564/564 - 0s - loss: 0.3139\n",
      "Epoch 50/50\n",
      "564/564 - 0s - loss: 0.3119\n"
     ]
    }
   ],
   "source": [
    "history = MLP_model_pe.fit(train_X, stage1['train_y'], epochs = 50, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9989bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = MLP_model_pe.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5d656df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.588890\n"
     ]
    }
   ],
   "source": [
    "r2 = sklearn.metrics.r2_score(stage1['test_y'], pred)\n",
    "print(\"R2 score: %f\"%r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beefd93c",
   "metadata": {},
   "source": [
    "금속분말 데이터셋에 대해서 positional encoding은 좋은 효과가 없었습니다. 모든 머신러닝 방법론이 항상 좋은 성능을 보장하진 않습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd63b8",
   "metadata": {},
   "source": [
    "**[TODO] Stage2 데이터에 대해 positional encoding 적용해보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f8961983",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = positional_encoding(stage2['train_X'], 5)\n",
    "test_X = positional_encoding(stage2['test_X'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d5fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "MLP_model_pe = ResidualMLP(stage2['train_y'].shape[1], use_residual = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1fc64f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model_pe.compile(loss = 'mse',\n",
    "              optimizer = tf.keras.optimizers.SGD(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f9e28be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "564/564 - 1s - loss: 0.8095\n",
      "Epoch 2/50\n",
      "564/564 - 0s - loss: 0.5806\n",
      "Epoch 3/50\n",
      "564/564 - 0s - loss: 0.5197\n",
      "Epoch 4/50\n",
      "564/564 - 0s - loss: 0.4760\n",
      "Epoch 5/50\n",
      "564/564 - 0s - loss: 0.4564\n",
      "Epoch 6/50\n",
      "564/564 - 0s - loss: 0.4457\n",
      "Epoch 7/50\n",
      "564/564 - 0s - loss: 0.4380\n",
      "Epoch 8/50\n",
      "564/564 - 0s - loss: 0.4315\n",
      "Epoch 9/50\n",
      "564/564 - 0s - loss: 0.4273\n",
      "Epoch 10/50\n",
      "564/564 - 0s - loss: 0.4226\n",
      "Epoch 11/50\n",
      "564/564 - 0s - loss: 0.4099\n",
      "Epoch 12/50\n",
      "564/564 - 0s - loss: 0.3824\n",
      "Epoch 13/50\n",
      "564/564 - 0s - loss: 0.3616\n",
      "Epoch 14/50\n",
      "564/564 - 0s - loss: 0.3534\n",
      "Epoch 15/50\n",
      "564/564 - 0s - loss: 0.3477\n",
      "Epoch 16/50\n",
      "564/564 - 0s - loss: 0.3446\n",
      "Epoch 17/50\n",
      "564/564 - 0s - loss: 0.3416\n",
      "Epoch 18/50\n",
      "564/564 - 0s - loss: 0.3386\n",
      "Epoch 19/50\n",
      "564/564 - 0s - loss: 0.3359\n",
      "Epoch 20/50\n",
      "564/564 - 0s - loss: 0.3334\n",
      "Epoch 21/50\n",
      "564/564 - 0s - loss: 0.3287\n",
      "Epoch 22/50\n",
      "564/564 - 0s - loss: 0.3265\n",
      "Epoch 23/50\n",
      "564/564 - 0s - loss: 0.3232\n",
      "Epoch 24/50\n",
      "564/564 - 0s - loss: 0.3217\n",
      "Epoch 25/50\n",
      "564/564 - 0s - loss: 0.3192\n",
      "Epoch 26/50\n",
      "564/564 - 0s - loss: 0.3185\n",
      "Epoch 27/50\n",
      "564/564 - 0s - loss: 0.3171\n",
      "Epoch 28/50\n",
      "564/564 - 0s - loss: 0.3152\n",
      "Epoch 29/50\n",
      "564/564 - 0s - loss: 0.3143\n",
      "Epoch 30/50\n",
      "564/564 - 0s - loss: 0.3144\n",
      "Epoch 31/50\n",
      "564/564 - 0s - loss: 0.3126\n",
      "Epoch 32/50\n",
      "564/564 - 0s - loss: 0.3123\n",
      "Epoch 33/50\n",
      "564/564 - 0s - loss: 0.3112\n",
      "Epoch 34/50\n",
      "564/564 - 0s - loss: 0.3103\n",
      "Epoch 35/50\n",
      "564/564 - 0s - loss: 0.3105\n",
      "Epoch 36/50\n",
      "564/564 - 0s - loss: 0.3093\n",
      "Epoch 37/50\n",
      "564/564 - 0s - loss: 0.3090\n",
      "Epoch 38/50\n",
      "564/564 - 0s - loss: 0.3079\n",
      "Epoch 39/50\n",
      "564/564 - 0s - loss: 0.3076\n",
      "Epoch 40/50\n",
      "564/564 - 0s - loss: 0.3068\n",
      "Epoch 41/50\n",
      "564/564 - 0s - loss: 0.3064\n",
      "Epoch 42/50\n",
      "564/564 - 0s - loss: 0.3060\n",
      "Epoch 43/50\n",
      "564/564 - 0s - loss: 0.3055\n",
      "Epoch 44/50\n",
      "564/564 - 0s - loss: 0.3050\n",
      "Epoch 45/50\n",
      "564/564 - 0s - loss: 0.3045\n",
      "Epoch 46/50\n",
      "564/564 - 0s - loss: 0.3043\n",
      "Epoch 47/50\n",
      "564/564 - 0s - loss: 0.3038\n",
      "Epoch 48/50\n",
      "564/564 - 0s - loss: 0.3038\n",
      "Epoch 49/50\n",
      "564/564 - 0s - loss: 0.3028\n",
      "Epoch 50/50\n",
      "564/564 - 0s - loss: 0.3027\n"
     ]
    }
   ],
   "source": [
    "history = MLP_model_pe.fit(train_X, stage2['train_y'], epochs = 50, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "da7a3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = MLP_model_pe.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1697cfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.674008\n"
     ]
    }
   ],
   "source": [
    "r2 = sklearn.metrics.r2_score(stage2['test_y'], pred)\n",
    "print(\"R2 score: %f\"%r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d812bc3",
   "metadata": {},
   "source": [
    "## 5. [TODO] 최적의 인공지능 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63fd4f",
   "metadata": {},
   "source": [
    "여태까지 배운 내용들을 종합하여 금속분말 데이터셋에 대해 최고 성능을 발휘하는 최적의 인공지능 모델을 만들어보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c02895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cea73b91",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(120, 120, 120)\">본 학습 자료를 포함한 사이트 내 모든 자료의 저작권은 엘리스에 있으며 외부로의 무단 복제, 배포 및 전송을 불허합니다.\n",
    "\n",
    "Copyright @ elice all rights reserved</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
