{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baec9a3c",
   "metadata": {},
   "source": [
    "# 실습1: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67692dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 사용할 1차 선형 회귀 모델\n",
    "\n",
    "def linear_model(w0, w1, X):\n",
    "\n",
    "\n",
    "    f_x = w0 + w1 * X\n",
    "    \n",
    "    return f_x\n",
    "    \n",
    "'''\n",
    "1. 설명 중 '손실 함수' 파트의 수식을 참고해\n",
    "   MSE 손실 함수를 완성하세요. \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def Loss(f_x, y):\n",
    "    \n",
    "    ls = np.mean((y - f_x)**2)\n",
    "    \n",
    "    return ls\n",
    "\n",
    "\n",
    "'''\n",
    "2. 설명 중 'Gradient' 파트의 마지막 두 수식을 참고해 두 가중치\n",
    "   w0와 w1에 대한 gradient인 'gradient0'와 'gradient1'을\n",
    "\n",
    "   반환하는 함수 'gradient_descent' 함수를 완성하세요.\n",
    "   \n",
    "   Step01. w0에 대한 gradient인 'gradient0'를 작성합니다.\n",
    "   \n",
    "   Step02. w1에 대한 gradient인 'gradient1'을 작성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def gradient_descent(w0, w1, X, y):\n",
    "\n",
    "    gradient0 = (-2/X.size) * np.sum(y - (w0 + w1*X))\n",
    "\n",
    "    gradient1 = (-2/X.size) * np.sum((y - (w0 + w1*X))*X)\n",
    "    \n",
    "    return np.array([gradient0, gradient1])\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "3. 설명 중 '가중치 업데이트' 파트의 두 수식을 참고해 \n",
    "\n",
    "   gradient descent를 통한 가중치 업데이트 코드를 작성하세요.\n",
    "   \n",
    "   Step01. 앞서 완성한 gradient_descent 함수를 이용해\n",
    "           w0와 w1에 대한 gradient인 'gd'를 정의하세요.\n",
    "           \n",
    "   Step02. 변수 'w0'와 'w1'에 두 가중치 w0와 w1을 \n",
    "           업데이트하는 코드를 작성합니다. 앞서 정의한\n",
    "           변수 'gd'와 이미 정의된 변수 'lr'을 사용하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    X = np.array([1,2,3,4]).reshape((-1,1))\n",
    "\n",
    "    y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "    \n",
    "    # 파라미터 초기화\n",
    "    w0 = 0\n",
    "    w1 = 0\n",
    "\n",
    "\n",
    "    # learning rate 설정\n",
    "\n",
    "    lr = 0.001\n",
    "    \n",
    "    # 반복 횟수 1000으로 설정\n",
    "    for i in range(1000):\n",
    "    \n",
    "        gd = gradient_descent(w0,w1,X,y)\n",
    "        \n",
    "        w0 = w0 - lr*gd[0]\n",
    "        w1 = w1 - lr*gd[1]\n",
    "        \n",
    "        # 100회마다의 해당 loss와 w0, w1 출력\n",
    "        if (i % 100 == 0):\n",
    "        \n",
    "            loss = Loss(linear_model(w0,w1,X),y)\n",
    "        \n",
    "            print(\"{}번째 loss : {}\".format(i, loss))\n",
    "            print(\"{}번째 w0, w1 : {}, {}\".format(i, w0, w1),'\\n')\n",
    "\n",
    "\n",
    "    return w0, w1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa9b03",
   "metadata": {},
   "source": [
    "# 실습2: 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06baedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x) :\n",
    "\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "'''\n",
    "    X, y 를 가장 잘 설명하는 parameter (w1, w2, w3)를 반환하는 함수를 작성하세요. 여기서 X는 (x1, x2, x3) 의 list이며, y 는 0 혹은 1로 이루어진 list입니다. 예를 들어, X, y는 다음의 값을 가질 수 있습니다.\n",
    "\n",
    "\n",
    "    X = [(1, 0, 0), (1, 0, 1), (0, 0, 1)]\n",
    "    y = [0, 1, 1]\n",
    "'''\n",
    "\n",
    "'''\n",
    "Step01. X의 한 원소가 3개이므로 가중치도 3개가 있어야 합니다.\n",
    "\n",
    "        초기 가중치 w를 [1,1,1]로 정의하는 코드를 작성하세요.\n",
    "        \n",
    "        단순히 f = 3, w = [1,1,1]이라고 하는 것보다 좀 더 \n",
    "        좋은 표현을 생각해보세요.\n",
    "        \n",
    "        \n",
    "Step02. 초기 가중치 w를 모델에 맞게 계속 업데이트 해야합니다.\n",
    "            \n",
    "        업데이트를 위해 초기 가중치 w에 더해지는 값들의 리스트\n",
    "        wPrime을 [0,0,0]로 정의하는 코드를 작성하세요.  \n",
    "            \n",
    "        마찬가지로 단순히 wPrime = [0,0,0]이라고 하는 것보다\n",
    "        좀 더 좋은 표현을 생각해보세요.\n",
    "        \n",
    "        \n",
    "Step03. sigmoid 함수를 통과할 r값을 정의해야합니다. r은 \n",
    "        X의 각 값과 그에 해당하는 가중치 w의 곱의 합입니다.\n",
    "            \n",
    "        즉, r = X_0_0 * w_0 + X_1_0 * w_0 + ... + X_2_2 * w_2\n",
    "        가 됩니다.\n",
    "            \n",
    "        그리고 sigmoid 함수를 통과한 r값을 v로 정의합시다.\n",
    "    \n",
    "    \n",
    "Step04. 가중치 w가 더이상 업데이트가 안될 때까지 업데이트 해줘야합니다.\n",
    "        즉, 가중치 w의 업데이트틀 위해 더해지는 wPrime 값이 어느 정도까지\n",
    "        작아지면 업데이트를 끝내야 합니다. \n",
    "            \n",
    "        그 값을 0.001로 정하고, wPrime이 그 값을 넘지 못하면 가중치 \n",
    "        업데이트를 끝내도록 합시다. Boolean 변수인 flag를 이용해보세요.\n",
    "        \n",
    "        다만 wPrime 값이 0.001보다 작아지기 전까지는 w에 wPrime을 계속\n",
    "        더하면서 w를 업데이트 합시다.    \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def getParameters(X, y) :\n",
    "    \n",
    "    f = len(X[0]) # None\n",
    "\n",
    "\n",
    "    w = [1 for i in range(f)] # None\n",
    "\n",
    "\n",
    "    values = []\n",
    "        \n",
    "    while True :\n",
    "        \n",
    "        wPrime = [0 for i in range(f)]\n",
    "                      \n",
    "        \n",
    "        for i in range(len(y)) :\n",
    "            r = 0\n",
    "            for j in range(f) :\n",
    "                r += X[i][j] * w[j] # None\n",
    "\n",
    "\n",
    "\n",
    "            v = sigmoid(r) # None\n",
    "            \n",
    "\n",
    "\n",
    "            # w를 업데이트하기 위한 wPrime을 역전파를 이용해 구하는 식\n",
    "            for j in range(f) :\n",
    "\n",
    "                wPrime[j] += -((v - y[i]) * v * (1-v) * X[i][j])\n",
    "    \n",
    "        flag = False\n",
    "\n",
    "\n",
    "        for i in range(f) :\n",
    "            if abs(wPrime[i]) >= 1e-3 : # None\n",
    "                flag = True # None\n",
    "                break\n",
    "\n",
    "        if flag == False : # None\n",
    "            break\n",
    "\n",
    "        for j in range(f) :\n",
    "            w[j] += wPrime[j] # None\n",
    "\n",
    "    return w\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    이 코드는 수정하지 마세요.\n",
    "    '''\n",
    "\n",
    "    X = [(1, 0, 0), (1, 0, 1), (0, 0, 1)]\n",
    "\n",
    "    y = [0, 1, 1]\n",
    "    \n",
    "    '''\n",
    "    # 아래의 예제 또한 테스트 해보세요.\n",
    "    X = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]\n",
    "    y = [0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "\n",
    "    # 아래의 예제를 perceptron이 100% training할 수 있는지도 확인해봅니다.\n",
    "    X = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]\n",
    "    y = [0, 0, 0, 1, 0, 1, 1, 1]\n",
    "    '''\n",
    "\n",
    "    print(getParameters(X, y))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7af30",
   "metadata": {},
   "source": [
    "# 실습3: 텐서의 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "1. 상수 텐서를 생성하는 'constant_tensors' 함수를 완성하세요.\n",
    "\n",
    "\n",
    "   Step01. 5의 값을 가지는 (1,1) shape의 8-bit integer 텐서를 만드세요.\n",
    "   \n",
    "   Step02. 모든 원소의 값이 0인 (3,5) shape의 16-bit integer 텐서를 만드세요.\n",
    "   \n",
    "   Step03. 모든 원소의 값이 1인 (4,3) shape의 8-bit integer 텐서를 만드세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def constant_tensors():\n",
    "\n",
    "    t1 = tf.constant(5, tf.int8)\n",
    "\n",
    "\n",
    "    t2 = tf.zeros((3, 5), tf.int16)\n",
    "    \n",
    "    t3 = tf.ones((4, 3), tf.int8)\n",
    "    \n",
    "    return t1, t2, t3\n",
    "    \n",
    "'''\n",
    "2. 시퀀스 텐서를 생성하는 'sequence_tensors' 함수를 완성하세요. \n",
    "\n",
    "\n",
    "\n",
    "   Step01. 1.5에서 10.5까지 증가하는 3개의 텐서를 만드세요.\n",
    "   \n",
    "   Step02. 2.5에서 20.5까지 증가하는 5개의 텐서를 만드세요. \n",
    "'''\n",
    "    \n",
    "def sequence_tensors():\n",
    "\n",
    "\n",
    "\n",
    "    seq_t1 = tf.linspace(1.5, 10.5, 3)\n",
    "    \n",
    "    seq_t2 = tf.linspace(2.5, 20.5, 5)\n",
    "    \n",
    "    return seq_t1, seq_t2\n",
    "    \n",
    "'''\n",
    "3. 변수를 생성하는 'variable_tensor' 함수를 완성하세요.\n",
    "\n",
    "\n",
    "\n",
    "   Step01. 값이 100인 변수 텐서를 만드세요.\n",
    "   \n",
    "   Step02. 모든 원소의 값이 1인 (2,2) shape의 변수 텐서를 만드세요.\n",
    "           이름도 'W'로 지정합니다.\n",
    "   \n",
    "   Step03. 모든 원소의 값이 0인 (2,) shape의 변수 텐서를 만드세요.\n",
    "           이름도 'b'로 지정합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def variable_tensor():\n",
    "\n",
    "\n",
    "    var_tensor = tf.Variable(100)\n",
    "    \n",
    "    W = tf.Variable(tf.ones(shape=(2,2)), name='W')\n",
    "    \n",
    "    b = tf.Variable(tf.zeros(shape=(2)), name='b')\n",
    "    \n",
    "    return var_tensor, W, b\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    # 1. constant_tensors 함수를 완성하세요.\n",
    "    t1, t2, t3 = constant_tensors()\n",
    "    \n",
    "    # 2. sequence_tensors 함수를 완성하세요.\n",
    "    seq_t1,seq_t2 = sequence_tensors()\n",
    "    \n",
    "    # 3. variable_tensor 함수를 완성하세요.\n",
    "    var_tensor, W, b = variable_tensor()\n",
    "    \n",
    "    constant_dict = {'t1':t1, 't2':t2, 't3':t3}\n",
    "    \n",
    "    sequence_dict = {'seq_t1':seq_t1, 'seq_t2':seq_t2}\n",
    "    \n",
    "    variable_dict = {'var_tensor':var_tensor, 'W':W, 'b':b}\n",
    "    \n",
    "    for key, value in constant_dict.items():\n",
    "        print(key, ' :', value.numpy())\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    for key, value in sequence_dict.items():\n",
    "        print(key, ' :', value.numpy())\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    for key, value in variable_dict.items():\n",
    "        print(key, ' :', value.numpy())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804f8cd",
   "metadata": {},
   "source": [
    "# 실습4: 텐서의 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93747674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "'''\n",
    "1. 이항 연산자를 사용해 사칙 연산을 수행하여 각 변수에 저장하세요.\n",
    "\n",
    "\n",
    "   Step01. 텐서 'a'와 'b'를 더해 `add`에 저장하세요.\n",
    "   \n",
    "   Step02. 텐서 'a'에서 'b'를 빼 `sub`에 저장하세요.\n",
    "   \n",
    "   Step03. 텐서 'a'와 'b'를 곱해 'mul'에 저장하세요.\n",
    "   \n",
    "   Step04. 텐서 'a'에서 'b'를 나눠 'div'에 저장하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    a = tf.constant(10, dtype = tf.int32)\n",
    "    b = tf.constant(3, dtype = tf.int32)\n",
    "\n",
    "    add = tf.add(a,b)           # add\n",
    "    sub = tf.subtract(a,b)      # subtract\n",
    "    mul = tf.multiply(a,b)     # multiply\n",
    "\n",
    "    div = tf.truediv(a,b)       # divide\n",
    "    \n",
    "    tensor_dict = {'add':add, 'sub':sub, 'mul':mul, 'div':div}\n",
    "\n",
    "\n",
    "    for key, value in tensor_dict.items():\n",
    "\n",
    "        print(key, ' :', value.numpy(), '\\n')\n",
    "        \n",
    "    return add, sub, mul, div\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613551ec",
   "metadata": {},
   "source": [
    "# 실습5: 선형회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "# 채점을 위한 랜덤 시드를 고정하는 코드입니다.\n",
    "# 정확한 채점을 위해 코드를 변경하지 마세요!\n",
    "np.random.seed(100)\n",
    "\n",
    "# 선형 회귀 클래스 구현\n",
    "class LinearModel:\n",
    "    def __init__(self):\n",
    "        # 1. 가중치 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.\n",
    "\n",
    "        self.W = tf.Variable(1.5)\n",
    "        \n",
    "        # 1. 편향 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.\n",
    "        self.b = tf.Variable(1.5)\n",
    "        \n",
    "    def __call__(self, X, Y):\n",
    "        # 2. W, X, b를 사용해 선형 모델을 구현하세요.\n",
    "        return tf.multiply(self.W, X) + self.b\n",
    "    \n",
    "# 3. MSE 값으로 정의된 loss 함수 선언 \n",
    "def loss(y, pred):\n",
    "    return tf.reduce_mean(tf.square(y - pred))\n",
    "\n",
    "\n",
    "# gradient descent 방식으로 학습 함수 선언\n",
    "def train(linear_model, x, y):\n",
    "\n",
    "    with tf.GradientTape() as t:\n",
    "\n",
    "        current_loss = loss(y, linear_model(x, y))\n",
    "    \n",
    "    # learning_rate 값 선언\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # gradient 값 계산\n",
    "    delta_W, delta_b = t.gradient(current_loss, [linear_model.W, linear_model.b])\n",
    "    \n",
    "    # learning rate와 계산한 gradient 값을 이용하여 업데이트할 파라미터 변화 값 계산 \n",
    "    W_update = (learning_rate * delta_W)\n",
    "    b_update = (learning_rate * delta_b)\n",
    "    \n",
    "    return W_update, b_update\n",
    " \n",
    "def main():\n",
    "    # 데이터 생성\n",
    "    x_data = np.linspace(0, 10, 50)\n",
    "    y_data = 4 * x_data + np.random.randn(*x_data.shape)*4 + 3\n",
    "\n",
    "\n",
    "    # 데이터 출력\n",
    "    plt.scatter(x_data,y_data)\n",
    "    plt.savefig('data.png')\n",
    "    elice_utils.send_image('data.png')\n",
    "\n",
    "    # 선형 함수 적용\n",
    "    linear_model = LinearModel()\n",
    "\n",
    "    # epochs 값 선언\n",
    "    epochs = 100\n",
    "\n",
    "    # epoch 값만큼 모델 학습\n",
    "    for epoch_count in range(epochs):\n",
    "\n",
    "        # 선형 모델의 예측 값 저장\n",
    "        y_pred_data=linear_model(x_data, y_data)\n",
    "\n",
    "        # 예측 값과 실제 데이터 값과의 loss 함수 값 저장\n",
    "        real_loss = loss(y_data, linear_model(x_data, y_data))\n",
    "\n",
    "        # 현재의 선형 모델을 사용하여  loss 값을 줄이는 새로운 파라미터로 갱신할 파라미터 변화 값을 계산\n",
    "\n",
    "        update_W, update_b = train(linear_model, x_data, y_data)\n",
    "        \n",
    "        # 선형 모델의 가중치와 편향을 업데이트합니다. \n",
    "        linear_model.W.assign_sub(update_W)\n",
    "        linear_model.b.assign_sub(update_b)\n",
    "\n",
    "\n",
    "        # 20번 마다 출력 (조건문 변경 가능)\n",
    "        if (epoch_count%20==0):\n",
    "            print(f\"Epoch count {epoch_count}: Loss value: {real_loss.numpy()}\")\n",
    "            print('W: {}, b: {}'.format(linear_model.W.numpy(), linear_model.b.numpy()))\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_subplot(111)\n",
    "            ax1.scatter(x_data,y_data)\n",
    "            ax1.plot(x_data,y_pred_data, color='red')\n",
    "            plt.savefig('prediction.png')\n",
    "\n",
    "            elice_utils.send_image('prediction.png')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c5db5",
   "metadata": {},
   "source": [
    "# 실습6: 비선형회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from visual import *\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "np.random.seed(100)\n",
    "tf.random.set_seed(100)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # 데이터 생성\n",
    "    x_data = np.linspace(0, 10, 100)\n",
    "    y_data = 1.5 * x_data**2 -12 * x_data + np.random.randn(*x_data.shape)*2 + 0.5\n",
    "    \n",
    "    # 신경망 모델 생성\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, input_dim=1, activation='relu'),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # 최적화 모델 설정\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # 학습 시작 \n",
    "    history = model.fit(x_data, y_data, epochs=500, verbose=2)\n",
    "    \n",
    "    # 학습된 모델을 사용하여 예측값 생성 및 저장\n",
    "    predictions = model.predict(x_data)\n",
    "    \n",
    "    Visualize(x_data, y_data, predictions)\n",
    "    \n",
    "    return history, model\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202198b",
   "metadata": {},
   "source": [
    "# 실습7: XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# XOR 문제를 위한 데이터 생성\n",
    "\n",
    "def main():\n",
    "\n",
    "    training_data = np.array([[0,0],[0,1],[1,0],[1,1]], 'float32')\n",
    "    target_data = np.array([[0],[1],[1],[0]], 'float32')\n",
    "\n",
    "    '''\n",
    "    1. 다층 퍼셉트론 모델을 만듭니다.\n",
    "    '''\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, input_dim=2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    '''\n",
    "    2. 모델 학습 방법을 설정합니다.\n",
    "    '''\n",
    "\n",
    "    model.compile(loss='mse',  optimizer='adam',  metrics=['binary_accuracy'])\n",
    "\n",
    "    '''\n",
    "    3. 모델을 학습시킵니다. epochs를 자유롭게 설정해보세요.\n",
    "    ''' \n",
    "\n",
    "\n",
    "    hist = model.fit(training_data, target_data, epochs = 20)\n",
    "    \n",
    "    score = hist.history['binary_accuracy'][-1]\n",
    "    \n",
    "    print('최종 정확도: ', score*100, '%')\n",
    "    \n",
    "    return hist\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
