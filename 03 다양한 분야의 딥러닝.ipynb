{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52411621",
   "metadata": {},
   "source": [
    "# 실습1: 이미지 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import PIL\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "# 이미지 목록을 불러오는 함수입니다.\n",
    "\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "'''\n",
    "1. PIL.Image를 이용하여 이름(경로+이름)을 바탕으로 이미지를 불러오고,\n",
    "   이를 리스트 'images'에 추가하는 함수를 완성합니다.\n",
    "   main 함수에서 'path'와 'names' 변수가 무엇에 해당하는지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def load_images(path, names):\n",
    "    \n",
    "    images=[]\n",
    "    \n",
    "    for name in names:\n",
    "        images.append(PIL.Image.open(path+name))\n",
    "    \n",
    "    return images\n",
    "\n",
    "\n",
    "'''\n",
    "2. 이미지의 사이즈를 main 함수에 있는 'IMG_SIZE'로 \n",
    "   조정하고, 이를 Numpy 배열로 변환하는 함수를 완성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def images2numpy(images, size):\n",
    "    \n",
    "    output=[]\n",
    "    \n",
    "    for img in images:\n",
    "        img = img.resize(size)\n",
    "        np_img = np.array(img)\n",
    "        output.append(np_img)     \n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# 이미지에 대한 정보를 나타내주는 함수입니다.\n",
    "\n",
    "def sampleVisualize(np_images):\n",
    "\n",
    "    fileName = \"./data/images/1000092795.jpg\"\n",
    "\n",
    "    ndarray = img.imread(fileName)\n",
    "    \n",
    "    plt.imshow(ndarray)\n",
    "    plt.show()    \n",
    "    plt.savefig(\"plot.png\")\n",
    "    \n",
    "    print(\"\\n1-1. 'fileName' 이미지(원본): \")\n",
    "    elice_utils.send_image(\"plot.png\")\n",
    "    \n",
    "    print('\\n1-2. Numpy array로 변환된 원본 이미지:', ndarray)\n",
    "    print('\\n1-3. Numpy array로 변환된 원본 이미지의 크기:', np.array(ndarray).shape)\n",
    "    \n",
    "    plt.imshow(np_images[0])\n",
    "    plt.show()\n",
    "    plt.savefig(\"plot_re.png\")\n",
    "    \n",
    "    print(\"\\n2-1. 'fileName' 이미지(resize 후): \")\n",
    "    elice_utils.send_image(\"plot_re.png\")\n",
    "    \n",
    "    print('\\n2-2. Numpy array로 변환된 resize 후 이미지:', np_images[0])\n",
    "    print('\\n2-3. Numpy array로 변환된 resize 후 이미지 크기:', np.array(np_images[0]).shape)    \n",
    "    \n",
    "    print('\\n3. Numpy array로 변환된 resize 후 이미지 10장의 크기:', np.array(np_images).shape)\n",
    "\n",
    "\n",
    "'''\n",
    "3. main 함수를 완성하세요.\n",
    "\n",
    "\n",
    "   Step01. 이미지를 불러오는 함수를 이용해 'images'를 정의합니다.\n",
    "   \n",
    "   Step02. 이미지를 Numpy 배열로 바꾸는 함수를 이용해 'np_images'를 정의합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    CSV_PATH = \"./data/data.csv\"\n",
    "    IMG_PATH = \"./data/images/\"\n",
    "    IMG_SIZE = (300,300)\n",
    "    MAX_LEN = 30\n",
    "    BATCH_SIZE = 2\n",
    "    \n",
    "    name_caption = load_data(CSV_PATH)\n",
    "    names = name_caption['file_name']\n",
    "    \n",
    "    images = load_images(IMG_PATH,names)\n",
    "    np_images = images2numpy(images, IMG_SIZE)\n",
    "    \n",
    "    sampleVisualize(np_images)\n",
    "    \n",
    "    return images, np_images\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea964ffb",
   "metadata": {},
   "source": [
    "# 실습2: MLP로 이미지 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee608ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "'''\n",
    "1. MNIST 데이테 셋을 전처리하는 'preprocess' 함수를 완성합니다.\n",
    "\n",
    "   Step01. MNIST 데이터 이미지를 0~1 사이 값으로 정규화해줍니다.\n",
    "\n",
    "\n",
    "           원본은 0~255 사이의 값입니다.\n",
    "           \n",
    "   Step02. 0~9 사이 값인 레이블을 클래스화 하기 위해 원-핫 인코딩을 진행합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    \n",
    "    # MNIST 데이터 세트를 불러옵니다.\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    \n",
    "    # MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \n",
    "    \n",
    "    train_images = train_images.astype(np.float32) / 255.\n",
    "    test_images = test_images.astype(np.float32) / 255.\n",
    "    \n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    test_labels = to_categorical(test_labels, 10)    \n",
    "    \n",
    "    return train_images, test_images, train_labels, test_labels\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "2. 다층 퍼셉트론(MLP) 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def MLP():\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "'''\n",
    "3. 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   Step01. MLP 함수를 통해 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 모델을 학습시킵니다. 검증용 데이터도 설정하세요.\n",
    "           'epochs'와 'batch_size'도 자유롭게 설정하세요.\n",
    "           단, 'epochs'이 클수록, 'batch_size'는 작을수록 학습 속도가 느립니다.\n",
    "   \n",
    "   Step05. 모델을 테스트하고 손실(loss)값과 Test Accuracy 값 및 예측 클래스, \n",
    "           손실 함수값 그래프를 출력합니다. 모델의 성능을 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    train_images, test_images, train_labels, test_labels = preprocess()\n",
    "    \n",
    "    model = MLP()\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "    \n",
    "    history = model.fit(train_images, train_labels, epochs = 3, batch_size = 500, validation_data = (test_images, test_labels), verbose = 0)\n",
    "    \n",
    "    loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    \n",
    "    print('\\nTest Loss : {:.4f} | Test Accuracy : {}'.format(loss, test_acc))\n",
    "    print('예측한 Test Data 클래스 : ',model.predict_classes(test_images))\n",
    "    \n",
    "    Visulaize([('MLP', history)], 'loss')\n",
    "    \n",
    "    return history\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33445976",
   "metadata": {},
   "source": [
    "# 실습3: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c172362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from visual import *\n",
    "from plotter import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "'''\n",
    "\n",
    "1. MNIST 데이테 셋을 전처리하는 'preprocess' 함수를 완성합니다.\n",
    "   \n",
    "   Step01과 Step03은 이전 실습과 동일한 코드를 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "   Step01. MNIST 데이터 이미지를 0~1 사이 값으로 정규화해줍니다.\n",
    "\n",
    "           원본은 0~255 사이의 값입니다.\n",
    "           \n",
    "   Step02. MNIST 데이터의 채널 차원을 추가해줍니다.\n",
    "           \n",
    "   Step03. 0~9 사이 값인 레이블을 클래스화 하기 위해 원-핫 인코딩을 진행합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    \n",
    "    # MNIST 데이터 세트를 불러옵니다.\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    \n",
    "    # MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "    \n",
    "    train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
    "    test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
    "    \n",
    "    train_images = train_images / 255.\n",
    "    test_images = test_images / 255.\n",
    "    \n",
    "    train_images = tf.expand_dims(train_images, -1)\n",
    "    test_images = tf.expand_dims(test_images, -1)\n",
    "        \n",
    "    train_labels = tf.one_hot(train_labels, depth = 10)\n",
    "    test_labels = tf.one_hot(test_labels, depth = 10)\n",
    "    \n",
    "    return train_images, test_images, train_labels, test_labels\n",
    "\n",
    "\n",
    "'''\n",
    "2. CNN 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "def CNN():\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding = 'SAME'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding = 'SAME'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(padding = 'SAME'))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "'''\n",
    "3. 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "\n",
    "   Step01. CNN 함수를 통해 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 모델을 학습시킵니다. 검증용 데이터도 설정하세요.\n",
    "           'epochs'와 'batch_size'도 자유롭게 설정하세요.\n",
    "           단, 'epochs'이 클수록, 'batch_size'는 작을수록 학습 속도가 느립니다.\n",
    "   \n",
    "   Step05. 모델을 테스트하고 손실(loss)값과 Test Accuracy 값 및 예측 클래스, \n",
    "           손실 함수값 그래프를 출력합니다. 모델의 성능을 확인해보고,\n",
    "           목표값을 달성해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    train_images, test_images, train_labels, test_labels = preprocess()\n",
    "    \n",
    "    model = CNN()\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    history = model.fit(train_images, train_labels, epochs = 20, batch_size = 512, verbose=0, validation_data = (test_images, test_labels))\n",
    "    \n",
    "    loss, test_acc = model.evaluate(test_images, test_labels, verbose = 0)\n",
    "    \n",
    "    print('\\nTest Loss : {:.4f} | Test Accuracy : {}'.format(loss, test_acc))\n",
    "    print('예측한 Test Data 클래스 : ',model.predict_classes(test_images)[:10])\n",
    "    \n",
    "    Visulaize([('CNN', history)], 'loss')\n",
    "    \n",
    "    Plotter(test_images, model)\n",
    "    \n",
    "    return history\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38d291",
   "metadata": {},
   "source": [
    "# 실습4: 토큰화와 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Twitter \n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "'''\n",
    "1. '문서'를 '문단'의 리스트로 변환하는 함수 doc2para를 완성합니다.\n",
    "\n",
    "   Step01. 문장의 마침표(.) 뒤에 있는 개행 표시(\\n)를 기준으로 \n",
    "\n",
    "           문서 내 글들을 리스트 요소 즉, 문단으로 나눕니다.\n",
    "           \n",
    "   Step02. 나누어진 글들 중 마지막 글자가 \".\"인 경우만 \n",
    "           문단이 나누어진 것으로 보고 그 외의 문장들은 서로 \n",
    "           병합하여 줍니다.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def doc2para(writing):\n",
    "    \n",
    "    paragraphs = []\n",
    "    \n",
    "    splited = writing.split('\\n')\n",
    "    splited = list(filter(lambda x: len(x)>0, splited))\n",
    "    para = \"\"\n",
    "    \n",
    "    for sentence in splited:\n",
    "        if sentence[-1] != '.':\n",
    "            para += sentence\n",
    "        else:\n",
    "            # 아래 코드를 추가하여 문단 내 마지막 문장을 읽어올 수 있습니다.\n",
    "            para += sentence\n",
    "            paragraphs.append(para)\n",
    "            para = \"\"\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "'''\n",
    "2. '문단'을 '문장'의 리스트로 변환하는 함수 para2sen을 완성합니다.\n",
    "\n",
    "   Step01. 문단을 \".\"으로 나누어 리스트로 만들고, \n",
    "\n",
    "           변수 sentences에 저장합니다.\n",
    "           \n",
    "   Step02. sentences 내 문장들에 대해서 \"?\"로 재분할 한 후, \n",
    "           ndarray.flatten()을 활용하여 재분할된 문장이 합쳐질 \n",
    "           수 있도록 리스트로 만들어 줍니다. \n",
    "           (\"!\"에 대해서도 마찬가지로 적용합니다.)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def para2sen(paragraph):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    # Step01.\n",
    "    sentences = paragraph.split('.')\n",
    "    \n",
    "    # Step02. \n",
    "    sentences = [sentence.split('?') for sentence in sentences]\n",
    "    sentences = np.array(sentences).flatten()\n",
    "    sentences = [sentence.split('!') for sentence in sentences]\n",
    "    sentences = np.array(sentences).flatten()\n",
    "    sentences = [ sentence.replace('\"','') for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# 띄어쓰기로 문장을 구분하는 함수\n",
    "\n",
    "\n",
    "def sen2words_byspace(sentence):\n",
    "    \n",
    "    words = []\n",
    "    words = sentence.strip().split(\" \")\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "'''\n",
    "3. 'Twitter()'로 선언된 Tokenizer인 'analyzer'를 이용해 형태소에 따라 \n",
    "    분할된 문장의 리스트를 변수 'morphs'에 저장하는 sen2morph\n",
    "    함수를 완성합니다. Twitter.morphs 메소드를 사용하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def sen2morph(sentence):\n",
    "    \n",
    "    morphs = []\n",
    "    \n",
    "    analyzer = Twitter()\n",
    "    morphs = analyzer.morphs(sentence)\n",
    "    \n",
    "    return morphs\n",
    "\n",
    "\n",
    "'''\n",
    "4. 3번과 같이 'Twitter()'로 선언된 Tokenizer인 'analyzer'를 이용해\n",
    "   형태소와 그에 따른 품사를 분할하는 analyzing_morphs 함수를 완성합니다.\n",
    "   Twitter.pos 메소드를 사용하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def analyzing_morphs(sentence):\n",
    "    \n",
    "    analyzer = Twitter()\n",
    "    \n",
    "    return analyzer.pos(sentence)\n",
    "    \n",
    "# 위에서 정의한 함수들을 바탕으로 문서를 토큰화를 진행합니다.\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    DATA_PATH = \"./data/blood_rain.txt\"\n",
    "    \n",
    "    blood_rain = load_data(DATA_PATH)\n",
    "    paragraphs = doc2para(blood_rain)\n",
    "    sentences = para2sen(paragraphs[4])\n",
    "    words_byspace = sen2words_byspace(sentences[3])\n",
    "    words_bymorphs = sen2morph(sentences[3])\n",
    "    morphs_analyzed = analyzing_morphs(sentences[3])\n",
    "    \n",
    "    # 출력을 통해 토큰화가 잘 되었는지 확인합니다.\n",
    "    \n",
    "    print(\"문장으로 구분된 5번째 문단: \", sentences)\n",
    "    print(\"\\n띄어쓰기로 구분된 문장 (5번째 문단의 4번째 문장): \", words_byspace)\n",
    "    print(\"\\n형태소 별로 구분된 문장 (5번째 문단의 4번째 문장): \", words_bymorphs)\n",
    "    print(\"\\n형태소와 그에 따른 품사로 분류된 문장 (5번째 문단의 4번째 문장): \", morphs_analyzed)\n",
    "    \n",
    "    return words_byspace, words_bymorphs, morphs_analyzed\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d7794",
   "metadata": {},
   "source": [
    "# 실습5: Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "# 텍스트 파일을 읽어서 저장하는 함수입니다.\n",
    "\n",
    "\n",
    "def read_txt(path):\n",
    "    \n",
    "    file = open(path, 'r')\n",
    "    output = str(file.read())\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "'''\n",
    "1. BoW를 딕셔너리 형태로 출력하는 함수를 만들어봅니다.\n",
    "\n",
    "   Step01. 기존 word_dict 에 그 값이 있으면 1을 더해주고, 없으면 1을 부여합니다.\n",
    "'''\n",
    "\n",
    "def bag_of_words(tokenized_sentences):\n",
    "    word_dict = {}\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        for token in tokenized_sentence:\n",
    "            try:\n",
    "                word_dict[token]+=1\n",
    "            except:\n",
    "                word_dict[token]=1\n",
    "    return word_dict\n",
    "\n",
    "'''\n",
    "2. 읽어온 텍스트 파일을 형태소 단위로 분석해서 저장하는 \n",
    "   함수를 완성하세요. 이전 실습을 참고해도 좋습니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def get_splited_doc(path):\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    text = read_txt(path)\n",
    "    analyzer = Twitter()\n",
    "    output = analyzer.morphs(text)\n",
    "    \n",
    "    return text, output\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    PATH = \"./data/text.txt\"\n",
    "    \n",
    "    origin, splitted = get_splited_doc(PATH)\n",
    "    \n",
    "    print('형법 제2장 원본: ', origin)\n",
    "    \n",
    "    bow_criminal_law = bag_of_words([splitted])\n",
    "    \n",
    "    print('\\n형법 제2장의 BoW: ', bow_criminal_law)\n",
    "    \n",
    "    return bow_criminal_law\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e0eae",
   "metadata": {},
   "source": [
    "# 실습6: 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "'''\n",
    "1. embedding 함수를 완성합니다.\n",
    "\n",
    "\n",
    "   Step01. 입력된 리스트에 존재하는 요소마다 고유 인덱스를 붙입니다.\n",
    "           \n",
    "   Step02. 요소와 인덱스를 짝지은 딕셔너리 'word_dict'를 정의합니다.\n",
    "   \n",
    "   Step03. 'sentence1', 'sentence2'를 정수값으로 변환하고 이를\n",
    "           각각 리스트 변수 'sen1', 'sen2'로 정의합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def embedding(sentence1, sentence2):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence1+sentence2)\n",
    "    word_dict = tokenizer.word_index\n",
    "    # word_dict의 value가 0부터 시작하게 바꿔줍니다.\n",
    "    for k, v in word_dict.items():\n",
    "        word_dict[k] = v - 1\n",
    "    \n",
    "    sen1 = tokenizer.texts_to_sequences(sentence1)\n",
    "    sen2 = tokenizer.texts_to_sequences(sentence2)\n",
    "    \n",
    "    sen1 = [token[0] for token in sen1]\n",
    "    sen2 = [token[0] for token in sen2]\n",
    "    \n",
    "    return word_dict, sen1, sen2\n",
    "    \n",
    "'''\n",
    "2. 텐서플로우를 사용하여 원-핫 인코딩을 실행합니다.\n",
    "   원-핫 벡터의 총 길이는 'word_dict' 안의 word의 총 개수입니다. \n",
    "   단어를 원-핫 인코딩 후 이것을 문장별로 (요소별) 더합니다.\n",
    "'''   \n",
    "\n",
    "\n",
    "\n",
    "def one_hot(sen1, sen2, word_dict):\n",
    "    \n",
    "    oh_sen1 = sum(tf.one_hot(sen1, len(word_dict)))\n",
    "    oh_sen2 = sum(tf.one_hot(sen2, len(word_dict)))\n",
    "    \n",
    "    return oh_sen1, oh_sen2\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    sentence1 = ['나','는','오늘','저녁','에','치킨','을','먹','을','예정','입니다']\n",
    "    sentence2 = ['나','는','어제', '맥주','와', '함께', '치킨','을', '먹었', '습니다']\n",
    "    \n",
    "    word_dict, seq_1, seq_2 = embedding(sentence1, sentence2)\n",
    "    onehot_sen1, onehot_sen2 = one_hot(seq_1, seq_2, word_dict)\n",
    "        \n",
    "    print('리스트 요소-인덱스 딕셔너리: ', word_dict)\n",
    "    \n",
    "    print('\\n정수값으로 변환된 sentence1:', seq_1)\n",
    "    print('\\n정수값으로 변환된 sentence2:', seq_2)\n",
    "    \n",
    "    print('\\n원-핫 인코딩된 문장1:', onehot_sen1.numpy())\n",
    "    print('\\n원-핫 인코딩된 문장2:', onehot_sen2.numpy())\n",
    "    \n",
    "    return onehot_sen1, onehot_sen2\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655edbb",
   "metadata": {},
   "source": [
    "# 실습7: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01516a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import list_file\n",
    "\n",
    "'''\n",
    "1. CBOW 함수를 설명을 참고하여 완성하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def CBOW(sentences):\n",
    "    \n",
    "    model_cbow = word2vec.Word2Vec(sentences, size = 300, min_count = 1, window = 10, sg = 0)\n",
    "    \n",
    "    return model_cbow\n",
    "\n",
    "\n",
    "'''\n",
    "2. Skip_Gram 함수를 설명을 참고하여 완성하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def Skip_Gram(sentences):\n",
    "    \n",
    "    model_skipgram = word2vec.Word2Vec(sentences, size = 300, min_count = 1, window = 10, sg = 1)\n",
    "    \n",
    "    return model_skipgram\n",
    "\n",
    "\n",
    "'''\n",
    "3. 각 모델의 결괏값을 정의하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    sen1, sen2 = list_file.sen1(), list_file.sen2()\n",
    "    \n",
    "    sentences = [sen1, sen2]\n",
    "    \n",
    "    cbow = CBOW(sentences)\n",
    "    skipgram = Skip_Gram(sentences)\n",
    "    \n",
    "    idx2word_set_cbow = cbow.wv.index2word\n",
    "    idx2word_set_skipgram = skipgram.wv.index2word\n",
    "    \n",
    "    print('CBOW: ', idx2word_set_cbow)\n",
    "    print('\\nSkip-Gram: ', idx2word_set_skipgram)\n",
    "    \n",
    "    return idx2word_set_cbow, idx2word_set_skipgram\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7e804",
   "metadata": {},
   "source": [
    "# 실습8: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3321f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "'''\n",
    "1. 간단한 RNN 모델을 만들어봅시다.\n",
    "\n",
    "\n",
    "   Step01. 0의 값을 갖는 (output_size,) 모양의 'state' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step02. 1의 값을 갖는 (output_size, input_size) 모양의 'w' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step03. 1의 값을 갖는 (output_size, output_size) 모양의 'u' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step04. 임의의 값을 갖는 (output_size,) 모양의 'b' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step05. bias 가 False 이면 b를 (output_size,) 모양의 영벡터로 만듭니다.\n",
    "   \n",
    "   Step06. Numpy를 사용해서 w와 _input을 내적하고, u 와 state를 내적한 후\n",
    "           b를 더한 다음 tanh 함수를 적용합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def rnn(inputs, input_size, output_size, bias = False):\n",
    "\n",
    "    input_size = len(inputs[0])\n",
    "    \n",
    "    state = np.zeros((output_size,))\n",
    "    \n",
    "    w = np.ones((output_size, input_size))\n",
    "    \n",
    "    u = np.ones((output_size, output_size))\n",
    "    \n",
    "    b = np.random.random((output_size,))\n",
    "    \n",
    "    if not bias:\n",
    "        b = np.zeros((output_size,))\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for _input in inputs:\n",
    "        \n",
    "        _output = np.tanh(np.dot(w, _input) + np.dot(u, state) + b)\n",
    "        outputs.append(_output)\n",
    "        state = _output\n",
    "        \n",
    "    return np.stack(outputs, axis=0) \n",
    "\n",
    "\n",
    "# 케이스에 따라 RNN 모델의 결과가 어떻게 바뀌는지 확인해보세요.\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(\"-----------------CASE 1-----------------\")\n",
    "    _input1 = [[0], [0], [0], [0], [0]]\n",
    "    \n",
    "    # 입력이 모두 0이고 출력 벡터의 크기가 1일 때 값의 추세가 어떠한지 확인해보세요.\n",
    "    case1_a = rnn(_input1, input_size=1, output_size=1)\n",
    "    print('\\nCASE 1_a:', case1_a)\n",
    "    # Bias 가 있으면 값이 어떻게 변화하는지 확인해보세요.\n",
    "    case1_b = rnn(_input1, input_size=1, output_size=1, bias = True)\n",
    "    print('\\nCASE 1_b:', case1_b)\n",
    "    \n",
    "    \n",
    "    print(\"\\n-----------------CASE 2-----------------\")\n",
    "    _input2 = [[1], [1], [1], [1], [1]]\n",
    "    \n",
    "    # 입력이 모두 1이고 출력 벡터의 크기가 1일 때 값의 추세가 어떠한지 확인해보세요.\n",
    "    case2_a = rnn(_input2, input_size=1, output_size=1)\n",
    "    print('\\nCASE 2_a:', case2_a)\n",
    "    # Bias 가 있으면 값이 어떻게 변화하는지 확인해보세요.\n",
    "    case2_b = rnn(_input2, input_size=1, output_size=1, bias = True)\n",
    "    print('\\nCASE 2_b:', case2_b)\n",
    "    \n",
    "    \n",
    "    print(\"\\n-----------------CASE 3-----------------\")\n",
    "    _input3 = [[1], [2], [3], [4], [5]]\n",
    "    \n",
    "    # 입력값이 증가하고 출력 벡터의 크기가 2일 때 값의 추세가 어떠한지 확인해보세요.\n",
    "    case3_a = rnn(_input3, input_size=1, output_size=2)\n",
    "    print('\\nCASE 3_a:', case3_a)\n",
    "    # Bias 가 있으면 값이 어떻게 변화하는지 확인해보세요.\n",
    "    case3_b = rnn(_input3, input_size=1, output_size=2, bias = True)\n",
    "    print('\\nCASE 3_b:', case3_b)\n",
    "    \n",
    "    return case1_a, case1_b, case2_a, case2_b, case3_a, case3_b\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee14be2",
   "metadata": {},
   "source": [
    "# 실습9: RNN keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa719f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# 데이터를 불러오고 전처리하는 함수입니다.\n",
    "\n",
    "\n",
    "def load_data(n_of_training_ex, n_of_testing_ex, max_review_length):\n",
    "    \n",
    "    PATH = \"./data/\"\n",
    "    \n",
    "    X_train = np.load(PATH + \"X_train.npy\")[:n_of_training_ex]\n",
    "    y_train = np.load(PATH + \"y_train.npy\")[:n_of_training_ex]\n",
    "    X_test = np.load(PATH + \"X_test.npy\")[:n_of_testing_ex]\n",
    "    y_test = np.load(PATH + \"y_test.npy\")[:n_of_testing_ex]\n",
    "    \n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "'''\n",
    "1. SimpleRNN을 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "    \n",
    "def SimpleRNN(embedding_vector_length, max_review_length):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length))\n",
    "    model.add(tf.keras.layers.SimpleRNN(5))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "2. LSTM을 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def LSTM(embedding_vector_length, max_review_length):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length))\n",
    "    model.add(tf.keras.layers.LSTM(5))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "3. GRU를 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def GRU(embedding_vector_length, max_review_length):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length))\n",
    "    model.add(tf.keras.layers.GRU(5))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "4. 세 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. SimpleRNN, LSTM, GRU 함수를 이용해 세 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 세 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 세 모델을 각각 학습시킵니다. 검증용 데이터는 설정하지 않습니다.\n",
    "           세 모델 모두 'epochs'는 3, 'batch_size'는 256으로 설정합니다.\n",
    "   \n",
    "   Step05. 세 모델을 테스트하고 각각의 Test Accuracy 값을 출력합니다. \n",
    "           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    max_review_length = 300\n",
    "    embedding_vector_length = 32\n",
    "    \n",
    "    n_of_training_ex = 25000\n",
    "    n_of_testing_ex = 3000\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_data(n_of_training_ex, n_of_testing_ex, max_review_length)\n",
    "    \n",
    "    model_simple_rnn = SimpleRNN(embedding_vector_length, max_review_length)\n",
    "    model_lstm = LSTM(embedding_vector_length, max_review_length)\n",
    "    model_gru = GRU(embedding_vector_length, max_review_length)\n",
    "    \n",
    "    model_simple_rnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model_lstm.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model_gru.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    model_simple_rnn.summary()\n",
    "    model_lstm.summary()\n",
    "    model_gru.summary()\n",
    "    \n",
    "    model_simple_rnn_history = model_simple_rnn.fit(X_train, y_train, epochs = 3, batch_size = 64, verbose = 0)\n",
    "    print('\\n')\n",
    "    model_lstm_history = model_lstm.fit(X_train, y_train, epochs = 3, batch_size = 64, verbose = 0)\n",
    "    print('\\n')\n",
    "    model_gru_history = model_gru.fit(X_train, y_train, epochs = 3, batch_size = 64, verbose = 0)\n",
    "    \n",
    "    scores_simple_rnn = model_simple_rnn.evaluate(X_test, y_test, verbose = 0)\n",
    "    scores_lstm = model_lstm.evaluate(X_test, y_test, verbose= 0)\n",
    "    scores_gru = model_gru.evaluate(X_test, y_test, verbose = 0)\n",
    "    \n",
    "    print('\\nTest Accuracy_simple rnn: ', scores_simple_rnn[-1])\n",
    "    print('Test Accuracy_lstm: ', scores_lstm[-1])\n",
    "    print('Test Accuracy_gru: ', scores_gru[-1])\n",
    "    \n",
    "    return model_simple_rnn_history, model_lstm_history, model_gru_history\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
