{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bcebb74",
   "metadata": {},
   "source": [
    "# 실습1: GD vs SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터 Shape을 맞춰주는 함수\n",
    "\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "'''\n",
    "1. GD와 SGD를 적용할 하나의 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def GD_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "'''\n",
    "2. SGD를 적용할 모델을 GD를 적용할 모델과 똑같이 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def SGD_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "            tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "3. 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 모델의 손실 함수, 최적화 방법, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 모델을 두 개로 나누어 각각 학습시킵니다. 'epochs'는 20으로 설정합니다.\n",
    "   \n",
    "           GD를 적용할 경우 학습 시 전체 데이터 셋(full-batch)을\n",
    "           사용하므로 'batch_size'를 전체 데이터 갯수로 설정합니다. \n",
    "           \n",
    "           SGD를 적용할 경우 학습 시 미니 배치(mini-batch)를 사용하므로\n",
    "           'batch_size'를 전체 데이터 갯수보다 작은 수로 설정합니다. \n",
    "           여기선 500으로 설정하겠습니다.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 점수를 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "\n",
    "\n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    gd_model = GD_model(word_num)\n",
    "    sgd_model = GD_model(word_num)\n",
    "    \n",
    "    gd_model.compile(optimizer = 'sgd',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy', 'binary_crossentropy'])\n",
    "                     \n",
    "    sgd_model.compile(optimizer = 'sgd',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    gd_model.summary()\n",
    "    sgd_model.summary()\n",
    "    \n",
    "    gd_history = gd_model.fit(train_data, train_labels, epochs = 20, batch_size = data_num, \n",
    "                           validation_data = (test_data, test_labels), verbose = 2)\n",
    "    print('\\n')\n",
    "    sgd_history = sgd_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, \n",
    "                            validation_data = (test_data, test_labels), verbose = 2)\n",
    "    \n",
    "    scores_gd = gd_history.history['val_binary_crossentropy'][-1]\n",
    "    scores_sgd = sgd_history.history['val_binary_crossentropy'][-1]\n",
    "    \n",
    "    print('\\nscores_gd: ', scores_gd)\n",
    "    print('scores_sgd: ', scores_sgd)\n",
    "    \n",
    "    Visulaize([('GD', gd_history),('SGD', sgd_history)])\n",
    "    \n",
    "    return gd_history, sgd_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19a7d5",
   "metadata": {},
   "source": [
    "# 실습2: Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "\n",
    "        results[i, word_indices] = 1.0 \n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "'''\n",
    "1. 모멘텀(momentum)을 적용/비적용 할 하나의 모델을 생성합니다.\n",
    "\n",
    "'''\n",
    "    \n",
    "def Momentum_model(word_num):\n",
    "\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "2. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. Momentum 함수를 이용해 두 모델을 불러옵니다. 모두 동일한 모델입니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 방법, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 각각 학습시킵니다. 검증용 데이터도 설정해주세요.\n",
    "           두 모델 모두 'epochs'는 20, 'batch_size'는 500으로 설정합니다.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 binary crossentropy 점수를 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    sgd_model = Momentum_model(word_num)\n",
    "    msgd_model = Momentum_model(word_num)\n",
    "    \n",
    "    sgd_opt = tf.keras.optimizers.SGD(lr = 0.01, momentum = 0.0)\n",
    "    sgd_model.compile(optimizer = sgd_opt,\n",
    "                      loss = 'binary_crossentropy',\n",
    "                      metrics = ['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    msgd_opt = tf.keras.optimizers.SGD(lr = 0.01, momentum = 0.9)\n",
    "    msgd_model.compile(optimizer = msgd_opt,\n",
    "                       loss = 'binary_crossentropy',\n",
    "                       metrics = ['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    sgd_model.summary()\n",
    "    msgd_model.summary()\n",
    "    \n",
    "    sgd_history = sgd_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, \n",
    "                                validation_data = (test_data, test_labels), verbose = 0)\n",
    "    print('\\n')\n",
    "    msgd_history = msgd_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, \n",
    "                                  validation_data = (test_data, test_labels), verbose = 0)\n",
    "                                  \n",
    "    scores_sgd = sgd_model.evaluate(test_data, test_labels)\n",
    "    scores_msgd = msgd_model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    print('\\nscores_sgd: ', scores_sgd[-1])\n",
    "    print('scores_msgd: ', scores_msgd[-1])\n",
    "    \n",
    "    Visulaize([('SGD', sgd_history),('mSGD', msgd_history)])\n",
    "    \n",
    "    return sgd_history, msgd_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2776696",
   "metadata": {},
   "source": [
    "# 실습3: 최적화 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "\n",
    "# 데이터 Shape을 맞춰주는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "    \n",
    "'''\n",
    "1. Adagrad, RMSprop, Adam을 적용할 하나의 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def OPT_model(word_num):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "\n",
    "        ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "2. 세 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. OPT_model 함수를 이용해 세 모델을 불러옵니다. 모두 동일한 모델입니다.\n",
    "   \n",
    "   Step02. 세 모델의 손실 함수, 최적화 방법, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 세 모델을 각각 학습시킵니다. \n",
    "           세 모델 모두 'epochs'는 20, 'batch_size'는 500으로 설정합니다.\n",
    "   \n",
    "   Step05. 세 모델을 테스트하고 binary crossentropy 점수를 출력합니다. \n",
    "           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "    # IMDb 데이터 세트는 훈련용 25000개 테스트용 25000개로 이루어져 있습니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    adagrad_model = OPT_model(word_num)\n",
    "    rmsprop_model = OPT_model(word_num)\n",
    "    adam_model = OPT_model(word_num)\n",
    "    \n",
    "    adagrad_opt = tf.keras.optimizers.Adagrad(lr = 0.01 , epsilon = 0.00001, decay = 0.4)\n",
    "    adagrad_model.compile(optimizer = adagrad_opt,\n",
    "                          loss = 'binary_crossentropy',\n",
    "                          metrics = ['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    rmsprop_opt = tf.keras.optimizers.RMSprop(lr = 0.001)\n",
    "    rmsprop_model.compile(optimizer = rmsprop_opt,\n",
    "                          loss = 'binary_crossentropy',\n",
    "                          metrics = ['accuracy', 'binary_crossentropy'])\n",
    "     \n",
    "    adam_opt = tf.keras.optimizers.Adam(lr = 0.01, beta_1 = 0.9, beta_2 = 0.999)\n",
    "    adam_model.compile(optimizer = adam_opt,\n",
    "                       loss = 'binary_crossentropy',\n",
    "                       metrics = ['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    adagrad_model.summary()\n",
    "    rmsprop_model.summary()\n",
    "    adam_model.summary()\n",
    "    \n",
    "    adagrad_history = adagrad_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, \n",
    "                                        validation_data = (test_data, test_labels), verbose = 0)\n",
    "    print('\\n')\n",
    "    rmsprop_history = rmsprop_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, \n",
    "                                        validation_data = (test_data, test_labels), verbose = 0)\n",
    "    print('\\n')\n",
    "    adam_history = adam_model.fit(train_data, train_labels, epochs = 20, batch_size = 500, \n",
    "                                  validation_data = (test_data, test_labels), verbose = 0)\n",
    "    \n",
    "    scores_adagrad = adagrad_model.evaluate(test_data, test_labels)\n",
    "    scores_rmsprop = rmsprop_model.evaluate(test_data, test_labels)\n",
    "    scores_adam = adam_model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    print('\\nscores_adagrad: ', scores_adagrad[-1])\n",
    "    print('scores_rmsprop: ', scores_rmsprop[-1])\n",
    "    print('sscores_adam: ', scores_adam[-1])\n",
    "    \n",
    "    Visulaize([('Adagrad', adagrad_history),('RMSprop', rmsprop_history),('Adam', adam_history)])\n",
    "    \n",
    "    return adagrad_history, rmsprop_history, adam_history\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59e80b",
   "metadata": {},
   "source": [
    "# 실습4: 기울기 소실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "'''\n",
    "1. 모델의 층을 설명과 같이 매우 깊게 쌓아보고,\n",
    "   활성화 함수는 마지막 층만 그대로 두고 나머지는\n",
    "   'relu'로 설정하세요.\n",
    "'''\n",
    "\n",
    "def make_model_relu():\n",
    "\n",
    "    model_relu = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return model_relu\n",
    "    \n",
    "'''\n",
    "2. 모델의 층을 1번과 같이 매우 깊게 쌓아보고,\n",
    "   활성화 함수는 마지막 층만 그대로 두고 나머지는\n",
    "   'sigmoid'로 설정하세요.\n",
    "'''\n",
    "    \n",
    "def make_model_sig():\n",
    "\n",
    "\n",
    "    model_sig = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return model_sig\n",
    "\n",
    "\n",
    "'''\n",
    "3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. model_relu와 model_sig 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 최적화 방법과 손실 함수를 똑같이 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "           우리가 만든 모델이 얼마나 깊은지 확인해보세요.\n",
    "   \n",
    "   Step04. 두 모델을 학습시킵니다. 'epochs'는 5로 설정합니다.\n",
    "           검증용 데이터는 설정하지 않습니다. 'verbose'는 0으로 설정합니다.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 점수를 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "    \n",
    "def main():\n",
    "\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    \n",
    "    model_relu = make_model_relu()\n",
    "    model_sig = make_model_sig()\n",
    "    \n",
    "    model_relu.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model_sig.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model_relu.summary()\n",
    "    model_sig.summary()\n",
    "    \n",
    "    model_relu_history = model_relu.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    print('\\n')\n",
    "    model_sig_history = model_sig.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    \n",
    "    scores_relu = model_relu.evaluate(x_test, y_test)\n",
    "    scores_sig = model_sig.evaluate(x_test, y_test)\n",
    "    \n",
    "    print('\\naccuracy_relu: ', scores_relu[-1])\n",
    "    print('accuracy_sig: ', scores_sig[-1])\n",
    "    \n",
    "    return model_relu_history, model_sig_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aff3bd",
   "metadata": {},
   "source": [
    "# 실습5: 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24513892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    \n",
    "'''\n",
    "1. 모델의 층을 10층 이상으로 매우 깊게 쌓되,\n",
    "   활성화 함수는 마지막 층만 그대로 두고 나머지는\n",
    "   'sigmoid'로 설정하세요.\n",
    "'''\n",
    "    \n",
    "def make_model_sig():\n",
    "\n",
    "\n",
    "    model_sig = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return model_sig\n",
    "    \n",
    "'''\n",
    "2. 모델의 층을 1번과 똑같이 매우 깊게 쌓되,\n",
    "   활성화 함수는 마지막 층만 그대로 두고 나머지는\n",
    "   'relu'로 설정하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def make_model_relu():\n",
    "\n",
    "    model_relu = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return model_relu\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "3. 모델의 층을 1번과 똑같이 매우 깊게 쌓되,\n",
    "   활성화 함수는 마지막 층만 그대로 두고 나머지는\n",
    "   'tanh'로 설정하세요. 레이어의 수가 1번과 같아야 합니다!\n",
    "\n",
    "'''\n",
    "    \n",
    "def make_model_tanh():\n",
    "\n",
    "\n",
    "    model_tanh = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(256, activation='tanh'),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(256, activation='tanh'),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(256, activation='tanh'),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(256, activation='tanh'),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(256, activation='tanh'),\n",
    "        tf.keras.layers.Dense(128, activation='tanh'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return model_tanh\n",
    "\n",
    "\n",
    "'''\n",
    "4. 세 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. make_model_sig, make_model_relu, make_model_tanh 함수를 이용해 세 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 세 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "           우리가 만든 모델이 얼마나 깊은지 확인해보세요.\n",
    "   \n",
    "   Step04. 세 모델을 학습시킵니다. 'epochs'는 5로 설정합니다.\n",
    "           검증용 데이터는 설정하지 않습니다.\n",
    "   \n",
    "   Step05. 세 모델을 테스트하고 accuracy 값을 출력합니다. \n",
    "           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    \n",
    "    model_sig = make_model_sig()\n",
    "    model_relu = make_model_relu()\n",
    "    model_tanh = make_model_tanh()\n",
    "    \n",
    "    model_sig.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model_relu.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model_tanh.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model_sig.summary()\n",
    "    model_relu.summary()\n",
    "    model_tanh.summary()\n",
    "    \n",
    "    model_sig_history = model_sig.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    print('\\n')\n",
    "    model_relu_history = model_relu.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    print('\\n')\n",
    "    model_tanh_history = model_tanh.fit(x_train, y_train, epochs=5, verbose=0)\n",
    "    \n",
    "    scores_sig = model_sig.evaluate(x_test, y_test)\n",
    "    scores_relu = model_relu.evaluate(x_test, y_test)\n",
    "    scores_tanh = model_tanh.evaluate(x_test, y_test)\n",
    "    \n",
    "    print('\\naccuracy_sig: ', scores_sig[-1])\n",
    "    print('accuracy_relu: ', scores_relu[-1])\n",
    "    print('accuracy_tanh: ', scores_tanh[-1])\n",
    "    \n",
    "    return model_sig_history, model_relu_history, model_tanh_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fdac5",
   "metadata": {},
   "source": [
    "# 실습6: 가중치조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from visual import *\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "def sigmoid(x):\n",
    "    result = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return result\n",
    "    \n",
    "'''\n",
    "1. 입력 데이터를 정의하세요.\n",
    "\n",
    "\n",
    "2. 가중치를 정의하세요.\n",
    "\n",
    "3. sigmoid를 통과할 값인 'a_1', 'a_2'를 정의하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    x_1 = np.random.randn(1000,100)\n",
    "    x_2 = np.random.randn(1000,100)\n",
    "    \n",
    "    node_num = 100\n",
    "    hidden_layer_size = 5\n",
    "    \n",
    "    activations_1 = {}\n",
    "    activations_2 = {}\n",
    "    \n",
    "    for i in range(hidden_layer_size):\n",
    "        if i != 0:\n",
    "            x_1 = activations_1[i-1]\n",
    "            x_2 = activations_2[i-1]\n",
    "            \n",
    "        w_1 = np.random.randn(node_num, node_num) * 1\n",
    "        w_2 = np.random.randn(node_num, node_num) * 0.01\n",
    "        \n",
    "        a_1 = np.dot(x_1,w_1)\n",
    "        a_2 = np.dot(x_2,w_2)\n",
    "        \n",
    "        z_1 = sigmoid(a_1)\n",
    "        z_2 = sigmoid(a_2)\n",
    "        \n",
    "        activations_1[i] = z_1\n",
    "        activations_2[i] = z_2\n",
    "        \n",
    "    Visual(activations_1,activations_2)\n",
    "    \n",
    "    return activations_1, activations_2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9e81b",
   "metadata": {},
   "source": [
    "# 실습7: Xaiver 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa90991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from visual import *\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "def sigmoid(x):\n",
    "    result = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return result\n",
    "    \n",
    "def relu(x):\n",
    "    result = np.maximum(0,x)\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    x_sig = np.random.randn(1000,100)\n",
    "\n",
    "    x_relu = np.random.randn(1000,100)\n",
    "    \n",
    "    node_num = 100\n",
    "    hidden_layer_size = 5\n",
    "    \n",
    "    activations_sig = {}\n",
    "    activations_relu = {}\n",
    "    \n",
    "    for i in range(hidden_layer_size):\n",
    "        if i != 0:\n",
    "            x_sig = activations_sig[i-1]\n",
    "            x_relu = activations_relu[i-1]\n",
    "            \n",
    "        w_sig = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
    "        w_relu = np.random.randn(node_num, node_num) / np.sqrt(node_num)\n",
    "        \n",
    "        a_sig = np.dot(x_sig,w_sig)\n",
    "        a_relu = np.dot(x_relu,w_relu)\n",
    "        \n",
    "        z_sig = sigmoid(a_sig)\n",
    "        z_relu = relu(a_relu)\n",
    "        \n",
    "        activations_sig[i] = z_sig\n",
    "        activations_relu[i] = z_relu\n",
    "        \n",
    "    Visual(activations_sig, activations_relu)\n",
    "    \n",
    "    return activations_sig, activations_relu    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2532d7d",
   "metadata": {},
   "source": [
    "# 실습8: He 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf10ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from visual import *\n",
    "\n",
    "\n",
    "np.random.seed(100)\n",
    "    \n",
    "def relu(x):\n",
    "    result = np.maximum(0,x)\n",
    "    return result\n",
    "\n",
    "\n",
    "'''\n",
    "1. 입력 데이터를 정의하세요.\n",
    "\n",
    "2. 가중치 초깃값 설정 부분을 왼쪽 설명에 맞게 바꿔보세요.\n",
    "\n",
    "   Numpy의 연산 메서드를 사용할 수 있습니다.\n",
    "   \n",
    "3. relu를 통과할 값인 'a_relu'를 정의하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    x_relu = np.random.randn(1000,100)\n",
    "    \n",
    "    node_num = 100\n",
    "    hidden_layer_size = 5\n",
    "    \n",
    "    activations_relu = {}\n",
    "    \n",
    "    for i in range(hidden_layer_size):\n",
    "        if i != 0:\n",
    "            x_relu = activations_relu[i-1]\n",
    "            \n",
    "        w_relu = np.sqrt(2) * np.random.randn(node_num, node_num) / (np.sqrt(node_num))\n",
    "        \n",
    "        a_relu = np.dot(x_relu,w_relu)\n",
    "        \n",
    "        z_relu = relu(a_relu)\n",
    "        \n",
    "        activations_relu[i] = z_relu\n",
    "        \n",
    "    Visual(activations_relu)\n",
    "    \n",
    "    return activations_relu\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840363a4",
   "metadata": {},
   "source": [
    "# 실습9: 과적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d980b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "\n",
    "\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "    \n",
    "'''\n",
    "1. 과적합 될 모델과 비교하기 위해 기본 모델을 \n",
    "   왼쪽 설명과 동일하게 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def Basic(word_num):\n",
    "\n",
    "    basic_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "                  tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "                  tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return basic_model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "2. 기본 모델의 레이어 수와 노드 수를 자유롭게 늘려서\n",
    "   과적합 될 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "def Overfitting(word_num):\n",
    "\n",
    "    overfit_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(1024, activation = 'relu', input_shape=(word_num,)),\n",
    "                  tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "                  tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "                  tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "                  tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return overfit_model\n",
    "    \n",
    "'''\n",
    "3. 두 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   Step01. basic_model와 overfit_model 함수를 이용해 두 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 학습시킵니다. 검증용 데이터도 설정해주세요.\n",
    "           기본 모델은 'epochs'를 20, 과적합 모델은 'epochs'를 300이상으로 설정합니다.\n",
    "           'batch_size'는 두 모델 모두 500으로 설정합니다.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 binary crossentropy 값을 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 안 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "    # IMDb 데이터 세트는 훈련용 25000개 테스트용 25000개로 이루어져 있습니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    basic_model = Basic(word_num)\n",
    "    overfit_model = Overfitting(word_num)\n",
    "    \n",
    "    basic_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',                               'binary_crossentropy'])\n",
    "    overfit_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',                               'binary_crossentropy'])\n",
    "    \n",
    "    basic_model.summary()\n",
    "    overfit_model.summary()\n",
    "    \n",
    "    basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 1)\n",
    "    print('\\n')\n",
    "    overfit_history = overfit_model.fit(train_data, train_labels, epochs = 300, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 1)\n",
    "    \n",
    "    scores_basic = basic_model.evaluate(test_data, test_labels)\n",
    "    scores_overfit = overfit_model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    print('\\nscores_basic: ', scores_basic[-1])\n",
    "    print('scores_overfit: ', scores_overfit[-1])\n",
    "    \n",
    "    Visualize([('Basic', basic_history),('Overfitting', overfit_history)])\n",
    "\n",
    "\n",
    "\n",
    "    return basic_history, overfit_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded434b",
   "metadata": {},
   "source": [
    "# 실습 10: 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "'''\n",
    "1. L1, L2 정규화를 적용한 모델과 비교하기 위해 \n",
    "   왼쪽 설명과 동일하게 기본 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "def Basic(word_num):\n",
    "\n",
    "    basic_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "                  tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "                  tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return basic_model\n",
    "    \n",
    "    \n",
    "'''\n",
    "2. 기본 모델에 L1 정규화를 적용합니다.\n",
    "   입력층과 히든층에만 적용하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def L1(word_num):\n",
    "\n",
    "    l1_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,), kernel_regularizer = tf.keras.regularizers.l1(0.001)),\n",
    "                  tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(0.001)),\n",
    "                  tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return l1_model\n",
    "\n",
    "\n",
    "'''\n",
    "3. 기본 모델에 L2 정규화를 적용합니다.\n",
    "   입력층과 히든층에만 적용하세요.\n",
    "'''\n",
    "\n",
    "def L2(word_num):\n",
    "\n",
    "    l2_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,), kernel_regularizer = tf.keras.regularizers.l2(0.001)),\n",
    "                  tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.001)),\n",
    "                  tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return l2_model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "4. 세 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. Basic, L1, L2 함수를 이용해 세 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 세 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 세 모델을 학습시킵니다. 세 모델 모두 'epochs'는 20,\n",
    "           'batch_size'는 500으로 설정합니다. 검증용 데이터도 설정해주세요.\n",
    "   \n",
    "   Step05. 세 모델을 테스트하고 binary crossentropy 값을 출력합니다. \n",
    "           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "\n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "\n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "\n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    basic_model = Basic(word_num)\n",
    "    l1_model = L1(word_num)\n",
    "    l2_model = L2(word_num)\n",
    "\n",
    "\n",
    "    basic_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',                               'binary_crossentropy'])\n",
    "    l1_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',                               'binary_crossentropy'])\n",
    "    l2_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',                               'binary_crossentropy'])\n",
    "\n",
    "    basic_model.summary()\n",
    "    l1_model.summary()\n",
    "    l2_model.summary()\n",
    "\n",
    "    basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 0)\n",
    "    print('\\n')\n",
    "    l1_history = l1_model.fit(train_data, train_labels, epochs = 20, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 0)\n",
    "    print('\\n')\n",
    "\n",
    "    l2_history = l2_model.fit(train_data, train_labels, epochs = 20, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 0)\n",
    "    \n",
    "    scores_basic = basic_model.evaluate(test_data, test_labels)\n",
    "    scores_l1 = l1_model.evaluate(test_data, test_labels)\n",
    "    scores_l2 = l2_model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    print('\\nscores_basic: ', scores_basic[-1])\n",
    "    print('scores_l1: ', scores_l1[-1])\n",
    "    print('scores_l2: ', scores_l2[-1])\n",
    "    \n",
    "    Visulaize([('Basic', basic_history),('L1 Regularization', l1_history), ('L2 Regularization', l2_history)])\n",
    "\n",
    "\n",
    "    return basic_history, l1_history, l2_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b51c9d",
   "metadata": {},
   "source": [
    "# 실습11: 드롭아웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7677fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터 Shape을 맞춰주는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "    \n",
    "'''\n",
    "1. 드롭 아웃을 적용할 모델과 비교하기 위한\n",
    "   하나의 기본 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "\n",
    "def Basic(word_num):\n",
    "\n",
    "    basic_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "                  tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "                  tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return basic_model\n",
    "    \n",
    "'''\n",
    "2. 기본 모델에 드롭 아웃 레이어를 추가합니다.\n",
    "   일반적으로 마지막 히든층과 출력층 사이에 하나만 추가합니다.\n",
    "   드롭 아웃 적용 확률은 자유롭게 설정하세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def Dropout(word_num):\n",
    "\n",
    "    dropout_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)),\n",
    "                  tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "                  tf.keras.layers.Dropout(0.5),\n",
    "                  tf.keras.layers.Dense(1, activation= 'sigmoid')\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return dropout_model\n",
    "\n",
    "\n",
    "'''\n",
    "3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. Basic, Dropout 함수를 이용해 두 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 학습시킵니다. 두 모델 모두 'epochs'는 20,\n",
    "           'batch_size'는 500으로 설정합니다. 검증용 데이터도 설정해주세요.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 점수를 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러옵니다.\n",
    "    # IMDb 데이터 세트는 훈련용 25000개 테스트용 25000개로 이루어져 있습니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    basic_model = Basic(word_num)\n",
    "    dropout_model = Dropout(word_num)\n",
    "    \n",
    "    basic_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',                               'binary_crossentropy'])\n",
    "    dropout_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',                               'binary_crossentropy'])\n",
    "    \n",
    "    basic_model.summary()\n",
    "    dropout_model.summary()\n",
    "    \n",
    "    basic_history = basic_model.fit(train_data, train_labels, epochs = 20, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 0)\n",
    "    print('\\n')\n",
    "    dropout_history = dropout_model.fit(train_data, train_labels, epochs = 20, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 0)\n",
    "    \n",
    "    scores_basic = basic_model.evaluate(test_data, test_labels)\n",
    "    scores_dropout = dropout_model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    print('\\nscores_basic: ', scores_basic[-1])\n",
    "    print('scores_dropout: ', scores_dropout[-1])\n",
    "    \n",
    "    Visulaize([('Basic', basic_history),('Dropout', dropout_history)])\n",
    "    \n",
    "    return basic_history, dropout_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49b129",
   "metadata": {},
   "source": [
    "# 실습12: 배치정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35aaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "np.random.seed(200)\n",
    "tf.random.set_seed(200)\n",
    "\n",
    "# 배치 정규화를 적용할 모델과 비교하기 위한 기본 모델입니다.\n",
    "\n",
    "def Basic():\n",
    "\n",
    "    basic_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "                  tf.keras.layers.Dense(256),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(128),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(512),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(64),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(128),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(256),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return basic_model\n",
    "\n",
    "\n",
    "'''\n",
    "2. 기본 모델에 배치 정규화 레이어를 적용한 \n",
    "   모델을 생성합니다. 입력층과 출력층은 그대로 사용합니다.\n",
    "'''\n",
    "\n",
    "def BN():\n",
    "\n",
    "    bn_model = tf.keras.Sequential([\n",
    "                  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "                  tf.keras.layers.Dense(256),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(128),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(512),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(64),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(128),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(256),\n",
    "                  tf.keras.layers.BatchNormalization(),\n",
    "                  tf.keras.layers.Activation('relu'),\n",
    "                  tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "                  ])\n",
    "    \n",
    "    return bn_model\n",
    "\n",
    "\n",
    "'''\n",
    "3. 두 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "\n",
    "   Step01. Basic, BN 함수를 이용해 두 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 학습시킵니다. 두 모델 모두 'epochs'는 2,\n",
    "           'batch_size'는 500으로 설정합니다. 검증용 데이터도 설정해주세요.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 accuracy 값을 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "\n",
    "    train_data, test_data = train_data / 255.0, test_data / 255.0\n",
    "    \n",
    "    basic_model = Basic()\n",
    "    bn_model = BN()\n",
    "\n",
    "\n",
    "    basic_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    bn_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    basic_model.summary()\n",
    "    bn_model.summary()\n",
    "\n",
    "\n",
    "    basic_history = basic_model.fit(train_data, train_labels, epochs = 2, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 0)\n",
    "    \n",
    "    bn_history = bn_model.fit(train_data, train_labels, epochs = 2, batch_size = 500,                                               validation_data = (test_data, test_labels), verbose = 0)\n",
    "    \n",
    "    scores_basic = basic_model.evaluate(test_data, test_labels)\n",
    "    scores_bn = bn_model.evaluate(test_data, test_labels)\n",
    "    \n",
    "    print('\\naccuracy_basic: ', scores_basic[-1])\n",
    "    print('accuracy_bn: ', scores_bn[-1])\n",
    "    \n",
    "    Visulaize([('Basic', basic_history),('Batch Normalization', bn_history)])\n",
    "\n",
    "\n",
    "    return basic_history, bn_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
